{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DL-Framework The DL-Framework is an approach to generate an experimental environment that can easily be reused and extended but without the requirements to change core functionality. The framework is implemented using Tensorflow 2.0 in combination with tf.keras. If you add new model functionalities, like data loading, these operations should be handled by the framework and not be implemented again and again. The same situation for the training loop. In a segmentation task, the training loop for all kind of networks is completely identical, it's not necessary to implement these loops for each model. For more flexibility, the framework is configured by YAML configuration files. This allows it, flexibly, to configure experiments without dying in an argument hell of a command-line application. Additional pre-processing methods, metrics or loss functions can easily append or replaced just by changing the parameter in an experiment configuration. If something is missing, don't hesitate to implement it! A model is missing? Checkout out the already implemented models and start with your implementation. This is also the case for loss functions, data loaders, metrics or pre-processing functions. While semantic segmentation training or classification tasks are identically in the training loop, you can also implement your loop. The best example is the CycleGAN implementation, that overrides the basic training loop. Docs Documentation Installation Use the following command to install the required python packages via pip. pip install -r requirements.txt To install dlf module on your system, use pip install . Usage Experiment Configuration experiment: Key Summary output_folder Required. Path where the models/weights and logging data are saved model: The model section contains required parameters to initialize an ANN model from the framework. All list of all available models and the corresponding parameters can you find in the Documentation/Models . input_reader: The input reader section is responsible to provide data during training, validation and test (not implemented at the moment) . You can find all data generators and the corresponding parameters in Documentation/Data generators Key Summary training_reader Optional. Provides data used during training validation_reader Optional. Provides data used during validation training_reader Optional. Provides data used during training training: This section contains all training specific parameters. These are, for instance, the number of steps to train, after every N step start the evaluation, callbacks, metrics or just the batch size used during training. Key Summary metrics Optional. A list of metrics which are evaluated during training/validation/test. Documentation/Metrics callbacks Optional. A list of callbacks which are executed during training/validation/test. Documentation/Callbacks num_steps Required. Number of steps to train batch_size Optional. Number of samples per gradient update eval_every_step Optional. Start evaluation at every N step (step mod N == 0) save_strategy Optional. Provides a strategy when a model should be saved. Documentation Example configuration Below is a sample configuration that is used to train a CNN for a segmentation task. In the experiment section, we define the output folder where all results are saved. As model the configuration specifies the vgg_encoder_decoder and the corresponding parameters. In this case, the input is an RGB image with a resolution of 512x512 pixels. We set up the network that we can distinguish between 7 classes. Note: &num_classes is YAML syntax to define a variable which we can reuse in our configuration With model_weights the network is forced to use pre-trained weights at the given path to initialize the network. By specifying SparseCategoricalCrossentropyIgnore as loss function we override the default loss of the model. The fact that the vgg_encoder_decoder model uses a softmax-layer as the last layer we pass for the argument from_loggits as False value. As mentioned before, we reuse the value of the num_classes variable as input for the SparseCategoricalCrossentropyIgnore objective. As the stochastic gradient descent method, this experiment uses Adam with a learning rate of 0.00001. During training, the network receives the input from a tf_record_segmentation_reader . Not only the path to the TFRecord and Labelmap file is specified also an option to remap classes. This functionality allows it to change categories of pixels during training from e.g. class 1 to class 0. The preprocess_list contains a list of data augmentation methods that are applied before the images are fed into the network. Not only single scalars can be used as variables also lists and dictionaries. In this case, we use the remap-list also for the validation_reader. For a better overview of the training, the experiment uses two metrics. First the SparseCategoricalCrossentropyIgnore and additional the SparseMeanIoU . All these metric values are logged to Tensorboard by using the callback SegmentationLogger but also segmentation mask examples. The training is executed for 50.000 steps with a batch size of 4 and after every hundredth step, the model is evaluated. The save_strategy ensures that the model is only stored when the value of validation_sparse_mean_iou improves. experiment: output_folder: /mnt/data/experiments/segmentation/SegNet_VGG_transposed model: vgg_encoder_decoder: input_shape: - 512 - 512 - 3 num_classes: &num_classes 7 summary: True use_skip_layers: True model_weights: /mnt/data/experiments/segmentation/SegNet_VGG_transposed/checkpoint loss: SparseCategoricalCrossentropyIgnore: num_classes: *num_classes from_logits: False optimizer: - Adam: learning_rate: 0.00001 input_reader: training_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: &mapping 1: 0 2: 0 3: 0 4: 1 5: 1 preprocess_list: h_flip: v_flip: resize: width: 512 height: 512 validation_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/omnidetector-Flat/training-mask.tfrecord labelmap: *labelmap shuffle: False ignore: remap: preprocess_list: resize: width: 512 height: 512 training: metrics: SparseCategoricalCrossentropyIgnore: from_logits: False num_classes: *num_classes SparseMeanIoU: num_classes: *num_classes callbacks: SegmentationLogger: num_classes: *num_classes # num_visualizations: 200 opacity: 0.4 num_steps: 50000 eval_every_step: 100 batch_size: 4 save_strategy: monitor: validation_sparse_mean_iou mode: max Run an experiment To start an experiment it is just required to execute the experiment.py with the corresponding configuration file. python experiment.py --config config/config_dst.yml","title":"Home"},{"location":"#dl-framework","text":"The DL-Framework is an approach to generate an experimental environment that can easily be reused and extended but without the requirements to change core functionality. The framework is implemented using Tensorflow 2.0 in combination with tf.keras. If you add new model functionalities, like data loading, these operations should be handled by the framework and not be implemented again and again. The same situation for the training loop. In a segmentation task, the training loop for all kind of networks is completely identical, it's not necessary to implement these loops for each model. For more flexibility, the framework is configured by YAML configuration files. This allows it, flexibly, to configure experiments without dying in an argument hell of a command-line application. Additional pre-processing methods, metrics or loss functions can easily append or replaced just by changing the parameter in an experiment configuration. If something is missing, don't hesitate to implement it! A model is missing? Checkout out the already implemented models and start with your implementation. This is also the case for loss functions, data loaders, metrics or pre-processing functions. While semantic segmentation training or classification tasks are identically in the training loop, you can also implement your loop. The best example is the CycleGAN implementation, that overrides the basic training loop.","title":"DL-Framework"},{"location":"#docs","text":"Documentation","title":"Docs"},{"location":"#installation","text":"Use the following command to install the required python packages via pip. pip install -r requirements.txt To install dlf module on your system, use pip install .","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#experiment-configuration","text":"","title":"Experiment Configuration"},{"location":"#experiment","text":"Key Summary output_folder Required. Path where the models/weights and logging data are saved","title":"experiment:"},{"location":"#model","text":"The model section contains required parameters to initialize an ANN model from the framework. All list of all available models and the corresponding parameters can you find in the Documentation/Models .","title":"model:"},{"location":"#input_reader","text":"The input reader section is responsible to provide data during training, validation and test (not implemented at the moment) . You can find all data generators and the corresponding parameters in Documentation/Data generators Key Summary training_reader Optional. Provides data used during training validation_reader Optional. Provides data used during validation training_reader Optional. Provides data used during training","title":"input_reader:"},{"location":"#training","text":"This section contains all training specific parameters. These are, for instance, the number of steps to train, after every N step start the evaluation, callbacks, metrics or just the batch size used during training. Key Summary metrics Optional. A list of metrics which are evaluated during training/validation/test. Documentation/Metrics callbacks Optional. A list of callbacks which are executed during training/validation/test. Documentation/Callbacks num_steps Required. Number of steps to train batch_size Optional. Number of samples per gradient update eval_every_step Optional. Start evaluation at every N step (step mod N == 0) save_strategy Optional. Provides a strategy when a model should be saved. Documentation","title":"training:"},{"location":"#example-configuration","text":"Below is a sample configuration that is used to train a CNN for a segmentation task. In the experiment section, we define the output folder where all results are saved. As model the configuration specifies the vgg_encoder_decoder and the corresponding parameters. In this case, the input is an RGB image with a resolution of 512x512 pixels. We set up the network that we can distinguish between 7 classes. Note: &num_classes is YAML syntax to define a variable which we can reuse in our configuration With model_weights the network is forced to use pre-trained weights at the given path to initialize the network. By specifying SparseCategoricalCrossentropyIgnore as loss function we override the default loss of the model. The fact that the vgg_encoder_decoder model uses a softmax-layer as the last layer we pass for the argument from_loggits as False value. As mentioned before, we reuse the value of the num_classes variable as input for the SparseCategoricalCrossentropyIgnore objective. As the stochastic gradient descent method, this experiment uses Adam with a learning rate of 0.00001. During training, the network receives the input from a tf_record_segmentation_reader . Not only the path to the TFRecord and Labelmap file is specified also an option to remap classes. This functionality allows it to change categories of pixels during training from e.g. class 1 to class 0. The preprocess_list contains a list of data augmentation methods that are applied before the images are fed into the network. Not only single scalars can be used as variables also lists and dictionaries. In this case, we use the remap-list also for the validation_reader. For a better overview of the training, the experiment uses two metrics. First the SparseCategoricalCrossentropyIgnore and additional the SparseMeanIoU . All these metric values are logged to Tensorboard by using the callback SegmentationLogger but also segmentation mask examples. The training is executed for 50.000 steps with a batch size of 4 and after every hundredth step, the model is evaluated. The save_strategy ensures that the model is only stored when the value of validation_sparse_mean_iou improves. experiment: output_folder: /mnt/data/experiments/segmentation/SegNet_VGG_transposed model: vgg_encoder_decoder: input_shape: - 512 - 512 - 3 num_classes: &num_classes 7 summary: True use_skip_layers: True model_weights: /mnt/data/experiments/segmentation/SegNet_VGG_transposed/checkpoint loss: SparseCategoricalCrossentropyIgnore: num_classes: *num_classes from_logits: False optimizer: - Adam: learning_rate: 0.00001 input_reader: training_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: &mapping 1: 0 2: 0 3: 0 4: 1 5: 1 preprocess_list: h_flip: v_flip: resize: width: 512 height: 512 validation_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/omnidetector-Flat/training-mask.tfrecord labelmap: *labelmap shuffle: False ignore: remap: preprocess_list: resize: width: 512 height: 512 training: metrics: SparseCategoricalCrossentropyIgnore: from_logits: False num_classes: *num_classes SparseMeanIoU: num_classes: *num_classes callbacks: SegmentationLogger: num_classes: *num_classes # num_visualizations: 200 opacity: 0.4 num_steps: 50000 eval_every_step: 100 batch_size: 4 save_strategy: monitor: validation_sparse_mean_iou mode: max","title":"Example configuration"},{"location":"#run-an-experiment","text":"To start an experiment it is just required to execute the experiment.py with the corresponding configuration file. python experiment.py --config config/config_dst.yml","title":"Run an experiment"},{"location":"development/","text":"Development","title":"Development"},{"location":"development/#development","text":"","title":"Development"},{"location":"dlf/callbacks/CycleGanLogger/","text":"CycleGanLogger class dlf.callbacks.gan_logger.CycleGanLogger(num_visualizations=50) A Callback for a Tensorboard visualization for a CycleGAN Aliases cycle_gan_logger CycleGanLogger Arguments num_visualizations : int, optional. Number of images to visualize. Defaults to 50. YAML Configuration callbacks: CycleGanLogger: num_visualizations: 200 Example","title":"CycleGanLogger"},{"location":"dlf/callbacks/CycleGanLogger/#cycleganlogger-class","text":"dlf.callbacks.gan_logger.CycleGanLogger(num_visualizations=50) A Callback for a Tensorboard visualization for a CycleGAN Aliases cycle_gan_logger CycleGanLogger Arguments num_visualizations : int, optional. Number of images to visualize. Defaults to 50. YAML Configuration callbacks: CycleGanLogger: num_visualizations: 200 Example","title":"CycleGanLogger class"},{"location":"dlf/callbacks/ObjectDetectionLogger/","text":"ObjectDetectionLogger class dlf.callbacks.object_detection_logger.ObjectDetectionLogger( evaluator, num_visualizations=50, log_train_images=False ) Tensorboard visualization for object detection tasks Aliases object_detection_logger ObjectDetectionLogger Arguments evaluator : dlf.core.evaluator. Evaluator used for the summerie and metrics num_visualizations : int, optional. Number of images to visualize. Defaults to 50. log_train_images : bool. If true training images are visualized as well. Defaults to False. YAML Configuration callbacks: ObjectDetectionLogger: num_visualizations: 200 Example","title":"ObjectDetectionLogger"},{"location":"dlf/callbacks/ObjectDetectionLogger/#objectdetectionlogger-class","text":"dlf.callbacks.object_detection_logger.ObjectDetectionLogger( evaluator, num_visualizations=50, log_train_images=False ) Tensorboard visualization for object detection tasks Aliases object_detection_logger ObjectDetectionLogger Arguments evaluator : dlf.core.evaluator. Evaluator used for the summerie and metrics num_visualizations : int, optional. Number of images to visualize. Defaults to 50. log_train_images : bool. If true training images are visualized as well. Defaults to False. YAML Configuration callbacks: ObjectDetectionLogger: num_visualizations: 200 Example","title":"ObjectDetectionLogger class"},{"location":"dlf/callbacks/SegmentationLogger/","text":"SegmentationLogger class dlf.callbacks.segmentation_logger.SegmentationLogger( num_classes, num_visualizations=50, opacity=0.5, log_train_images=False ) Tensorboard visualization for segmentation tasks Aliases segmentation_logger SegmentationLogger Arguments num_classes : int. Number of classes, required for confusion matrix num_visualizations : int, optional. Number of images to visualize. Defaults to 50. opacity : float. Opacity of segmentation overlay. Defaults to 0.5. log_train_images : bool. If true training images are visualized as well. Defaults to False. YAML Configuration callbacks: SegmentationLogger: num_classes: 7 num_visualizations: 200 Example","title":"SegmentationLogger"},{"location":"dlf/callbacks/SegmentationLogger/#segmentationlogger-class","text":"dlf.callbacks.segmentation_logger.SegmentationLogger( num_classes, num_visualizations=50, opacity=0.5, log_train_images=False ) Tensorboard visualization for segmentation tasks Aliases segmentation_logger SegmentationLogger Arguments num_classes : int. Number of classes, required for confusion matrix num_visualizations : int, optional. Number of images to visualize. Defaults to 50. opacity : float. Opacity of segmentation overlay. Defaults to 0.5. log_train_images : bool. If true training images are visualized as well. Defaults to False. YAML Configuration callbacks: SegmentationLogger: num_classes: 7 num_visualizations: 200 Example","title":"SegmentationLogger class"},{"location":"dlf/callbacks/TensorboardLogger/","text":"TensorboardLogger class dlf.callbacks.tensorboard_logger.TensorboardLogger() A callback which logs every available metric to TensorBoard. Note This should be the base class for every TensorBoard logging related implementation Aliases tensorboard_logger TensorboardLogger callbacks: TensorboardLogger:","title":"TensorboardLogger"},{"location":"dlf/callbacks/TensorboardLogger/#tensorboardlogger-class","text":"dlf.callbacks.tensorboard_logger.TensorboardLogger() A callback which logs every available metric to TensorBoard. Note This should be the base class for every TensorBoard logging related implementation Aliases tensorboard_logger TensorboardLogger callbacks: TensorboardLogger:","title":"TensorboardLogger class"},{"location":"dlf/core/builder/","text":"build_callback function dlf.core.builder.build_callback(callback, args) Builds and initializes a callback object. Valid names are all framework callbacks located at frameworks/callback Args callback : str. Name of the registered callback args : dict[str, dict[str, Any]]. Arguments to initialize the callback Raises ValueError : If callback not exists ValueError : If initialization of callback went wrong Returns An instances of dlf.core.callback.Callback build_data_generator function dlf.core.builder.build_data_generator(name, args) Builds and initializes a data generator. Valid names are all registered data generators which are located at dlf/data_generators Args name : str. Name of the registered data generator args : dict[str, dict[str, Any]]. Keyword arguments for the data generator Raises FileNotFoundError : If there is no data reader for the given name ValueError : If the initialization of the data reader went wrong Returns A tf.data.Dataset object build_evaluator function dlf.core.builder.build_evaluator(evaluator, args) Builds and initializes an evaluator object. Valid names are all framework evaluators located at frameworks/evaluator Args evaluator : str. Name of the registered evaluator args : dict[str, dict[str, Any]]. Arguments to initialize the evaluator Raises ValueError : If evaluator not exists ValueError : If initialization of evaluator went wrong Returns An instances of dlf.core.evaluator.evaluator build_loss function dlf.core.builder.build_loss(name, args) Builds and initializes a loss function. Valid names are all in Keras available Losses and all losses in module dlf/losses (if registered) Args name : str. Name of the loss function args : dict[str, dict[str, Any]]. Arguments to initialize the loss function Raises ValueError : If loss function not exists Returns A tf.keras.losses.Loss object build_metric function dlf.core.builder.build_metric(name, kwargs) Builds and initializes a metric based on a given name for this metric and with given arguments. Valid names are all in Keras available Metrics and all metrics in module dlf/metrics (if registered) Args name (str): Name of the registered metric kwargs (dict): Arguments for the metric to initialize Raises ValueError : If the metric is not valid or not registered Returns A metric with tf.keras.metrics.Metric as base class build_model_wrapper function dlf.core.builder.build_model_wrapper(name, kwargs) Builds and initializes a model based on the name parameter with kwargs arguments. Valid names are all in Keras available Applications and all models in module dlf/models (if registered) Args name : str. Name of the model to initialize (Keras model or a framework model) kwargs : dict. Required argument to initialize a model Raises FileNotFoundError : If no model exists for a given name ValueError : If the registered model is not subclass of ModelWrapper or the preprocessing function is not callable ValueError : If something went wrong during the initialization with the kwargs Returns A ModelWrapper object containing a the specified model, training and validation step build_optimizer function dlf.core.builder.build_optimizer(optimizer, args) Builds and initializes an optimizer. Valid names are all in Keras available Optimizers Args optimizer : str. Name of the optimizer to initialize args : dict[str, dict[str, Any]]. Intialization arguments for the selected optimizier Raises ValueError : If optimizer name does not exists in tf.keras.optimizers ValueError : If an invalid decay learning object is used as learning_rate Returns An instance of tf.keras.optimizers.Optimiizer build_preprocessing_exectutor function dlf.core.builder.build_preprocessing_exectutor(pipeline) Builds and initializes a PreprocessingExecutor which contains a list of preprocessing methods. Valid names are all registered methods in dlf/preprocessing (if registered) Args pipeline : dict[str, dict[str, Any]. dictionary containing the name of a method as key and keyword arguments as values Raises ValueError : If the requested preprocessing method not exists Returns A dlf.core.preprocessing.PreprocessingExecutor objects","title":"builder"},{"location":"dlf/core/builder/#build_callback-function","text":"dlf.core.builder.build_callback(callback, args) Builds and initializes a callback object. Valid names are all framework callbacks located at frameworks/callback Args callback : str. Name of the registered callback args : dict[str, dict[str, Any]]. Arguments to initialize the callback Raises ValueError : If callback not exists ValueError : If initialization of callback went wrong Returns An instances of dlf.core.callback.Callback","title":"build_callback function"},{"location":"dlf/core/builder/#build_data_generator-function","text":"dlf.core.builder.build_data_generator(name, args) Builds and initializes a data generator. Valid names are all registered data generators which are located at dlf/data_generators Args name : str. Name of the registered data generator args : dict[str, dict[str, Any]]. Keyword arguments for the data generator Raises FileNotFoundError : If there is no data reader for the given name ValueError : If the initialization of the data reader went wrong Returns A tf.data.Dataset object","title":"build_data_generator function"},{"location":"dlf/core/builder/#build_evaluator-function","text":"dlf.core.builder.build_evaluator(evaluator, args) Builds and initializes an evaluator object. Valid names are all framework evaluators located at frameworks/evaluator Args evaluator : str. Name of the registered evaluator args : dict[str, dict[str, Any]]. Arguments to initialize the evaluator Raises ValueError : If evaluator not exists ValueError : If initialization of evaluator went wrong Returns An instances of dlf.core.evaluator.evaluator","title":"build_evaluator function"},{"location":"dlf/core/builder/#build_loss-function","text":"dlf.core.builder.build_loss(name, args) Builds and initializes a loss function. Valid names are all in Keras available Losses and all losses in module dlf/losses (if registered) Args name : str. Name of the loss function args : dict[str, dict[str, Any]]. Arguments to initialize the loss function Raises ValueError : If loss function not exists Returns A tf.keras.losses.Loss object","title":"build_loss function"},{"location":"dlf/core/builder/#build_metric-function","text":"dlf.core.builder.build_metric(name, kwargs) Builds and initializes a metric based on a given name for this metric and with given arguments. Valid names are all in Keras available Metrics and all metrics in module dlf/metrics (if registered) Args name (str): Name of the registered metric kwargs (dict): Arguments for the metric to initialize Raises ValueError : If the metric is not valid or not registered Returns A metric with tf.keras.metrics.Metric as base class","title":"build_metric function"},{"location":"dlf/core/builder/#build_model_wrapper-function","text":"dlf.core.builder.build_model_wrapper(name, kwargs) Builds and initializes a model based on the name parameter with kwargs arguments. Valid names are all in Keras available Applications and all models in module dlf/models (if registered) Args name : str. Name of the model to initialize (Keras model or a framework model) kwargs : dict. Required argument to initialize a model Raises FileNotFoundError : If no model exists for a given name ValueError : If the registered model is not subclass of ModelWrapper or the preprocessing function is not callable ValueError : If something went wrong during the initialization with the kwargs Returns A ModelWrapper object containing a the specified model, training and validation step","title":"build_model_wrapper function"},{"location":"dlf/core/builder/#build_optimizer-function","text":"dlf.core.builder.build_optimizer(optimizer, args) Builds and initializes an optimizer. Valid names are all in Keras available Optimizers Args optimizer : str. Name of the optimizer to initialize args : dict[str, dict[str, Any]]. Intialization arguments for the selected optimizier Raises ValueError : If optimizer name does not exists in tf.keras.optimizers ValueError : If an invalid decay learning object is used as learning_rate Returns An instance of tf.keras.optimizers.Optimiizer","title":"build_optimizer function"},{"location":"dlf/core/builder/#build_preprocessing_exectutor-function","text":"dlf.core.builder.build_preprocessing_exectutor(pipeline) Builds and initializes a PreprocessingExecutor which contains a list of preprocessing methods. Valid names are all registered methods in dlf/preprocessing (if registered) Args pipeline : dict[str, dict[str, Any]. dictionary containing the name of a method as key and keyword arguments as values Raises ValueError : If the requested preprocessing method not exists Returns A dlf.core.preprocessing.PreprocessingExecutor objects","title":"build_preprocessing_exectutor function"},{"location":"dlf/core/callback/","text":"Callback class dlf.core.callback.Callback(*args, **kwargs) Base class for a callback on_evaluation method Callback.on_evaluation(step, losses, metrics, target) Called at every N evaluation step. Args step : int. Current step losses : dict[str, float]. A dictionary containing loss name as key and the loss value as float metrics : dict[str, tf.keras.metrics.Metric]. A list of all available metrics target : dlf.core.experiment.ExperimentTarget. Target of this callback on_train_begin method Callback.on_train_begin() Called at the beginning of a training. on_train_end method Callback.on_train_end() Called at the end of a training. set_experiment method Callback.set_experiment(experiment) Sets a reference to the current experiment. Args experiment : Experiment. Current experiment","title":"callback"},{"location":"dlf/core/callback/#callback-class","text":"dlf.core.callback.Callback(*args, **kwargs) Base class for a callback","title":"Callback class"},{"location":"dlf/core/callback/#on_evaluation-method","text":"Callback.on_evaluation(step, losses, metrics, target) Called at every N evaluation step. Args step : int. Current step losses : dict[str, float]. A dictionary containing loss name as key and the loss value as float metrics : dict[str, tf.keras.metrics.Metric]. A list of all available metrics target : dlf.core.experiment.ExperimentTarget. Target of this callback","title":"on_evaluation method"},{"location":"dlf/core/callback/#on_train_begin-method","text":"Callback.on_train_begin() Called at the beginning of a training.","title":"on_train_begin method"},{"location":"dlf/core/callback/#on_train_end-method","text":"Callback.on_train_end() Called at the end of a training.","title":"on_train_end method"},{"location":"dlf/core/callback/#set_experiment-method","text":"Callback.set_experiment(experiment) Sets a reference to the current experiment. Args experiment : Experiment. Current experiment","title":"set_experiment method"},{"location":"dlf/core/data_generator/","text":"DataGenerator class dlf.core.data_generator.DataGenerator(name, dataset, labels, padded_batch_shape=None, padded_values=None) Abstract class to describe a data generation object In general a data generator is responsible to provide data for training, validation or test. For this porpuse, a DataGenerator object needs to contains a a tf.data.Dataset object as dataset. Args dataset : tf.data.Dataset. Dataset object labels : dict[int, str]. Labels for the dataset where keys are the label ids and the values are the label names","title":"data_generator"},{"location":"dlf/core/data_generator/#datagenerator-class","text":"dlf.core.data_generator.DataGenerator(name, dataset, labels, padded_batch_shape=None, padded_values=None) Abstract class to describe a data generation object In general a data generator is responsible to provide data for training, validation or test. For this porpuse, a DataGenerator object needs to contains a a tf.data.Dataset object as dataset. Args dataset : tf.data.Dataset. Dataset object labels : dict[int, str]. Labels for the dataset where keys are the label ids and the values are the label names","title":"DataGenerator class"},{"location":"dlf/core/evaluator/","text":"Evaluator class dlf.core.evaluator.Evaluator(*args, **kwargs) ObjectDetectionEvaluator class dlf.core.evaluator.ObjectDetectionEvaluator(*args, **kwargs)","title":"evaluator"},{"location":"dlf/core/evaluator/#evaluator-class","text":"dlf.core.evaluator.Evaluator(*args, **kwargs)","title":"Evaluator class"},{"location":"dlf/core/evaluator/#objectdetectionevaluator-class","text":"dlf.core.evaluator.ObjectDetectionEvaluator(*args, **kwargs)","title":"ObjectDetectionEvaluator class"},{"location":"dlf/core/experiment/","text":"Experiment class dlf.core.experiment.Experiment(config) A representation of an experiment This class represents a setup of an experiment. Based on *.yaml configuration file this class maps these values and initializes all required elements. Args config : str. Path to a configuration file Raises FileNotFoundError : If the specified configuration file does not exists KeyError : If required keys are missing in a configuration file ExperimentTarget class dlf.core.experiment.ExperimentTarget(*args, **kwargs) Enum to identify the current state during the experiment InputReaderSettings class dlf.core.experiment.InputReaderSettings(settings) Container for all selected input readers Args settings : dict[str,Any]. Dictionary of input_reader section YAML Configuration input_reader: training_reader: name: fs_random_unpaired_reader path_lhs: /mnt/data/datasets/theodore_wheeled_walker/*_img.png path_rhs: /mnt/data/datasets/omnidetector-Flat/JPEGImages/*.jpg lhs_limit: 10000 rhs_limit: 10000 shuffle_buffer: 100 preprocess_list: resize: output_shape: - 512 - 512 set_batch_size method InputReaderSettings.set_batch_size(batch_size) Updates the batch size of all input readers Args batch_size : int. Size of a batch ModelSettings class dlf.core.experiment.ModelSettings(settings) Object to describe model specific settings Args settings : dict[str,Any]. Dictionary to initialize a model Raises KeyError : If a specified model is not available or the setup is wrong SaveStrategy class dlf.core.experiment.SaveStrategy(monitor, mode) Represents a saving strategy for a model during training This class implements a simple saving strategy to save models during training. With the configuration file you have the opportunity to specify different metrics and losses. Each of them have an unique name and regarding the evaluation target ... an alias will be prepended. For instance you setup the metric SparseMeanIOU. The value of this metric will be propagated through all evaluation steps. For training the monitor value will be train_sparse_mean_iou and for validation val_sparse_mean_iou . As second parameter you have to specify whether to save the model if the min or max value of this metric improves during training. Args monitor : str. Value to monitor during training mode : {'max','min'}. Whether to save the model if the minimum value or maximum value improves need_to_save method SaveStrategy.need_to_save(monitor_values) Method which decides whether a model should be saved or not Args monitor_values : dict[str,float]. Dictionary containing monitor values Returns bool : Describes whether a model has been improved and needs to be saved or not. TrainingSettings class dlf.core.experiment.TrainingSettings(settings) Object to describe the selected training settings This object parses the training section of a config file. Based on the configuration callbacks and metrics are initialized for the experiment. Args settings : dict[key, Any]. The training section of configuration file Raises Exception : If the monitor value is not specified KeyError : If the configuration files contains not all required keys for the training section generate_metrics method TrainingSettings.generate_metrics() Generate instances of the selected and available metrics Returns A List of instances of selected metrics","title":"experiment"},{"location":"dlf/core/experiment/#experiment-class","text":"dlf.core.experiment.Experiment(config) A representation of an experiment This class represents a setup of an experiment. Based on *.yaml configuration file this class maps these values and initializes all required elements. Args config : str. Path to a configuration file Raises FileNotFoundError : If the specified configuration file does not exists KeyError : If required keys are missing in a configuration file","title":"Experiment class"},{"location":"dlf/core/experiment/#experimenttarget-class","text":"dlf.core.experiment.ExperimentTarget(*args, **kwargs) Enum to identify the current state during the experiment","title":"ExperimentTarget class"},{"location":"dlf/core/experiment/#inputreadersettings-class","text":"dlf.core.experiment.InputReaderSettings(settings) Container for all selected input readers Args settings : dict[str,Any]. Dictionary of input_reader section YAML Configuration input_reader: training_reader: name: fs_random_unpaired_reader path_lhs: /mnt/data/datasets/theodore_wheeled_walker/*_img.png path_rhs: /mnt/data/datasets/omnidetector-Flat/JPEGImages/*.jpg lhs_limit: 10000 rhs_limit: 10000 shuffle_buffer: 100 preprocess_list: resize: output_shape: - 512 - 512","title":"InputReaderSettings class"},{"location":"dlf/core/experiment/#set_batch_size-method","text":"InputReaderSettings.set_batch_size(batch_size) Updates the batch size of all input readers Args batch_size : int. Size of a batch","title":"set_batch_size method"},{"location":"dlf/core/experiment/#modelsettings-class","text":"dlf.core.experiment.ModelSettings(settings) Object to describe model specific settings Args settings : dict[str,Any]. Dictionary to initialize a model Raises KeyError : If a specified model is not available or the setup is wrong","title":"ModelSettings class"},{"location":"dlf/core/experiment/#savestrategy-class","text":"dlf.core.experiment.SaveStrategy(monitor, mode) Represents a saving strategy for a model during training This class implements a simple saving strategy to save models during training. With the configuration file you have the opportunity to specify different metrics and losses. Each of them have an unique name and regarding the evaluation target ... an alias will be prepended. For instance you setup the metric SparseMeanIOU. The value of this metric will be propagated through all evaluation steps. For training the monitor value will be train_sparse_mean_iou and for validation val_sparse_mean_iou . As second parameter you have to specify whether to save the model if the min or max value of this metric improves during training. Args monitor : str. Value to monitor during training mode : {'max','min'}. Whether to save the model if the minimum value or maximum value improves","title":"SaveStrategy class"},{"location":"dlf/core/experiment/#need_to_save-method","text":"SaveStrategy.need_to_save(monitor_values) Method which decides whether a model should be saved or not Args monitor_values : dict[str,float]. Dictionary containing monitor values Returns bool : Describes whether a model has been improved and needs to be saved or not.","title":"need_to_save method"},{"location":"dlf/core/experiment/#trainingsettings-class","text":"dlf.core.experiment.TrainingSettings(settings) Object to describe the selected training settings This object parses the training section of a config file. Based on the configuration callbacks and metrics are initialized for the experiment. Args settings : dict[key, Any]. The training section of configuration file Raises Exception : If the monitor value is not specified KeyError : If the configuration files contains not all required keys for the training section","title":"TrainingSettings class"},{"location":"dlf/core/experiment/#generate_metrics-method","text":"TrainingSettings.generate_metrics() Generate instances of the selected and available metrics Returns A List of instances of selected metrics","title":"generate_metrics method"},{"location":"dlf/core/model/","text":"ModelWrapper class dlf.core.model.ModelWrapper( model, preprocessing=None, optimizer=None, loss=None, model_weights=None, additional_models=[], is_finetune=False, ) Base class which wraps all necessary objects and methods to make training feasible Args model : tf.keras.models.Model. The model which should be wrapped preprocessing : callable, optional. A preprocessing function which will be executed. Defaults to None. optimizer : tf.keras.optimizers.Optimizer, optional. An optimizer used for training. Defaults to None. loss : dict[str,dict[str, Any]], optional. A loss function which should be build to use for training. Defaults to None. init_checkpoint method ModelWrapper.init_checkpoint(skip_optimizer=False) Initialize a checkpoint object for saving and restoring num_iterations method ModelWrapper.num_iterations() Get current number of iterations done by the optimizer Returns int. Number of iterations preprocess_input_wrapper method ModelWrapper.preprocess_input_wrapper(x) Model specific prerprocessing function This methods normalizes an input image in range -1 to 1 and is executedbefore it is feeds into a model Args x : tf.Tensor. Image which should be normalized Returns tf.Tensor : normalized image restore_model method ModelWrapper.restore_model(path, step=None) Restores a checkpoint at a given path Arguments path : str. Path where the checkpoints are stored to restore step : int, optional. Sepcifies a checkpoint at step X which should be restored. Defaults to None. save_model method ModelWrapper.save_model(path, step) Methods which handles a request to save a model Args path : str. Path where the checkpoints should be stored step : int. Current step tape_step method ModelWrapper.tape_step(tape, loss_values) Executed during training to backpropagate loss values Args tape : Instance of GradientTape loss_values : dict[str, List[tf.Tensor]]. Dictionary containing the name of a loss and the corresponding values Raises ValueError : If no optimizer is available Returns List[tf.Tensor] : List of gradients training_step method ModelWrapper.training_step(record) Executet during training for each batch of data This method is executed during training for each batch which should be feed into the network to train it. Args record : dict. Training related data which contain the key 'x_batch' and 'y_batch' Raises ValueError : If the ModelWrapper contains no loss function Returns tuple(dict[str,Any], tf.Tensor): A dictionary with all assigned losses and the predicted logits validation_step method ModelWrapper.validation_step(record) Exectued during validation See training_step Args record : dict. Training related data which contain the key 'x_batch' and 'y_batch' Raises ValueError : If the ModelWrapper contains no loss function Returns tuple(dict[str,Any], tf.Tensor): A dictionary with all assigned losses and the predicted logits","title":"model"},{"location":"dlf/core/model/#modelwrapper-class","text":"dlf.core.model.ModelWrapper( model, preprocessing=None, optimizer=None, loss=None, model_weights=None, additional_models=[], is_finetune=False, ) Base class which wraps all necessary objects and methods to make training feasible Args model : tf.keras.models.Model. The model which should be wrapped preprocessing : callable, optional. A preprocessing function which will be executed. Defaults to None. optimizer : tf.keras.optimizers.Optimizer, optional. An optimizer used for training. Defaults to None. loss : dict[str,dict[str, Any]], optional. A loss function which should be build to use for training. Defaults to None.","title":"ModelWrapper class"},{"location":"dlf/core/model/#init_checkpoint-method","text":"ModelWrapper.init_checkpoint(skip_optimizer=False) Initialize a checkpoint object for saving and restoring","title":"init_checkpoint method"},{"location":"dlf/core/model/#num_iterations-method","text":"ModelWrapper.num_iterations() Get current number of iterations done by the optimizer Returns int. Number of iterations","title":"num_iterations method"},{"location":"dlf/core/model/#preprocess_input_wrapper-method","text":"ModelWrapper.preprocess_input_wrapper(x) Model specific prerprocessing function This methods normalizes an input image in range -1 to 1 and is executedbefore it is feeds into a model Args x : tf.Tensor. Image which should be normalized Returns tf.Tensor : normalized image","title":"preprocess_input_wrapper method"},{"location":"dlf/core/model/#restore_model-method","text":"ModelWrapper.restore_model(path, step=None) Restores a checkpoint at a given path Arguments path : str. Path where the checkpoints are stored to restore step : int, optional. Sepcifies a checkpoint at step X which should be restored. Defaults to None.","title":"restore_model method"},{"location":"dlf/core/model/#save_model-method","text":"ModelWrapper.save_model(path, step) Methods which handles a request to save a model Args path : str. Path where the checkpoints should be stored step : int. Current step","title":"save_model method"},{"location":"dlf/core/model/#tape_step-method","text":"ModelWrapper.tape_step(tape, loss_values) Executed during training to backpropagate loss values Args tape : Instance of GradientTape loss_values : dict[str, List[tf.Tensor]]. Dictionary containing the name of a loss and the corresponding values Raises ValueError : If no optimizer is available Returns List[tf.Tensor] : List of gradients","title":"tape_step method"},{"location":"dlf/core/model/#training_step-method","text":"ModelWrapper.training_step(record) Executet during training for each batch of data This method is executed during training for each batch which should be feed into the network to train it. Args record : dict. Training related data which contain the key 'x_batch' and 'y_batch' Raises ValueError : If the ModelWrapper contains no loss function Returns tuple(dict[str,Any], tf.Tensor): A dictionary with all assigned losses and the predicted logits","title":"training_step method"},{"location":"dlf/core/model/#validation_step-method","text":"ModelWrapper.validation_step(record) Exectued during validation See training_step Args record : dict. Training related data which contain the key 'x_batch' and 'y_batch' Raises ValueError : If the ModelWrapper contains no loss function Returns tuple(dict[str,Any], tf.Tensor): A dictionary with all assigned losses and the predicted logits","title":"validation_step method"},{"location":"dlf/core/preprocessing/","text":"PreprocessingExecutor class dlf.core.preprocessing.PreprocessingExecutor(preprocessing_methods) Object that applies a list of preprocessing methods to images,masks and boxes Arguments preprocessing_methods : [PreprocessingMethod]. List of preprocessing methods which should be applied PreprocessingJob class dlf.core.preprocessing.PreprocessingJob(image, mask=None, boxes=None) A container that hosts images, masks and bounding boxes for preprocessing Arguments image : tf.tensor. Image that should be preprocessed mask : tf.tensor, optional. Mask that should be preprocessed. Defaults to None. boxes : tf.tensor, optional. Bounding Boxes to preprocess. Defaults to None. PreprocessingMethod class dlf.core.preprocessing.PreprocessingMethod() Abstract class to describe a Preprocessing Object In general a preprocessing method is executed for an image. Optional you can pass a mask or a pair image to apply the same transformation like it is done for the image. call method PreprocessingMethod.__call__(processing_job: dlf.core.preprocessing.PreprocessingJob) Executed during preprocessing step Args processing_job : PreprocessingJob. Job that should be preprocessed","title":"preprocessing"},{"location":"dlf/core/preprocessing/#preprocessingexecutor-class","text":"dlf.core.preprocessing.PreprocessingExecutor(preprocessing_methods) Object that applies a list of preprocessing methods to images,masks and boxes Arguments preprocessing_methods : [PreprocessingMethod]. List of preprocessing methods which should be applied","title":"PreprocessingExecutor class"},{"location":"dlf/core/preprocessing/#preprocessingjob-class","text":"dlf.core.preprocessing.PreprocessingJob(image, mask=None, boxes=None) A container that hosts images, masks and bounding boxes for preprocessing Arguments image : tf.tensor. Image that should be preprocessed mask : tf.tensor, optional. Mask that should be preprocessed. Defaults to None. boxes : tf.tensor, optional. Bounding Boxes to preprocess. Defaults to None.","title":"PreprocessingJob class"},{"location":"dlf/core/preprocessing/#preprocessingmethod-class","text":"dlf.core.preprocessing.PreprocessingMethod() Abstract class to describe a Preprocessing Object In general a preprocessing method is executed for an image. Optional you can pass a mask or a pair image to apply the same transformation like it is done for the image.","title":"PreprocessingMethod class"},{"location":"dlf/core/preprocessing/#call-method","text":"PreprocessingMethod.__call__(processing_job: dlf.core.preprocessing.PreprocessingJob) Executed during preprocessing step Args processing_job : PreprocessingJob. Job that should be preprocessed","title":"call method"},{"location":"dlf/core/registry/","text":"get_active_experiment function dlf.core.registry.get_active_experiment() Gets the current, active, experiment Returns dlf.core.Experiment. Active experiment import_framework_modules function dlf.core.registry.import_framework_modules(module_folder, package) Auto import of all files in module folder Note This is necessary for the register_* decorator to work properly. Args module_folder : str.path to folder where files to import are located package : str. module path e.g. dlf.metrics register_callback function dlf.core.registry.register_callback(*names) Decorator to register a callback to the framework Args *names : Tuple(str). List of aliases for this callback Raises ValueError : If a given alias is not valid register_data_generator function dlf.core.registry.register_data_generator(*names) Decorator to register a data reader to the framework Args *names : Tuple(str). List of aliases for this data reader Raises ValueError : If a given alias is not valid register_evaluator function dlf.core.registry.register_evaluator(*names) Decorator to register an evaluator to the framework Args *names : Tuple(str). List of aliases for this evaluator Raises ValueError : If a given alias is not valid register_loss function dlf.core.registry.register_loss(*names) Decorator to register a custom loss to the framework Args *names : Tuple(str) List of aliases for this loss Raises Exception : If object is not subclass of tf.keras.losses.Loss ValueError : If a given alias is not valid register_metric function dlf.core.registry.register_metric(*names) Decorator to register a custom metric to the framework Args *names : Tuple(str). List of aliases for this metric Raises ValueError : If the parent of this method is not of type tf.keras.metrics.Metrics ValueError : If a given alias is not valid register_model function dlf.core.registry.register_model(*names) Decorator to register a custom model to the framework Args *names : Tuple(str). List of aliases for this model Raises ValueError : If a given alias is not valid register_preprocessing_method function dlf.core.registry.register_preprocessing_method(*names) Decorator to register a preprocessing object to the framework Args *names : Tuple(str). List of aliases for this preprocessing object Raises ValueError : If the parent of this method is not of type PreprocessingMethod set_active_experiment function dlf.core.registry.set_active_experiment(exp) Sets active experiment to global state and allows all modules to access it Arguments exp : dlf.core.Experiment. Active experiment","title":"registry"},{"location":"dlf/core/registry/#get_active_experiment-function","text":"dlf.core.registry.get_active_experiment() Gets the current, active, experiment Returns dlf.core.Experiment. Active experiment","title":"get_active_experiment function"},{"location":"dlf/core/registry/#import_framework_modules-function","text":"dlf.core.registry.import_framework_modules(module_folder, package) Auto import of all files in module folder Note This is necessary for the register_* decorator to work properly. Args module_folder : str.path to folder where files to import are located package : str. module path e.g. dlf.metrics","title":"import_framework_modules function"},{"location":"dlf/core/registry/#register_callback-function","text":"dlf.core.registry.register_callback(*names) Decorator to register a callback to the framework Args *names : Tuple(str). List of aliases for this callback Raises ValueError : If a given alias is not valid","title":"register_callback function"},{"location":"dlf/core/registry/#register_data_generator-function","text":"dlf.core.registry.register_data_generator(*names) Decorator to register a data reader to the framework Args *names : Tuple(str). List of aliases for this data reader Raises ValueError : If a given alias is not valid","title":"register_data_generator function"},{"location":"dlf/core/registry/#register_evaluator-function","text":"dlf.core.registry.register_evaluator(*names) Decorator to register an evaluator to the framework Args *names : Tuple(str). List of aliases for this evaluator Raises ValueError : If a given alias is not valid","title":"register_evaluator function"},{"location":"dlf/core/registry/#register_loss-function","text":"dlf.core.registry.register_loss(*names) Decorator to register a custom loss to the framework Args *names : Tuple(str) List of aliases for this loss Raises Exception : If object is not subclass of tf.keras.losses.Loss ValueError : If a given alias is not valid","title":"register_loss function"},{"location":"dlf/core/registry/#register_metric-function","text":"dlf.core.registry.register_metric(*names) Decorator to register a custom metric to the framework Args *names : Tuple(str). List of aliases for this metric Raises ValueError : If the parent of this method is not of type tf.keras.metrics.Metrics ValueError : If a given alias is not valid","title":"register_metric function"},{"location":"dlf/core/registry/#register_model-function","text":"dlf.core.registry.register_model(*names) Decorator to register a custom model to the framework Args *names : Tuple(str). List of aliases for this model Raises ValueError : If a given alias is not valid","title":"register_model function"},{"location":"dlf/core/registry/#register_preprocessing_method-function","text":"dlf.core.registry.register_preprocessing_method(*names) Decorator to register a preprocessing object to the framework Args *names : Tuple(str). List of aliases for this preprocessing object Raises ValueError : If the parent of this method is not of type PreprocessingMethod","title":"register_preprocessing_method function"},{"location":"dlf/core/registry/#set_active_experiment-function","text":"dlf.core.registry.set_active_experiment(exp) Sets active experiment to global state and allows all modules to access it Arguments exp : dlf.core.Experiment. Active experiment","title":"set_active_experiment function"},{"location":"dlf/data_generators/FsImageNetLikeReader/","text":"FsImageNetLikeReader class dlf.data_generators.fs_imagenet_like_reader.FsImageNetLikeReader( path, is_simclr=False, categorical_labels=False, preprocess_list=None, shuffle=True, shuffle_buffer=100 ) A data generator which reads imagenet like file structures - folder - folder class1 - file 1 - file 2 - ... - folder class2 - file 1 - file 2 - ... - .... Aliases FsImageNetLikeReader fs_imagenet_like_reader Arguments path : str. glob pattern to find images is_simclr : bool. If true, for each image a replicat with additional augmentation is generated. Defaults to False. categorical_labels : bool. If true, labes are one-hot encoded. Defaults to False. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If path is not valid YAML Configuration input_reader: training_reader: name: fs_imagenet_like_reader path: /mnt/data/datasets/openimages/semisupervised/filtered_62/train/**/*.jpg categorical_labels: True is_simclr: True preprocess_list: h_flip: color_distortion: s: 1.0 color_jitter_probability: 0.5 color_drop_probability: 0.3 resize: width: 224 height: 224","title":"FsImageNetLikeReader"},{"location":"dlf/data_generators/FsImageNetLikeReader/#fsimagenetlikereader-class","text":"dlf.data_generators.fs_imagenet_like_reader.FsImageNetLikeReader( path, is_simclr=False, categorical_labels=False, preprocess_list=None, shuffle=True, shuffle_buffer=100 ) A data generator which reads imagenet like file structures - folder - folder class1 - file 1 - file 2 - ... - folder class2 - file 1 - file 2 - ... - .... Aliases FsImageNetLikeReader fs_imagenet_like_reader Arguments path : str. glob pattern to find images is_simclr : bool. If true, for each image a replicat with additional augmentation is generated. Defaults to False. categorical_labels : bool. If true, labes are one-hot encoded. Defaults to False. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If path is not valid YAML Configuration input_reader: training_reader: name: fs_imagenet_like_reader path: /mnt/data/datasets/openimages/semisupervised/filtered_62/train/**/*.jpg categorical_labels: True is_simclr: True preprocess_list: h_flip: color_distortion: s: 1.0 color_jitter_probability: 0.5 color_drop_probability: 0.3 resize: width: 224 height: 224","title":"FsImageNetLikeReader class"},{"location":"dlf/data_generators/FsRandomUnpairedReader/","text":"FsRandomUnpairedReader class dlf.data_generators.fs_pair_reader.FsRandomUnpairedReader( paths_lhs, paths_rhs, channels=3, lhs_limit=None, rhs_limit=None, preprocess_list=None, shuffle_buffer=100 ) A data generator which generates random pairs of images Aliases FsRandomUnpairedReader fs_random_unpaired_reader Arguments paths_lhs : list of str. List of glob patterns to find images for the left hand side paths_rhs : list of str. List of glob patterns to find images for the right hand side channels : int. Number of channels which each image should contain. Defaults to 3. lhs_limit : int. If specified N elements of lhs set are selected. Defaults to None. rhs_limit : int. If specified N elements of rhs set are selected. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If one path of the lhs path set is not valid FileNotFoundError : If one path of the rhs path set is not valid ValueError : If crop or resize is not specified in preprocessing pipeline YAML Configuration input_reader: training_reader: name: fs_random_unpaired_reader paths_lhs: /mnt/data/datasets/theodore_wheeled_walker/*_img.png paths_rhs: - /mnt/data/datasets/omnidetector-Flat/JPEGImages/*.jpg - /mnt/data/datasets/indoor_cvpr/Images/**/*.jpg lhs_limit: 20000 rhs_limit: 20000 shuffle_buffer: 400 preprocess_list: crop: width: 512 height: 512","title":"FsRandomUnpairedReader"},{"location":"dlf/data_generators/FsRandomUnpairedReader/#fsrandomunpairedreader-class","text":"dlf.data_generators.fs_pair_reader.FsRandomUnpairedReader( paths_lhs, paths_rhs, channels=3, lhs_limit=None, rhs_limit=None, preprocess_list=None, shuffle_buffer=100 ) A data generator which generates random pairs of images Aliases FsRandomUnpairedReader fs_random_unpaired_reader Arguments paths_lhs : list of str. List of glob patterns to find images for the left hand side paths_rhs : list of str. List of glob patterns to find images for the right hand side channels : int. Number of channels which each image should contain. Defaults to 3. lhs_limit : int. If specified N elements of lhs set are selected. Defaults to None. rhs_limit : int. If specified N elements of rhs set are selected. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If one path of the lhs path set is not valid FileNotFoundError : If one path of the rhs path set is not valid ValueError : If crop or resize is not specified in preprocessing pipeline YAML Configuration input_reader: training_reader: name: fs_random_unpaired_reader paths_lhs: /mnt/data/datasets/theodore_wheeled_walker/*_img.png paths_rhs: - /mnt/data/datasets/omnidetector-Flat/JPEGImages/*.jpg - /mnt/data/datasets/indoor_cvpr/Images/**/*.jpg lhs_limit: 20000 rhs_limit: 20000 shuffle_buffer: 400 preprocess_list: crop: width: 512 height: 512","title":"FsRandomUnpairedReader class"},{"location":"dlf/data_generators/TfRecordSSDReader/","text":"TfRecordSSDReader class dlf.data_generators.tf_record_object_detection_reader.TfRecordSSDReader( labelmap, path=None, glob_pattern=None, image_key=\"image/encoded\", bounding_box_key=\"image/object/bbox/\", label_key=\"image/object/class/label\", id_key=\"image/source_id\", background_as_zero=True, shuffle=True, ignore=None, remap=None, preprocess_list=None, shuffle_buffer=2000, ) A data generator which uses the TFRecord format for object detection tasks using SSD architectures Arguments path : str. Path to TFRecord file. Defaults to None. glob_pattern : str. Glob pattern for multiple TFRecord files. This is the case if a record file was sharded over multiple files. Defaults to None. labelmap : str. Path to labelmap related to TFRecord file image_key : str. Key where the images are located in tf record example. Defaults to 'image/encoded'. bounding_box_key : str. Key where the bounding boxes are located in tf record example Defaults to 'image/object/bbox/'. label_key : str. Key where the labes are located. Defaults to 'image/object/class/label'. id_key : str. Key where the images ids are lcoated. Defaults to 'image/source_id'. background_as_zero : bool. If true, background with 0 zero will be added to label list. Defaults to True. shuffle : bool. If true, dataset is shuffled. Defaults to True. ignore : list of int. A list of integers where each number is ignored during validation e.g. label 255 in PascalVOC. Defaults to None. remap : dict[int, int]. Dictionary where the key is the source category which is remaped to the value, target id. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If TFRecord file is not found FileNotFoundError : If Labelmap file is not found YAML Configuration Sample configuration for a object detection reader that applies remapping and uses a list of pre-processing functions before passing the image to CNN. input_reader: training_reader: name: tf_record_ssd_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: 1:0 2:0 3:0 5:1 6:1 7:1 preprocess_list: h_flip: v_flip: resize: output_shape: - 512 - 512 saturation: brightness: max_delta: 0.6 blur: noise: mean: 0 std: 5","title":"TfRecordSSDReader"},{"location":"dlf/data_generators/TfRecordSSDReader/#tfrecordssdreader-class","text":"dlf.data_generators.tf_record_object_detection_reader.TfRecordSSDReader( labelmap, path=None, glob_pattern=None, image_key=\"image/encoded\", bounding_box_key=\"image/object/bbox/\", label_key=\"image/object/class/label\", id_key=\"image/source_id\", background_as_zero=True, shuffle=True, ignore=None, remap=None, preprocess_list=None, shuffle_buffer=2000, ) A data generator which uses the TFRecord format for object detection tasks using SSD architectures Arguments path : str. Path to TFRecord file. Defaults to None. glob_pattern : str. Glob pattern for multiple TFRecord files. This is the case if a record file was sharded over multiple files. Defaults to None. labelmap : str. Path to labelmap related to TFRecord file image_key : str. Key where the images are located in tf record example. Defaults to 'image/encoded'. bounding_box_key : str. Key where the bounding boxes are located in tf record example Defaults to 'image/object/bbox/'. label_key : str. Key where the labes are located. Defaults to 'image/object/class/label'. id_key : str. Key where the images ids are lcoated. Defaults to 'image/source_id'. background_as_zero : bool. If true, background with 0 zero will be added to label list. Defaults to True. shuffle : bool. If true, dataset is shuffled. Defaults to True. ignore : list of int. A list of integers where each number is ignored during validation e.g. label 255 in PascalVOC. Defaults to None. remap : dict[int, int]. Dictionary where the key is the source category which is remaped to the value, target id. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If TFRecord file is not found FileNotFoundError : If Labelmap file is not found YAML Configuration Sample configuration for a object detection reader that applies remapping and uses a list of pre-processing functions before passing the image to CNN. input_reader: training_reader: name: tf_record_ssd_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: 1:0 2:0 3:0 5:1 6:1 7:1 preprocess_list: h_flip: v_flip: resize: output_shape: - 512 - 512 saturation: brightness: max_delta: 0.6 blur: noise: mean: 0 std: 5","title":"TfRecordSSDReader class"},{"location":"dlf/data_generators/TfRecordSegmentationReader/","text":"TfRecordSegmentationReader class dlf.data_generators.tf_record_segmentation_reader.TfRecordSegmentationReader( path, labelmap, background_as_zero=True, shuffle=True, ignore=None, remap=None, preprocess_list=None, shuffle_buffer=2000, ) A data generator for segmentation tasks which uses the TFRecord format Arguments path : str. Path to TFRecord file labelmap : str. Path to labelmap related to TFRecord file background_as_zero : bool. If true, the ID 0 is discribing the background. Defaults to True. shuffle : bool. If true, dataset is shuffled. Defaults to True. ignore : list of int. A list of integers where each number is ignored during validation e.g. label 255 in PascalVOC. Defaults to None. remap : dict[int, int]. Dictionary where the key is the source category which is remaped to the value, target id. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If TFRecord file is not found FileNotFoundError : If Labelmap file is not found YAML Configuration Sample configuration for a segmentation reader that applies remapping and uses a list of pre-processing functions before passing the image to CNN. input_reader: training_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: 1:0 2:0 3:0 5:1 6:1 7:1 preprocess_list: h_flip: v_flip: resize: output_shape: - 512 - 512 saturation: brightness: max_delta: 0.6 blur: noise: mean: 0 std: 5","title":"TfRecordSegmentationReader"},{"location":"dlf/data_generators/TfRecordSegmentationReader/#tfrecordsegmentationreader-class","text":"dlf.data_generators.tf_record_segmentation_reader.TfRecordSegmentationReader( path, labelmap, background_as_zero=True, shuffle=True, ignore=None, remap=None, preprocess_list=None, shuffle_buffer=2000, ) A data generator for segmentation tasks which uses the TFRecord format Arguments path : str. Path to TFRecord file labelmap : str. Path to labelmap related to TFRecord file background_as_zero : bool. If true, the ID 0 is discribing the background. Defaults to True. shuffle : bool. If true, dataset is shuffled. Defaults to True. ignore : list of int. A list of integers where each number is ignored during validation e.g. label 255 in PascalVOC. Defaults to None. remap : dict[int, int]. Dictionary where the key is the source category which is remaped to the value, target id. Defaults to None. preprocess_list : dict. Dictionary where the value describes a preprocessing method and the values are the arguments. Defaults to None. shuffle_buffer : int. Size of shuffle buffer. For details check: tf.data.Dataset . Defaults to 2000. Raises FileNotFoundError : If TFRecord file is not found FileNotFoundError : If Labelmap file is not found YAML Configuration Sample configuration for a segmentation reader that applies remapping and uses a list of pre-processing functions before passing the image to CNN. input_reader: training_reader: name: tf_record_segmentation_reader path: /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/25k_sample_4_mask/training.tfrecord labelmap: &labelmap /mnt/data/datasets/wheeled_walker_100k_8fps_25switch/label_map.pbtxt ignore: remap: 1:0 2:0 3:0 5:1 6:1 7:1 preprocess_list: h_flip: v_flip: resize: output_shape: - 512 - 512 saturation: brightness: max_delta: 0.6 blur: noise: mean: 0 std: 5","title":"TfRecordSegmentationReader class"},{"location":"dlf/evaluators/CocoObjectDectectionEvaluator/","text":"CocoObjectDectectionEvaluator class dlf.evaluators.coco_object_detection.CocoObjectDectectionEvaluator(per_class=True) Evaluator object that evaluates object detection results based on MS COCO metrics. Arguments per_class : bool. If true, all metrics are additionally logged for each class. Defaults to True. Raises ValueError : If pycocotools are not installed","title":"CocoObjectDectectionEvaluator"},{"location":"dlf/evaluators/CocoObjectDectectionEvaluator/#cocoobjectdectectionevaluator-class","text":"dlf.evaluators.coco_object_detection.CocoObjectDectectionEvaluator(per_class=True) Evaluator object that evaluates object detection results based on MS COCO metrics. Arguments per_class : bool. If true, all metrics are additionally logged for each class. Defaults to True. Raises ValueError : If pycocotools are not installed","title":"CocoObjectDectectionEvaluator class"},{"location":"dlf/losses/CenterNetLoss/","text":"CenterNetLoss class dlf.losses.centernet_loss.CenterNetLoss(loss_scale=0.1, loss_offset=1.0) Implementation of CenterNet loss Aliases CenternetLoss centernetloss center_net_loss centernet_loss Arguments loss_scale : int. Scaling factor for the loss loss_offset : float. Offset factor for the loss YAML Configuration loss: centernetloss: loss_scale: 0.1 loss_offset: 1.0 References Objects as Points","title":"CenterNetLoss"},{"location":"dlf/losses/CenterNetLoss/#centernetloss-class","text":"dlf.losses.centernet_loss.CenterNetLoss(loss_scale=0.1, loss_offset=1.0) Implementation of CenterNet loss Aliases CenternetLoss centernetloss center_net_loss centernet_loss Arguments loss_scale : int. Scaling factor for the loss loss_offset : float. Offset factor for the loss YAML Configuration loss: centernetloss: loss_scale: 0.1 loss_offset: 1.0 References Objects as Points","title":"CenterNetLoss class"},{"location":"dlf/losses/Keras/","text":"Usage in framework All in tf.keras.losses available loss functions can be used in our framework. Just use the class name as configuration key and the arguments as as dict. YAML Configuration E.g. for tf.losses.MeanAbsoluteError without no arguments loss: MeanSquaredError: or for tf.losses.Huber with arguments: loss: Huber: delta: 0.3 Keras loss functions BinaryCrossentropy class tensorflow.keras.losses.BinaryCrossentropy( from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\" ) Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. bce = tf.keras.losses.BinaryCrossentropy() bce(y_true, y_pred).numpy() 0.815 Calling with 'sample_weight'. bce(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.458 Using 'sum' reduction type. bce = tf.keras.losses.BinaryCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) bce(y_true, y_pred).numpy() 1.630 Using 'none' reduction type. bce = tf.keras.losses.BinaryCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) bce(y_true, y_pred).numpy() array([0.916 , 0.714], dtype=float32) Usage with the tf.keras API: model.compile(optimizer='sgd', loss=tf.keras.losses.BinaryCrossentropy()) CategoricalCrossentropy class tensorflow.keras.losses.CategoricalCrossentropy( from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"categorical_crossentropy\" ) Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] Using 'auto'/'sum_over_batch_size' reduction type. cce = tf.keras.losses.CategoricalCrossentropy() cce(y_true, y_pred).numpy() 1.177 Calling with 'sample_weight'. cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy() 0.814 Using 'sum' reduction type. cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) cce(y_true, y_pred).numpy() 2.354 Using 'none' reduction type. cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) cce(y_true, y_pred).numpy() array([0.0513, 2.303], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy()) CategoricalHinge class tensorflow.keras.losses.CategoricalHinge(reduction=\"auto\", name=\"categorical_hinge\") Computes the categorical hinge loss between y_true and y_pred . loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred) Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. h = tf.keras.losses.CategoricalHinge() h(y_true, y_pred).numpy() 1.4 Calling with 'sample_weight'. h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.6 Using 'sum' reduction type. h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 2.8 Using 'none' reduction type. h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.2, 1.6], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge()) CosineSimilarity class tensorflow.keras.losses.CosineSimilarity(axis=-1, reduction=\"auto\", name=\"cosine_similarity\") Computes the cosine similarity between labels and predictions. Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where you try to maximize the proximity between predictions and targets. If either y_true or y_pred is a zero vector, cosine similarity will be 0 regardless of the proximity between predictions and targets. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Standalone usage: y_true = [[0., 1.], [1., 1.]] y_pred = [[1., 0.], [1., 1.]] Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = tf.keras.losses.CosineSimilarity(axis=1) l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]] l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]] l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]] loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1)) = -((0. + 0.) + (0.5 + 0.5)) / 2 cosine_loss(y_true, y_pred).numpy() -0.5 Calling with 'sample_weight'. cosine_loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() -0.0999 Using 'sum' reduction type. cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, ... reduction=tf.keras.losses.Reduction.SUM) cosine_loss(y_true, y_pred).numpy() -0.999 Using 'none' reduction type. cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, ... reduction=tf.keras.losses.Reduction.NONE) cosine_loss(y_true, y_pred).numpy() array([-0., -0.999], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1)) Args: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of tf.keras.losses.Reduction to apply to loss. Default value is AUTO . AUTO indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , using AUTO or SUM_OVER_BATCH_SIZE will raise an error. Please see this custom training [tutorial] (https://www.tensorflow.org/tutorials/distribute/custom_training) for more details. name: Optional name for the op. Hinge class tensorflow.keras.losses.Hinge(reduction=\"auto\", name=\"hinge\") Computes the hinge loss between y_true and y_pred . loss = maximum(1 - y_true * y_pred, 0) y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. h = tf.keras.losses.Hinge() h(y_true, y_pred).numpy() 1.3 Calling with 'sample_weight'. h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.55 Using 'sum' reduction type. h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 2.6 Using 'none' reduction type. h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.1, 1.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge()) Huber class tensorflow.keras.losses.Huber(delta=1.0, reduction=\"auto\", name=\"huber_loss\") Computes the Huber loss between y_true and y_pred . For each value x in error = y_true - y_pred : loss = 0.5 * x^2 if |x| <= d loss = 0.5 * d^2 + d * (|x| - d) if |x| > d where d is delta . See: https://en.wikipedia.org/wiki/Huber_loss Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. h = tf.keras.losses.Huber() h(y_true, y_pred).numpy() 0.155 Calling with 'sample_weight'. h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.09 Using 'sum' reduction type. h = tf.keras.losses.Huber( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 0.31 Using 'none' reduction type. h = tf.keras.losses.Huber( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([0.18, 0.13], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Huber()) kl_divergence function tensorflow.keras.losses.KLD(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . KLDivergence class tensorflow.keras.losses.KLDivergence(reduction=\"auto\", name=\"kl_divergence\") Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. kl = tf.keras.losses.KLDivergence() kl(y_true, y_pred).numpy() 0.458 Calling with 'sample_weight'. kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.366 Using 'sum' reduction type. kl = tf.keras.losses.KLDivergence( ... reduction=tf.keras.losses.Reduction.SUM) kl(y_true, y_pred).numpy() 0.916 Using 'none' reduction type. kl = tf.keras.losses.KLDivergence( ... reduction=tf.keras.losses.Reduction.NONE) kl(y_true, y_pred).numpy() array([0.916, -3.08e-06], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence()) LogCosh class tensorflow.keras.losses.LogCosh(reduction=\"auto\", name=\"log_cosh\") Computes the logarithm of the hyperbolic cosine of the prediction error. logcosh = log((exp(x) + exp(-x))/2) , where x is the error y_pred - y_true . Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [0., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. l = tf.keras.losses.LogCosh() l(y_true, y_pred).numpy() 0.108 Calling with 'sample_weight'. l(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.087 Using 'sum' reduction type. l = tf.keras.losses.LogCosh( ... reduction=tf.keras.losses.Reduction.SUM) l(y_true, y_pred).numpy() 0.217 Using 'none' reduction type. l = tf.keras.losses.LogCosh( ... reduction=tf.keras.losses.Reduction.NONE) l(y_true, y_pred).numpy() array([0.217, 0.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh()) Loss class tensorflow.keras.losses.Loss(reduction=\"auto\", name=None) Loss base class. To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) mean_absolute_error function tensorflow.keras.losses.MAE(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.losses.MAPE(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.losses.MSE(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.losses.MSLE(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . MeanAbsoluteError class tensorflow.keras.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\") Computes the mean of absolute difference between labels and predictions. loss = abs(y_true - y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. mae = tf.keras.losses.MeanAbsoluteError() mae(y_true, y_pred).numpy() 0.5 Calling with 'sample_weight'. mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.25 Using 'sum' reduction type. mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.SUM) mae(y_true, y_pred).numpy() 1.0 Using 'none' reduction type. mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.NONE) mae(y_true, y_pred).numpy() array([0.5, 0.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError()) MeanAbsolutePercentageError class tensorflow.keras.losses.MeanAbsolutePercentageError( reduction=\"auto\", name=\"mean_absolute_percentage_error\" ) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * abs(y_true - y_pred) / y_true Standalone usage: y_true = [[2., 1.], [2., 3.]] y_pred = [[1., 1.], [1., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. mape = tf.keras.losses.MeanAbsolutePercentageError() mape(y_true, y_pred).numpy() 50. Calling with 'sample_weight'. mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 20. Using 'sum' reduction type. mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.SUM) mape(y_true, y_pred).numpy() 100. Using 'none' reduction type. mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.NONE) mape(y_true, y_pred).numpy() array([25., 75.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsolutePercentageError()) MeanSquaredError class tensorflow.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\") Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. mse = tf.keras.losses.MeanSquaredError() mse(y_true, y_pred).numpy() 0.5 Calling with 'sample_weight'. mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.25 Using 'sum' reduction type. mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.SUM) mse(y_true, y_pred).numpy() 1.0 Using 'none' reduction type. mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.NONE) mse(y_true, y_pred).numpy() array([0.5, 0.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError()) MeanSquaredLogarithmicError class tensorflow.keras.losses.MeanSquaredLogarithmicError( reduction=\"auto\", name=\"mean_squared_logarithmic_error\" ) Computes the mean squared logarithmic error between y_true and y_pred . loss = square(log(y_true + 1.) - log(y_pred + 1.)) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. msle = tf.keras.losses.MeanSquaredLogarithmicError() msle(y_true, y_pred).numpy() 0.240 Calling with 'sample_weight'. msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.120 Using 'sum' reduction type. msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.SUM) msle(y_true, y_pred).numpy() 0.480 Using 'none' reduction type. msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.NONE) msle(y_true, y_pred).numpy() array([0.240, 0.240], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredLogarithmicError()) Poisson class tensorflow.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\") Computes the Poisson loss between y_true and y_pred . loss = y_pred - y_true * log(y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [0., 0.]] Using 'auto'/'sum_over_batch_size' reduction type. p = tf.keras.losses.Poisson() p(y_true, y_pred).numpy() 0.5 Calling with 'sample_weight'. p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.4 Using 'sum' reduction type. p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.SUM) p(y_true, y_pred).numpy() 0.999 Using 'none' reduction type. p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.NONE) p(y_true, y_pred).numpy() array([0.999, 0.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson()) ReductionV2 class tensorflow.keras.losses.Reduction(*args, **kwargs) Types of loss reduction. Contains the following values: AUTO : Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , we expect reduction value to be SUM or NONE . Using AUTO in that case will raise an error. NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. SUM : Scalar sum of weighted losses. SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. This reduction type is not supported when used with tf.distribute.Strategy outside of built-in training loops like tf.keras compile / fit . You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size) Please see the custom training guide # pylint: disable=line-too-long for more details on this. SparseCategoricalCrossentropy class tensorflow.keras.losses.SparseCategoricalCrossentropy( from_logits=False, reduction=\"auto\", name=\"sparse_categorical_crossentropy\" ) Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] Using 'auto'/'sum_over_batch_size' reduction type. scce = tf.keras.losses.SparseCategoricalCrossentropy() scce(y_true, y_pred).numpy() 1.177 Calling with 'sample_weight'. scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy() 0.814 Using 'sum' reduction type. scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) scce(y_true, y_pred).numpy() 2.354 Using 'none' reduction type. scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) scce(y_true, y_pred).numpy() array([0.0513, 2.303], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy()) SquaredHinge class tensorflow.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\") Computes the squared hinge loss between y_true and y_pred . loss = square(maximum(1 - y_true * y_pred, 0)) y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]] Using 'auto'/'sum_over_batch_size' reduction type. h = tf.keras.losses.SquaredHinge() h(y_true, y_pred).numpy() 1.86 Calling with 'sample_weight'. h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.73 Using 'sum' reduction type. h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 3.72 Using 'none' reduction type. h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.46, 2.26], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge()) binary_crossentropy function tensorflow.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the binary crossentropy loss. Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] loss = tf.keras.losses.binary_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.916 , 0.714], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1] . categorical_crossentropy function tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the categorical crossentropy loss. Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Tensor of one-hot true targets. y_pred: Tensor of predicted targets. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Categorical crossentropy loss value. categorical_hinge function tensorflow.keras.losses.categorical_hinge(y_true, y_pred) Computes the categorical hinge loss between y_true and y_pred . loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred) Standalone usage: y_true = np.random.randint(0, 3, size=(2,)) y_true = tf.keras.utils.to_categorical(y_true, num_classes=3) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.categorical_hinge(y_true, y_pred) assert loss.shape == (2,) pos = np.sum(y_true * y_pred, axis=-1) neg = np.amax((1. - y_true) * y_pred, axis=-1) assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.)) Args: y_true: The ground truth values. y_true values are expected to be 0 or 1. y_pred: The predicted values. Returns: Categorical hinge loss values. cosine_similarity function tensorflow.keras.losses.cosine_similarity(y_true, y_pred, axis=-1) Computes the cosine similarity between labels and predictions. Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where you try to maximize the proximity between predictions and targets. If either y_true or y_pred is a zero vector, cosine similarity will be 0 regardless of the proximity between predictions and targets. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Standalone usage: y_true = [[0., 1.], [1., 1.], [1., 1.]] y_pred = [[1., 0.], [1., 1.], [-1., -1.]] loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1) loss.numpy() array([-0., -0.999, 0.999], dtype=float32) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. axis: Axis along which to determine similarity. Returns: Cosine similarity tensor. deserialize function tensorflow.keras.losses.deserialize(name, custom_objects=None) Deserializes a serialized loss class/function instance. Arguments: name: Loss configuration. custom_objects: Optional dictionary mapping names (strings) to custom objects (classes and functions) to be considered during deserialization. Returns: A Keras Loss instance or a loss function. get function tensorflow.keras.losses.get(identifier) Retrieves a Keras loss as a function / Loss class instance. The identifier may be the string name of a loss function or Loss class. loss = tf.keras.losses.get(\"categorical_crossentropy\") type(loss) loss = tf.keras.losses.get(\"CategoricalCrossentropy\") type(loss) You can also specify config of the loss to this function by passing dict containing class_name and config as an identifier. Also note that the class_name must map to a Loss class identifier = {\"class_name\": \"CategoricalCrossentropy\", ... \"config\": {\"from_logits\": True}} loss = tf.keras.losses.get(identifier) type(loss) Arguments: identifier: A loss identifier. One of None or string name of a loss function/class or loss configuration dictionary or a loss function or a loss class instance Returns: A Keras loss as a function / Loss class instance. Raises: ValueError: If identifier cannot be interpreted. hinge function tensorflow.keras.losses.hinge(y_true, y_pred) Computes the hinge loss between y_true and y_pred . loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Hinge loss values. shape = [batch_size, d0, .. dN-1] . huber function tensorflow.keras.losses.huber(y_true, y_pred, delta=1.0) Computes Huber loss value. For each value x in error = y_true - y_pred : loss = 0.5 * x^2 if |x| <= d loss = 0.5 * d^2 + d * (|x| - d) if |x| > d where d is delta . See: https://en.wikipedia.org/wiki/Huber_loss Args: y_true: tensor of true targets. y_pred: tensor of predicted targets. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: Tensor with one scalar loss entry per sample. kl_divergence function tensorflow.keras.losses.kl_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . kl_divergence function tensorflow.keras.losses.kld(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . kl_divergence function tensorflow.keras.losses.kullback_leibler_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . log_cosh function tensorflow.keras.losses.log_cosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] . log_cosh function tensorflow.keras.losses.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_error function tensorflow.keras.losses.mae(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.losses.mape(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_error function tensorflow.keras.losses.mean_absolute_error(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.losses.mean_absolute_percentage_error(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.losses.mean_squared_error(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.losses.mse(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.losses.msle(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . poisson function tensorflow.keras.losses.poisson(y_true, y_pred) Computes the Poisson loss between y_true and y_pred. The Poisson loss is the mean of the elements of the Tensor y_pred - y_true * log(y_pred) . Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.poisson(y_true, y_pred) assert loss.shape == (2,) y_pred = y_pred + 1e-7 assert np.allclose( ... loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Poisson loss value. shape = [batch_size, d0, .. dN-1] . Raises: InvalidArgumentError: If y_true and y_pred have incompatible shapes. serialize function tensorflow.keras.losses.serialize(loss) Serializes loss function or Loss instance. Arguments: loss: A Keras Loss instance or a loss function. Returns: Loss configuration dictionary. sparse_categorical_crossentropy function tensorflow.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1) Computes the sparse categorical crossentropy loss. Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Ground truth values. y_pred: The predicted values. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the entropy is computed. Returns: Sparse categorical crossentropy loss value. squared_hinge function tensorflow.keras.losses.squared_hinge(y_true, y_pred) Computes the squared hinge loss between y_true and y_pred . loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.squared_hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Squared hinge loss values. shape = [batch_size, d0, .. dN-1] .","title":"Keras"},{"location":"dlf/losses/Keras/#usage-in-framework","text":"All in tf.keras.losses available loss functions can be used in our framework. Just use the class name as configuration key and the arguments as as dict.","title":"Usage in framework"},{"location":"dlf/losses/Keras/#yaml-configuration","text":"E.g. for tf.losses.MeanAbsoluteError without no arguments loss: MeanSquaredError: or for tf.losses.Huber with arguments: loss: Huber: delta: 0.3","title":"YAML Configuration"},{"location":"dlf/losses/Keras/#keras-loss-functions","text":"","title":"Keras loss functions"},{"location":"dlf/losses/Keras/#binarycrossentropy-class","text":"tensorflow.keras.losses.BinaryCrossentropy( from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\" ) Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"BinaryCrossentropy class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type","text":"bce = tf.keras.losses.BinaryCrossentropy() bce(y_true, y_pred).numpy() 0.815","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight","text":"bce(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.458","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type","text":"bce = tf.keras.losses.BinaryCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) bce(y_true, y_pred).numpy() 1.630","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type","text":"bce = tf.keras.losses.BinaryCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) bce(y_true, y_pred).numpy() array([0.916 , 0.714], dtype=float32) Usage with the tf.keras API: model.compile(optimizer='sgd', loss=tf.keras.losses.BinaryCrossentropy())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#categoricalcrossentropy-class","text":"tensorflow.keras.losses.CategoricalCrossentropy( from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"categorical_crossentropy\" ) Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]","title":"CategoricalCrossentropy class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_1","text":"cce = tf.keras.losses.CategoricalCrossentropy() cce(y_true, y_pred).numpy() 1.177","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_1","text":"cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy() 0.814","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_1","text":"cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) cce(y_true, y_pred).numpy() 2.354","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_1","text":"cce = tf.keras.losses.CategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) cce(y_true, y_pred).numpy() array([0.0513, 2.303], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#categoricalhinge-class","text":"tensorflow.keras.losses.CategoricalHinge(reduction=\"auto\", name=\"categorical_hinge\") Computes the categorical hinge loss between y_true and y_pred . loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred) Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"CategoricalHinge class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_2","text":"h = tf.keras.losses.CategoricalHinge() h(y_true, y_pred).numpy() 1.4","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_2","text":"h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.6","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_2","text":"h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 2.8","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_2","text":"h = tf.keras.losses.CategoricalHinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.2, 1.6], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#cosinesimilarity-class","text":"tensorflow.keras.losses.CosineSimilarity(axis=-1, reduction=\"auto\", name=\"cosine_similarity\") Computes the cosine similarity between labels and predictions. Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where you try to maximize the proximity between predictions and targets. If either y_true or y_pred is a zero vector, cosine similarity will be 0 regardless of the proximity between predictions and targets. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Standalone usage: y_true = [[0., 1.], [1., 1.]] y_pred = [[1., 0.], [1., 1.]]","title":"CosineSimilarity class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_3","text":"cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#l2_normy_true-0-1-11414-11414","text":"","title":"l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]"},{"location":"dlf/losses/Keras/#l2_normy_pred-1-0-11414-11414","text":"","title":"l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]"},{"location":"dlf/losses/Keras/#l2_normy_true-l2_normy_pred-0-0-05-05","text":"","title":"l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]"},{"location":"dlf/losses/Keras/#loss-meansuml2_normy_true-l2_normy_pred-axis1","text":"","title":"loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))"},{"location":"dlf/losses/Keras/#-0-0-05-05-2","text":"cosine_loss(y_true, y_pred).numpy() -0.5","title":"= -((0. + 0.) +  (0.5 + 0.5)) / 2"},{"location":"dlf/losses/Keras/#calling-with-sample_weight_3","text":"cosine_loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() -0.0999","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_3","text":"cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, ... reduction=tf.keras.losses.Reduction.SUM) cosine_loss(y_true, y_pred).numpy() -0.999","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_3","text":"cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, ... reduction=tf.keras.losses.Reduction.NONE) cosine_loss(y_true, y_pred).numpy() array([-0., -0.999], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1)) Args: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of tf.keras.losses.Reduction to apply to loss. Default value is AUTO . AUTO indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , using AUTO or SUM_OVER_BATCH_SIZE will raise an error. Please see this custom training [tutorial] (https://www.tensorflow.org/tutorials/distribute/custom_training) for more details. name: Optional name for the op.","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#hinge-class","text":"tensorflow.keras.losses.Hinge(reduction=\"auto\", name=\"hinge\") Computes the hinge loss between y_true and y_pred . loss = maximum(1 - y_true * y_pred, 0) y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"Hinge class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_4","text":"h = tf.keras.losses.Hinge() h(y_true, y_pred).numpy() 1.3","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_4","text":"h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.55","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_4","text":"h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 2.6","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_4","text":"h = tf.keras.losses.Hinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.1, 1.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#huber-class","text":"tensorflow.keras.losses.Huber(delta=1.0, reduction=\"auto\", name=\"huber_loss\") Computes the Huber loss between y_true and y_pred . For each value x in error = y_true - y_pred : loss = 0.5 * x^2 if |x| <= d loss = 0.5 * d^2 + d * (|x| - d) if |x| > d where d is delta . See: https://en.wikipedia.org/wiki/Huber_loss Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"Huber class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_5","text":"h = tf.keras.losses.Huber() h(y_true, y_pred).numpy() 0.155","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_5","text":"h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.09","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_5","text":"h = tf.keras.losses.Huber( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 0.31","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_5","text":"h = tf.keras.losses.Huber( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([0.18, 0.13], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#kl_divergence-function","text":"tensorflow.keras.losses.KLD(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/losses/Keras/#kldivergence-class","text":"tensorflow.keras.losses.KLDivergence(reduction=\"auto\", name=\"kl_divergence\") Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"KLDivergence class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_6","text":"kl = tf.keras.losses.KLDivergence() kl(y_true, y_pred).numpy() 0.458","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_6","text":"kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.366","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_6","text":"kl = tf.keras.losses.KLDivergence( ... reduction=tf.keras.losses.Reduction.SUM) kl(y_true, y_pred).numpy() 0.916","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_6","text":"kl = tf.keras.losses.KLDivergence( ... reduction=tf.keras.losses.Reduction.NONE) kl(y_true, y_pred).numpy() array([0.916, -3.08e-06], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#logcosh-class","text":"tensorflow.keras.losses.LogCosh(reduction=\"auto\", name=\"log_cosh\") Computes the logarithm of the hyperbolic cosine of the prediction error. logcosh = log((exp(x) + exp(-x))/2) , where x is the error y_pred - y_true . Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [0., 0.]]","title":"LogCosh class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_7","text":"l = tf.keras.losses.LogCosh() l(y_true, y_pred).numpy() 0.108","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_7","text":"l(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.087","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_7","text":"l = tf.keras.losses.LogCosh( ... reduction=tf.keras.losses.Reduction.SUM) l(y_true, y_pred).numpy() 0.217","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_7","text":"l = tf.keras.losses.LogCosh( ... reduction=tf.keras.losses.Reduction.NONE) l(y_true, y_pred).numpy() array([0.217, 0.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#loss-class","text":"tensorflow.keras.losses.Loss(reduction=\"auto\", name=None) Loss base class. To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size))","title":"Loss class"},{"location":"dlf/losses/Keras/#mean_absolute_error-function","text":"tensorflow.keras.losses.MAE(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/losses/Keras/#mean_absolute_percentage_error-function","text":"tensorflow.keras.losses.MAPE(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/losses/Keras/#mean_squared_error-function","text":"tensorflow.keras.losses.MSE(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/losses/Keras/#mean_squared_logarithmic_error-function","text":"tensorflow.keras.losses.MSLE(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/losses/Keras/#meanabsoluteerror-class","text":"tensorflow.keras.losses.MeanAbsoluteError(reduction=\"auto\", name=\"mean_absolute_error\") Computes the mean of absolute difference between labels and predictions. loss = abs(y_true - y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]]","title":"MeanAbsoluteError class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_8","text":"mae = tf.keras.losses.MeanAbsoluteError() mae(y_true, y_pred).numpy() 0.5","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_8","text":"mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.25","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_8","text":"mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.SUM) mae(y_true, y_pred).numpy() 1.0","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_8","text":"mae = tf.keras.losses.MeanAbsoluteError( ... reduction=tf.keras.losses.Reduction.NONE) mae(y_true, y_pred).numpy() array([0.5, 0.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#meanabsolutepercentageerror-class","text":"tensorflow.keras.losses.MeanAbsolutePercentageError( reduction=\"auto\", name=\"mean_absolute_percentage_error\" ) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * abs(y_true - y_pred) / y_true Standalone usage: y_true = [[2., 1.], [2., 3.]] y_pred = [[1., 1.], [1., 0.]]","title":"MeanAbsolutePercentageError class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_9","text":"mape = tf.keras.losses.MeanAbsolutePercentageError() mape(y_true, y_pred).numpy() 50.","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_9","text":"mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 20.","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_9","text":"mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.SUM) mape(y_true, y_pred).numpy() 100.","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_9","text":"mape = tf.keras.losses.MeanAbsolutePercentageError( ... reduction=tf.keras.losses.Reduction.NONE) mape(y_true, y_pred).numpy() array([25., 75.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsolutePercentageError())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#meansquarederror-class","text":"tensorflow.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\") Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]]","title":"MeanSquaredError class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_10","text":"mse = tf.keras.losses.MeanSquaredError() mse(y_true, y_pred).numpy() 0.5","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_10","text":"mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.25","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_10","text":"mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.SUM) mse(y_true, y_pred).numpy() 1.0","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_10","text":"mse = tf.keras.losses.MeanSquaredError( ... reduction=tf.keras.losses.Reduction.NONE) mse(y_true, y_pred).numpy() array([0.5, 0.5], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#meansquaredlogarithmicerror-class","text":"tensorflow.keras.losses.MeanSquaredLogarithmicError( reduction=\"auto\", name=\"mean_squared_logarithmic_error\" ) Computes the mean squared logarithmic error between y_true and y_pred . loss = square(log(y_true + 1.) - log(y_pred + 1.)) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [1., 0.]]","title":"MeanSquaredLogarithmicError class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_11","text":"msle = tf.keras.losses.MeanSquaredLogarithmicError() msle(y_true, y_pred).numpy() 0.240","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_11","text":"msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy() 0.120","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_11","text":"msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.SUM) msle(y_true, y_pred).numpy() 0.480","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_11","text":"msle = tf.keras.losses.MeanSquaredLogarithmicError( ... reduction=tf.keras.losses.Reduction.NONE) msle(y_true, y_pred).numpy() array([0.240, 0.240], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredLogarithmicError())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#poisson-class","text":"tensorflow.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\") Computes the Poisson loss between y_true and y_pred . loss = y_pred - y_true * log(y_pred) Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[1., 1.], [0., 0.]]","title":"Poisson class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_12","text":"p = tf.keras.losses.Poisson() p(y_true, y_pred).numpy() 0.5","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_12","text":"p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy() 0.4","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_12","text":"p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.SUM) p(y_true, y_pred).numpy() 0.999","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_12","text":"p = tf.keras.losses.Poisson( ... reduction=tf.keras.losses.Reduction.NONE) p(y_true, y_pred).numpy() array([0.999, 0.], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#reductionv2-class","text":"tensorflow.keras.losses.Reduction(*args, **kwargs) Types of loss reduction. Contains the following values: AUTO : Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , we expect reduction value to be SUM or NONE . Using AUTO in that case will raise an error. NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. SUM : Scalar sum of weighted losses. SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. This reduction type is not supported when used with tf.distribute.Strategy outside of built-in training loops like tf.keras compile / fit . You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size) Please see the custom training guide # pylint: disable=line-too-long for more details on this.","title":"ReductionV2 class"},{"location":"dlf/losses/Keras/#sparsecategoricalcrossentropy-class","text":"tensorflow.keras.losses.SparseCategoricalCrossentropy( from_logits=False, reduction=\"auto\", name=\"sparse_categorical_crossentropy\" ) Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]","title":"SparseCategoricalCrossentropy class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_13","text":"scce = tf.keras.losses.SparseCategoricalCrossentropy() scce(y_true, y_pred).numpy() 1.177","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_13","text":"scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy() 0.814","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_13","text":"scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.SUM) scce(y_true, y_pred).numpy() 2.354","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_13","text":"scce = tf.keras.losses.SparseCategoricalCrossentropy( ... reduction=tf.keras.losses.Reduction.NONE) scce(y_true, y_pred).numpy() array([0.0513, 2.303], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#squaredhinge-class","text":"tensorflow.keras.losses.SquaredHinge(reduction=\"auto\", name=\"squared_hinge\") Computes the squared hinge loss between y_true and y_pred . loss = square(maximum(1 - y_true * y_pred, 0)) y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Standalone usage: y_true = [[0., 1.], [0., 0.]] y_pred = [[0.6, 0.4], [0.4, 0.6]]","title":"SquaredHinge class"},{"location":"dlf/losses/Keras/#using-autosum_over_batch_size-reduction-type_14","text":"h = tf.keras.losses.SquaredHinge() h(y_true, y_pred).numpy() 1.86","title":"Using 'auto'/'sum_over_batch_size' reduction type."},{"location":"dlf/losses/Keras/#calling-with-sample_weight_14","text":"h(y_true, y_pred, sample_weight=[1, 0]).numpy() 0.73","title":"Calling with 'sample_weight'."},{"location":"dlf/losses/Keras/#using-sum-reduction-type_14","text":"h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.SUM) h(y_true, y_pred).numpy() 3.72","title":"Using 'sum' reduction type."},{"location":"dlf/losses/Keras/#using-none-reduction-type_14","text":"h = tf.keras.losses.SquaredHinge( ... reduction=tf.keras.losses.Reduction.NONE) h(y_true, y_pred).numpy() array([1.46, 2.26], dtype=float32) Usage with the compile() API: model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge())","title":"Using 'none' reduction type."},{"location":"dlf/losses/Keras/#binary_crossentropy-function","text":"tensorflow.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the binary crossentropy loss. Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] loss = tf.keras.losses.binary_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.916 , 0.714], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1] .","title":"binary_crossentropy function"},{"location":"dlf/losses/Keras/#categorical_crossentropy-function","text":"tensorflow.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the categorical crossentropy loss. Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Tensor of one-hot true targets. y_pred: Tensor of predicted targets. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Categorical crossentropy loss value.","title":"categorical_crossentropy function"},{"location":"dlf/losses/Keras/#categorical_hinge-function","text":"tensorflow.keras.losses.categorical_hinge(y_true, y_pred) Computes the categorical hinge loss between y_true and y_pred . loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred) Standalone usage: y_true = np.random.randint(0, 3, size=(2,)) y_true = tf.keras.utils.to_categorical(y_true, num_classes=3) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.categorical_hinge(y_true, y_pred) assert loss.shape == (2,) pos = np.sum(y_true * y_pred, axis=-1) neg = np.amax((1. - y_true) * y_pred, axis=-1) assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.)) Args: y_true: The ground truth values. y_true values are expected to be 0 or 1. y_pred: The predicted values. Returns: Categorical hinge loss values.","title":"categorical_hinge function"},{"location":"dlf/losses/Keras/#cosine_similarity-function","text":"tensorflow.keras.losses.cosine_similarity(y_true, y_pred, axis=-1) Computes the cosine similarity between labels and predictions. Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where you try to maximize the proximity between predictions and targets. If either y_true or y_pred is a zero vector, cosine similarity will be 0 regardless of the proximity between predictions and targets. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Standalone usage: y_true = [[0., 1.], [1., 1.], [1., 1.]] y_pred = [[1., 0.], [1., 1.], [-1., -1.]] loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1) loss.numpy() array([-0., -0.999, 0.999], dtype=float32) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. axis: Axis along which to determine similarity. Returns: Cosine similarity tensor.","title":"cosine_similarity function"},{"location":"dlf/losses/Keras/#deserialize-function","text":"tensorflow.keras.losses.deserialize(name, custom_objects=None) Deserializes a serialized loss class/function instance. Arguments: name: Loss configuration. custom_objects: Optional dictionary mapping names (strings) to custom objects (classes and functions) to be considered during deserialization. Returns: A Keras Loss instance or a loss function.","title":"deserialize function"},{"location":"dlf/losses/Keras/#get-function","text":"tensorflow.keras.losses.get(identifier) Retrieves a Keras loss as a function / Loss class instance. The identifier may be the string name of a loss function or Loss class. loss = tf.keras.losses.get(\"categorical_crossentropy\") type(loss) loss = tf.keras.losses.get(\"CategoricalCrossentropy\") type(loss) You can also specify config of the loss to this function by passing dict containing class_name and config as an identifier. Also note that the class_name must map to a Loss class identifier = {\"class_name\": \"CategoricalCrossentropy\", ... \"config\": {\"from_logits\": True}} loss = tf.keras.losses.get(identifier) type(loss) Arguments: identifier: A loss identifier. One of None or string name of a loss function/class or loss configuration dictionary or a loss function or a loss class instance Returns: A Keras loss as a function / Loss class instance. Raises: ValueError: If identifier cannot be interpreted.","title":"get function"},{"location":"dlf/losses/Keras/#hinge-function","text":"tensorflow.keras.losses.hinge(y_true, y_pred) Computes the hinge loss between y_true and y_pred . loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Hinge loss values. shape = [batch_size, d0, .. dN-1] .","title":"hinge function"},{"location":"dlf/losses/Keras/#huber-function","text":"tensorflow.keras.losses.huber(y_true, y_pred, delta=1.0) Computes Huber loss value. For each value x in error = y_true - y_pred : loss = 0.5 * x^2 if |x| <= d loss = 0.5 * d^2 + d * (|x| - d) if |x| > d where d is delta . See: https://en.wikipedia.org/wiki/Huber_loss Args: y_true: tensor of true targets. y_pred: tensor of predicted targets. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: Tensor with one scalar loss entry per sample.","title":"huber function"},{"location":"dlf/losses/Keras/#kl_divergence-function_1","text":"tensorflow.keras.losses.kl_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/losses/Keras/#kl_divergence-function_2","text":"tensorflow.keras.losses.kld(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/losses/Keras/#kl_divergence-function_3","text":"tensorflow.keras.losses.kullback_leibler_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/losses/Keras/#log_cosh-function","text":"tensorflow.keras.losses.log_cosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] .","title":"log_cosh function"},{"location":"dlf/losses/Keras/#log_cosh-function_1","text":"tensorflow.keras.losses.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] .","title":"log_cosh function"},{"location":"dlf/losses/Keras/#mean_absolute_error-function_1","text":"tensorflow.keras.losses.mae(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/losses/Keras/#mean_absolute_percentage_error-function_1","text":"tensorflow.keras.losses.mape(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/losses/Keras/#mean_absolute_error-function_2","text":"tensorflow.keras.losses.mean_absolute_error(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/losses/Keras/#mean_absolute_percentage_error-function_2","text":"tensorflow.keras.losses.mean_absolute_percentage_error(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/losses/Keras/#mean_squared_error-function_1","text":"tensorflow.keras.losses.mean_squared_error(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/losses/Keras/#mean_squared_logarithmic_error-function_1","text":"tensorflow.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/losses/Keras/#mean_squared_error-function_2","text":"tensorflow.keras.losses.mse(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/losses/Keras/#mean_squared_logarithmic_error-function_2","text":"tensorflow.keras.losses.msle(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/losses/Keras/#poisson-function","text":"tensorflow.keras.losses.poisson(y_true, y_pred) Computes the Poisson loss between y_true and y_pred. The Poisson loss is the mean of the elements of the Tensor y_pred - y_true * log(y_pred) . Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.poisson(y_true, y_pred) assert loss.shape == (2,) y_pred = y_pred + 1e-7 assert np.allclose( ... loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Poisson loss value. shape = [batch_size, d0, .. dN-1] . Raises: InvalidArgumentError: If y_true and y_pred have incompatible shapes.","title":"poisson function"},{"location":"dlf/losses/Keras/#serialize-function","text":"tensorflow.keras.losses.serialize(loss) Serializes loss function or Loss instance. Arguments: loss: A Keras Loss instance or a loss function. Returns: Loss configuration dictionary.","title":"serialize function"},{"location":"dlf/losses/Keras/#sparse_categorical_crossentropy-function","text":"tensorflow.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1) Computes the sparse categorical crossentropy loss. Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Ground truth values. y_pred: The predicted values. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the entropy is computed. Returns: Sparse categorical crossentropy loss value.","title":"sparse_categorical_crossentropy function"},{"location":"dlf/losses/Keras/#squared_hinge-function","text":"tensorflow.keras.losses.squared_hinge(y_true, y_pred) Computes the squared hinge loss between y_true and y_pred . loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.squared_hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Squared hinge loss values. shape = [batch_size, d0, .. dN-1] .","title":"squared_hinge function"},{"location":"dlf/losses/NTXentLoss/","text":"NTXentLoss class dlf.losses.ntxent_loss.NTXentLoss(batch_size, temperature=0.5) Implementation of NTXentLoss like used in SimCLR Arguments batch_size : int. Used batch size temperature : float. Temperature to scale features YAML Configuration loss: NTXentLoss: batch_size: 16 temperature: 0.5 References SimCLR Tensorflow implementation","title":"NTXentLoss"},{"location":"dlf/losses/NTXentLoss/#ntxentloss-class","text":"dlf.losses.ntxent_loss.NTXentLoss(batch_size, temperature=0.5) Implementation of NTXentLoss like used in SimCLR Arguments batch_size : int. Used batch size temperature : float. Temperature to scale features YAML Configuration loss: NTXentLoss: batch_size: 16 temperature: 0.5 References SimCLR Tensorflow implementation","title":"NTXentLoss class"},{"location":"dlf/losses/SSDLoss/","text":"SSDLoss class dlf.losses.ssd_loss.SSDLoss(neg_ratio, num_classes, name=\"ssd_loss\", from_logits=False) Loss base class. To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size))","title":"SSDLoss"},{"location":"dlf/losses/SSDLoss/#ssdloss-class","text":"dlf.losses.ssd_loss.SSDLoss(neg_ratio, num_classes, name=\"ssd_loss\", from_logits=False) Loss base class. To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size))","title":"SSDLoss class"},{"location":"dlf/losses/SparseCategoricalCrossentropyIgnore/","text":"SparseCategoricalCrossentropyIgnore class dlf.losses.sparse_categorial_crossentropy_ignore.SparseCategoricalCrossentropyIgnore( num_classes, name=\"sparse_categorical_crossentropy_ignore\", from_logits=False ) Implementation for sparse crossentropy loss with ignore label functionality. The same implmentation like tf.keras.losses.SparseCategoricalCrossentropy but with the addtion that this implementation ignores all label ids <= num_classes. Aliases SparseCategoricalCrossentropyIgnore sparse_categorical_crossentropy_ignore Arguments num_classes : int. Number of classes, everthing above or equal will be ignored name : str, optonal, Name of the loss function. Defaults to 'sparse_categorical_crossentropy_ignore'. from_logits : bool. If true, y_pred is expected to be a logits tensor. Defaults to False. Returns A tf.keras.losses.Loss YAML Configuration loss: SparseCategoricalCrossentropyIgnore: num_classes: 7 from_logits: False","title":"SparseCategoricalCrossentropyIgnore"},{"location":"dlf/losses/SparseCategoricalCrossentropyIgnore/#sparsecategoricalcrossentropyignore-class","text":"dlf.losses.sparse_categorial_crossentropy_ignore.SparseCategoricalCrossentropyIgnore( num_classes, name=\"sparse_categorical_crossentropy_ignore\", from_logits=False ) Implementation for sparse crossentropy loss with ignore label functionality. The same implmentation like tf.keras.losses.SparseCategoricalCrossentropy but with the addtion that this implementation ignores all label ids <= num_classes. Aliases SparseCategoricalCrossentropyIgnore sparse_categorical_crossentropy_ignore Arguments num_classes : int. Number of classes, everthing above or equal will be ignored name : str, optonal, Name of the loss function. Defaults to 'sparse_categorical_crossentropy_ignore'. from_logits : bool. If true, y_pred is expected to be a logits tensor. Defaults to False. Returns A tf.keras.losses.Loss YAML Configuration loss: SparseCategoricalCrossentropyIgnore: num_classes: 7 from_logits: False","title":"SparseCategoricalCrossentropyIgnore class"},{"location":"dlf/metrics/Keras/","text":"Usage in framework All in tf.keras.metrics available metrics can be used in our framework. Just use the class name as configuration key and the arguments as as dict. YAML Configuration E.g. for tf.metrics.MeanAbsoluteError without no arguments metrics: MeanSquaredError: or for tf.metrics.MeanIoU with arguments: metrics: MeanIoU: num_classes: 7 Keras metrics AUC class tensorflow.keras.metrics.AUC( num_thresholds=200, curve=\"ROC\", summation_method=\"interpolation\", name=None, dtype=None, thresholds=None, multi_label=False, label_weights=None, ) Computes the approximate AUC (Area under the curve) via a Riemann sum. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall. This value is ultimately returned as auc , an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The num_thresholds variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on num_thresholds . The thresholds parameter can be used to manually specify thresholds which split the predictions more evenly. For best results, predictions should be distributed approximately uniformly in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC approximation may be poor if this is not the case. Setting summation_method to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: num_thresholds: (Optional) Defaults to 200. The number of thresholds to use when discretizing the roc curve. Values must be > 1. curve: (Optional) Specifies the name of the curve to be computed, 'ROC' [default] or 'PR' for the Precision-Recall-curve. summation_method: (Optional) Specifies the Riemann summation method used. 'interpolation' (default) applies mid-point summation scheme for ROC . For PR-AUC, interpolates (true/false) positives but not the ratio that is precision (see Davis & Goadrich 2006 for details); 'minoring' applies left summation for increasing intervals and right summation for decreasing intervals; 'majoring' does the opposite. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. thresholds: (Optional) A list of floating point values to use as the thresholds for discretizing the curve. If set, the num_thresholds parameter is ignored. Values should be in [0, 1]. Endpoint thresholds equal to {-epsilon, 1+epsilon} for a small positive epsilon value will be automatically included with these to correctly handle predictions equal to exactly 0 or 1. multi_label: boolean indicating whether multilabel data should be treated as such, wherein AUC is computed separately for each label and then averaged across labels, or (when False) if the data should be flattened into a single label before AUC computation. In the latter case, when multilabel data is passed to AUC, each label-prediction pair is treated as an individual data point. Should be set to False for multi-class data. label_weights: (optional) list, array, or tensor of non-negative weights used to compute AUCs for multilabel data. When multi_label is True, the weights are applied to the individual label AUCs when they are averaged to produce the multi-label AUC. When it's False, they are used to weight the individual label predictions in computing the confusion matrix on the flattened data. Note that this is unlike class_weights in that class_weights weights the example depending on the value of its label, whereas label_weights depends only on the index of that label before flattening; therefore label_weights should not be used for multi-class data. Standalone usage: m = tf.keras.metrics.AUC(num_thresholds=3) m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9]) threshold values are [0 - 1e-7, 0.5, 1 + 1e-7] tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2] recall = [1, 0.5, 0], fp_rate = [1, 0, 0] auc = ((((1+0.5)/2) (1-0))+ (((0.5+0)/2) (0-0))) = 0.75 m.result().numpy() 0.75 m.reset_states() m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.AUC()]) Accuracy class tensorflow.keras.metrics.Accuracy(name=\"accuracy\", dtype=None) Calculates how often predictions equal labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Accuracy() m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]]) m.result().numpy() 0.75 m.reset_states() m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]], ... sample_weight=[1, 1, 0, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Accuracy()]) BinaryAccuracy class tensorflow.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5) Calculates how often predictions match binary labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. threshold: (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Standalone usage: m = tf.keras.metrics.BinaryAccuracy() m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]]) m.result().numpy() 0.75 m.reset_states() m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.BinaryAccuracy()]) BinaryCrossentropy class tensorflow.keras.metrics.BinaryCrossentropy( name=\"binary_crossentropy\", dtype=None, from_logits=False, label_smoothing=0 ) Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional )Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \". Standalone usage: m = tf.keras.metrics.BinaryCrossentropy() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 0.81492424 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 0.9162905 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.BinaryCrossentropy()]) CategoricalAccuracy class tensorflow.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None) Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.CategoricalAccuracy() m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], ... [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], ... [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalAccuracy()]) CategoricalCrossentropy class tensorflow.keras.metrics.CategoricalCrossentropy( name=\"categorical_crossentropy\", dtype=None, from_logits=False, label_smoothing=0 ) Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are multiple label classes (2 or more). Here we assume that labels are given as a one_hot representation. eg., When labels values are [2, 0, 1], y_true = [[0, 0, 1], [1, 0, 0], [0, 1, 0]]. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional) Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" Standalone usage: EPSILON = 1e-7, y = y_true, y` = y_pred y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON) y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]] xent = -sum(y * log(y'), axis = -1) = -((log 0.95), (log 0.1)) = [0.051, 2.302] Reduced xent = (0.051 + 2.302) / 2 m = tf.keras.metrics.CategoricalCrossentropy() m.update_state([[0, 1, 0], [0, 0, 1]], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) m.result().numpy() 1.1769392 m.reset_states() m.update_state([[0, 1, 0], [0, 0, 1]], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], ... sample_weight=tf.constant([0.3, 0.7])) m.result().numpy() 1.6271976 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalCrossentropy()]) CategoricalHinge class tensorflow.keras.metrics.CategoricalHinge(name=\"categorical_hinge\", dtype=None) Computes the categorical hinge metric between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.CategoricalHinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.4000001 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.2 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalHinge()]) CosineSimilarity class tensorflow.keras.metrics.CosineSimilarity(name=\"cosine_similarity\", dtype=None, axis=-1) Computes the cosine similarity between the labels and predictions. cosine similarity = (a . b) / ||a|| ||b|| See: Cosine Similarity . This metric keeps the average cosine similarity between predictions and labels over a stream of data. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. Standalone usage: l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]] l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]] l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]] result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1)) = ((0. + 0.) + (0.5 + 0.5)) / 2 m = tf.keras.metrics.CosineSimilarity(axis=1) m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]]) m.result().numpy() 0.49999997 m.reset_states() m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]], ... sample_weight=[0.3, 0.7]) m.result().numpy() 0.6999999 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CosineSimilarity(axis=1)]) FalseNegatives class tensorflow.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None) Calculates the number of false negatives. If sample_weight is given, calculates the sum of the weights of false negatives. This metric creates one local variable, accumulator that is used to keep track of the number of false negatives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.FalseNegatives() m.update_state([0, 1, 1, 1], [0, 1, 0, 0]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.FalseNegatives()]) FalsePositives class tensorflow.keras.metrics.FalsePositives(thresholds=None, name=None, dtype=None) Calculates the number of false positives. If sample_weight is given, calculates the sum of the weights of false positives. This metric creates one local variable, accumulator that is used to keep track of the number of false positives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.FalsePositives() m.update_state([0, 1, 0, 0], [0, 0, 1, 1]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.FalsePositives()]) Hinge class tensorflow.keras.metrics.Hinge(name=\"hinge\", dtype=None) Computes the hinge metric between y_true and y_pred . y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Hinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.3 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.1 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()]) kl_divergence function tensorflow.keras.metrics.KLD(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . KLDivergence class tensorflow.keras.metrics.KLDivergence(name=\"kullback_leibler_divergence\", dtype=None) Computes Kullback-Leibler divergence metric between y_true and y_pred . metric = y_true * log(y_true / y_pred) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.KLDivergence() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 0.45814306 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 0.9162892 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.KLDivergence()]) LogCoshError class tensorflow.keras.metrics.LogCoshError(name=\"logcosh\", dtype=None) Computes the logarithm of the hyperbolic cosine of the prediction error. logcosh = log((exp(x) + exp(-x))/2) , where x is the error (y_pred - y_true) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.LogCoshError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.10844523 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.21689045 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.LogCoshError()]) mean_absolute_error function tensorflow.keras.metrics.MAE(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.metrics.MAPE(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.metrics.MSE(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.metrics.MSLE(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Mean class tensorflow.keras.metrics.Mean(name=\"mean\", dtype=None) Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4. If the weights were specified as [1, 1, 0, 0] then the mean would be 2. This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Mean() m.update_state([1, 3, 5, 7]) m.result().numpy() 4.0 m.reset_states() m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0]) m.result().numpy() 2.0 Usage with compile() API: model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs)) model.compile(optimizer='sgd', loss='mse') MeanAbsoluteError class tensorflow.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None) Computes the mean absolute error between the labels and predictions. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanAbsoluteError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.25 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()]) MeanAbsolutePercentageError class tensorflow.keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\", dtype=None) Computes the mean absolute percentage error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanAbsolutePercentageError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 250000000.0 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 500000000.0 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()]) MeanIoU class tensorflow.keras.metrics.MeanIoU(num_classes, name=None, dtype=None) Computes the mean Intersection-Over-Union metric. Mean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes. IOU is defined as follows: IOU = true_positive / (true_positive + false_positive + false_negative). The predictions are accumulated in a confusion matrix, weighted by sample_weight and the metric is then calculated from it. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: num_classes: The possible number of labels the prediction task can have. This value must be provided, since a confusion matrix of dimension = [num_classes, num_classes] will be allocated. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: cm = [[1, 1], [1, 1]] sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1] iou = true_positives / (sum_row + sum_col - true_positives)) result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33 m = tf.keras.metrics.MeanIoU(num_classes=2) m.update_state([0, 0, 1, 1], [0, 1, 0, 1]) m.result().numpy() 0.33333334 m.reset_states() m.update_state([0, 0, 1, 1], [0, 1, 0, 1], ... sample_weight=[0.3, 0.3, 0.3, 0.1]) m.result().numpy() 0.23809525 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)]) MeanRelativeError class tensorflow.keras.metrics.MeanRelativeError(normalizer, name=None, dtype=None) Computes the mean relative error by normalizing with the given values. This metric creates two local variables, total and count that are used to compute the mean relative error. This is weighted by sample_weight , and it is ultimately returned as mean_relative_error : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: normalizer: The normalizer values with same shape as predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3]) m.update_state([1, 3, 2, 3], [2, 4, 6, 8]) metric = mean(|y_pred - y_true| / normalizer) = mean([1, 1, 4, 5] / [1, 3, 2, 3]) = mean([1, 1/3, 2, 5/3]) = 5/4 = 1.25 m.result().numpy() 1.25 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])]) MeanSquaredError class tensorflow.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None) Computes the mean squared error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanSquaredError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.25 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanSquaredError()]) MeanSquaredLogarithmicError class tensorflow.keras.metrics.MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\", dtype=None) Computes the mean squared logarithmic error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanSquaredLogarithmicError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.12011322 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.24022643 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()]) MeanTensor class tensorflow.keras.metrics.MeanTensor(name=\"mean_tensor\", dtype=None) Computes the element-wise (weighted) mean of the given tensors. MeanTensor returns a tensor with the same shape of the input tensors. The mean value is updated by keeping local variables total and count . The total tracks the sum of the weighted values, and count stores the sum of the weighted counts. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanTensor() m.update_state([0, 1, 2, 3]) m.update_state([4, 5, 6, 7]) m.result().numpy() array([2., 3., 4., 5.], dtype=float32) m.update_state([12, 10, 8, 6], sample_weight= [0, 0.2, 0.5, 1]) m.result().numpy() array([2. , 3.6363635, 4.8 , 5.3333335], dtype=float32) Metric class tensorflow.keras.metrics.Metric(name=None, dtype=None, **kwargs) Encapsulates metric logic and state. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. **kwargs: Additional layer keywords arguments. Standalone usage: m = SomeMetric(...) for input in ...: m.update_state(input) print('Final result: ', m.result().numpy()) Usage with compile() API: model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(10, activation='softmax')) model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy()]) data = np.random.random((1000, 32)) labels = np.random.random((1000, 10)) dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32) model.fit(dataset, epochs=10) To be implemented by subclasses: * __init__() : All state variables should be created in this method by calling self.add_weight() like: self.var = self.add_weight(...) * update_state() : Has all updates to the state variables like: self.var.assign_add(...). * result() : Computes and returns a value for the metric from the state variables. Example subclass implementation: class BinaryTruePositives(tf.keras.metrics.Metric): def __init__(self, name='binary_true_positives', **kwargs): super(BinaryTruePositives, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name='tp', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.cast(y_true, tf.bool) y_pred = tf.cast(y_pred, tf.bool) values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True)) values = tf.cast(values, self.dtype) if sample_weight is not None: sample_weight = tf.cast(sample_weight, self.dtype) sample_weight = tf.broadcast_to(sample_weight, values.shape) values = tf.multiply(values, sample_weight) self.true_positives.assign_add(tf.reduce_sum(values)) def result(self): return self.true_positives Poisson class tensorflow.keras.metrics.Poisson(name=\"poisson\", dtype=None) Computes the Poisson metric between y_true and y_pred . metric = y_pred - y_true * log(y_pred) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Poisson() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.49999997 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.99999994 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Poisson()]) Precision class tensorflow.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None) Computes the precision of the predictions with respect to the labels. The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If top_k is set, we'll calculate precision as how often on average a class among the top-k classes with the highest predicted values of a batch entry is correct and can be found in the label for that entry. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and/or in the top-k highest predictions, and computing the fraction of them for which class_id is indeed a correct label. Args: thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate precision with thresholds=0.5 . top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating precision. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Precision() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 0.6666667 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 With top_k=2, it will calculate precision over y_true[:2] and y_pred[:2] m = tf.keras.metrics.Precision(top_k=2) m.update_state([0, 0, 1, 1], [1, 1, 1, 1]) m.result().numpy() 0.0 With top_k=4, it will calculate precision over y_true[:4] and y_pred[:4] m = tf.keras.metrics.Precision(top_k=4) m.update_state([0, 0, 1, 1], [1, 1, 1, 1]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Precision()]) PrecisionAtRecall class tensorflow.keras.metrics.PrecisionAtRecall(recall, num_thresholds=200, name=None, dtype=None) Computes best precision where recall is >= specified value. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: recall: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given recall. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.PrecisionAtRecall(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[2, 2, 2, 1, 1]) m.result().numpy() 0.33333333 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)]) Recall class tensorflow.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None) Computes the recall of the predictions with respect to the labels. This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If top_k is set, recall will be computed as how often on average a class among the labels of a batch entry is in the top-k predictions. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is in the label, and computing the fraction of them for which class_id is above the threshold and/or in the top-k predictions. Args: thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate recall with thresholds=0.5 . top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating recall. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Recall() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 0.6666667 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Recall()]) RecallAtPrecision class tensorflow.keras.metrics.RecallAtPrecision(precision, num_thresholds=200, name=None, dtype=None) Computes best recall where precision is >= specified value. For a given score-label-distribution the required precision might not be achievable, in this case 0.0 is returned as recall. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the recall at the given precision. The threshold for the given precision value is computed and used to evaluate the corresponding recall. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: precision: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given precision. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.RecallAtPrecision(0.8) m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 1.0 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)]) RootMeanSquaredError class tensorflow.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None) Computes root mean squared error metric between y_true and y_pred . Standalone usage: m = tf.keras.metrics.RootMeanSquaredError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.70710677 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()]) SensitivityAtSpecificity class tensorflow.keras.metrics.SensitivityAtSpecificity(specificity, num_thresholds=200, name=None, dtype=None) Computes best sensitivity where specificity is >= specified value. the sensitivity at a given specificity. Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)). This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. For additional information about specificity and sensitivity, see the following . Args: specificity: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given specificity. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SensitivityAtSpecificity(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[1, 1, 2, 2, 1]) m.result().numpy() 0.333333 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SensitivityAtSpecificity()]) SparseCategoricalAccuracy class tensorflow.keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\", dtype=None) Calculates how often predictions matches integer labels. acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1)) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SparseCategoricalAccuracy() m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]) SparseCategoricalCrossentropy class tensorflow.keras.metrics.SparseCategoricalCrossentropy( name=\"sparse_categorical_crossentropy\", dtype=None, from_logits=False, axis=-1 ) Computes the crossentropy metric between the labels and predictions. Use this crossentropy metric when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy metric. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional) Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the metric is computed. Standalone usage: y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]] logits = log(y_pred) softmax = exp(logits) / sum(exp(logits), axis=-1) softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]] xent = -sum(y * log(softmax), 1) log(softmax) = [[-2.9957, -0.0513, -16.1181], [-2.3026, -0.2231, -2.3026]] y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]] xent = [0.0513, 2.3026] Reduced xent = (0.0513 + 2.3026) / 2 m = tf.keras.metrics.SparseCategoricalCrossentropy() m.update_state([1, 2], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) m.result().numpy() 1.1769392 m.reset_states() m.update_state([1, 2], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], ... sample_weight=tf.constant([0.3, 0.7])) m.result().numpy() 1.6271976 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()]) SparseTopKCategoricalAccuracy class tensorflow.keras.metrics.SparseTopKCategoricalAccuracy( k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None ) Computes how often integer targets are in the top K predictions. Args: k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1) m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()]) SpecificityAtSensitivity class tensorflow.keras.metrics.SpecificityAtSensitivity(sensitivity, num_thresholds=200, name=None, dtype=None) Computes best specificity where sensitivity is >= specified value. Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)). This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the specificity at the given sensitivity. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. For additional information about specificity and sensitivity, see the following . Args: sensitivity: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given sensitivity. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SpecificityAtSensitivity(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.66666667 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[1, 1, 2, 2, 2]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SpecificityAtSensitivity()]) SquaredHinge class tensorflow.keras.metrics.SquaredHinge(name=\"squared_hinge\", dtype=None) Computes the squared hinge metric between y_true and y_pred . y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SquaredHinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.86 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.46 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SquaredHinge()]) Sum class tensorflow.keras.metrics.Sum(name=\"sum\", dtype=None) Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Sum() m.update_state([1, 3, 5, 7]) m.result().numpy() 16.0 Usage with compile() API: model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs)) model.compile(optimizer='sgd', loss='mse') TopKCategoricalAccuracy class tensorflow.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top_k_categorical_accuracy\", dtype=None) Computes how often targets are in the top K predictions. Args: k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TopKCategoricalAccuracy(k=1) m.update_state([[0, 0, 1], [0, 1, 0]], ... [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 0, 1], [0, 1, 0]], ... [[0.1, 0.9, 0.8], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TopKCategoricalAccuracy()]) TrueNegatives class tensorflow.keras.metrics.TrueNegatives(thresholds=None, name=None, dtype=None) Calculates the number of true negatives. If sample_weight is given, calculates the sum of the weights of true negatives. This metric creates one local variable, accumulator that is used to keep track of the number of true negatives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TrueNegatives() m.update_state([0, 1, 0, 0], [1, 1, 0, 0]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TrueNegatives()]) TruePositives class tensorflow.keras.metrics.TruePositives(thresholds=None, name=None, dtype=None) Calculates the number of true positives. If sample_weight is given, calculates the sum of the weights of true positives. This metric creates one local variable, true_positives that is used to keep track of the number of true positives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TruePositives() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TruePositives()]) binary_accuracy function tensorflow.keras.metrics.binary_accuracy(y_true, y_pred, threshold=0.5) Calculates how often predictions matches binary labels. Standalone usage: y_true = [[1], [1], [0], [0]] y_pred = [[1], [1], [0], [0]] m = tf.keras.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) m.numpy() array([1., 1., 1., 1.], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . threshold: (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = [batch_size, d0, .. dN-1] binary_crossentropy function tensorflow.keras.metrics.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the binary crossentropy loss. Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] loss = tf.keras.losses.binary_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.916 , 0.714], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1] . categorical_accuracy function tensorflow.keras.metrics.categorical_accuracy(y_true, y_pred) Calculates how often predictions matches one-hot labels. Standalone usage: y_true = [[0, 0, 1], [0, 1, 0]] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.categorical_accuracy(y_true, y_pred) assert m.shape == (2,) m.numpy() array([0., 1.], dtype=float32) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. Args: y_true: One-hot ground truth values. y_pred: The prediction values. Returns: Categorical accuracy values. categorical_crossentropy function tensorflow.keras.metrics.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the categorical crossentropy loss. Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Tensor of one-hot true targets. y_pred: Tensor of predicted targets. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Categorical crossentropy loss value. deserialize function tensorflow.keras.metrics.deserialize(config, custom_objects=None) Deserializes a serialized metric class/function instance. Arguments: config: Metric configuration. custom_objects: Optional dictionary mapping names (strings) to custom objects (classes and functions) to be considered during deserialization. Returns: A Keras Metric instance or a metric function. get function tensorflow.keras.metrics.get(identifier) Retrieves a Keras metric as a function / Metric class instance. The identifier may be the string name of a metric function or class. metric = tf.keras.metrics.get(\"categorical_crossentropy\") type(metric) metric = tf.keras.metrics.get(\"CategoricalCrossentropy\") type(metric) You can also specify config of the metric to this function by passing dict containing class_name and config as an identifier. Also note that the class_name must map to a Metric class identifier = {\"class_name\": \"CategoricalCrossentropy\", ... \"config\": {\"from_logits\": True}} metric = tf.keras.metrics.get(identifier) type(metric) Arguments: identifier: A metric identifier. One of None or string name of a metric function/class or metric configuration dictionary or a metric function or a metric class instance Returns: A Keras metric as a function / Metric class instance. Raises: ValueError: If identifier cannot be interpreted. hinge function tensorflow.keras.metrics.hinge(y_true, y_pred) Computes the hinge loss between y_true and y_pred . loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Hinge loss values. shape = [batch_size, d0, .. dN-1] . kl_divergence function tensorflow.keras.metrics.kl_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . kl_divergence function tensorflow.keras.metrics.kld(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . kl_divergence function tensorflow.keras.metrics.kullback_leibler_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype . log_cosh function tensorflow.keras.metrics.log_cosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] . log_cosh function tensorflow.keras.metrics.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_error function tensorflow.keras.metrics.mae(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.metrics.mape(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_error function tensorflow.keras.metrics.mean_absolute_error(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] . mean_absolute_percentage_error function tensorflow.keras.metrics.mean_absolute_percentage_error(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.metrics.mean_squared_error(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.metrics.mean_squared_logarithmic_error(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . mean_squared_error function tensorflow.keras.metrics.mse(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] . mean_squared_logarithmic_error function tensorflow.keras.metrics.msle(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . poisson function tensorflow.keras.metrics.poisson(y_true, y_pred) Computes the Poisson loss between y_true and y_pred. The Poisson loss is the mean of the elements of the Tensor y_pred - y_true * log(y_pred) . Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.poisson(y_true, y_pred) assert loss.shape == (2,) y_pred = y_pred + 1e-7 assert np.allclose( ... loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Poisson loss value. shape = [batch_size, d0, .. dN-1] . Raises: InvalidArgumentError: If y_true and y_pred have incompatible shapes. serialize function tensorflow.keras.metrics.serialize(metric) Serializes metric function or Metric instance. Arguments: metric: A Keras Metric instance or a metric function. Returns: Metric configuration dictionary. sparse_categorical_accuracy function tensorflow.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) Calculates how often predictions matches integer labels. Standalone usage: y_true = [2, 1] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) assert m.shape == (2,) m.numpy() array([0., 1.], dtype=float32) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. Args: y_true: Integer ground truth values. y_pred: The prediction values. Returns: Sparse categorical accuracy values. sparse_categorical_crossentropy function tensorflow.keras.metrics.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1) Computes the sparse categorical crossentropy loss. Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Ground truth values. y_pred: The predicted values. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the entropy is computed. Returns: Sparse categorical crossentropy loss value. sparse_top_k_categorical_accuracy function tensorflow.keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5) Computes how often integer targets are in the top K predictions. Standalone usage: y_true = [2, 1] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.sparse_top_k_categorical_accuracy( ... y_true, y_pred, k=3) assert m.shape == (2,) m.numpy() array([1., 1.], dtype=float32) Args: y_true: tensor of true targets. y_pred: tensor of predicted targets. k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. Returns: Sparse top K categorical accuracy value. squared_hinge function tensorflow.keras.metrics.squared_hinge(y_true, y_pred) Computes the squared hinge loss between y_true and y_pred . loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.squared_hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Squared hinge loss values. shape = [batch_size, d0, .. dN-1] . top_k_categorical_accuracy function tensorflow.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5) Computes how often targets are in the top K predictions. Standalone usage: y_true = [[0, 0, 1], [0, 1, 0]] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3) assert m.shape == (2,) m.numpy() array([1., 1.], dtype=float32) Args: y_true: The ground truth values. y_pred: The prediction values. k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. Returns: Top K categorical accuracy value.","title":"Keras"},{"location":"dlf/metrics/Keras/#usage-in-framework","text":"All in tf.keras.metrics available metrics can be used in our framework. Just use the class name as configuration key and the arguments as as dict.","title":"Usage in framework"},{"location":"dlf/metrics/Keras/#yaml-configuration","text":"E.g. for tf.metrics.MeanAbsoluteError without no arguments metrics: MeanSquaredError: or for tf.metrics.MeanIoU with arguments: metrics: MeanIoU: num_classes: 7","title":"YAML Configuration"},{"location":"dlf/metrics/Keras/#keras-metrics","text":"","title":"Keras metrics"},{"location":"dlf/metrics/Keras/#auc-class","text":"tensorflow.keras.metrics.AUC( num_thresholds=200, curve=\"ROC\", summation_method=\"interpolation\", name=None, dtype=None, thresholds=None, multi_label=False, label_weights=None, ) Computes the approximate AUC (Area under the curve) via a Riemann sum. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall. This value is ultimately returned as auc , an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The num_thresholds variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on num_thresholds . The thresholds parameter can be used to manually specify thresholds which split the predictions more evenly. For best results, predictions should be distributed approximately uniformly in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC approximation may be poor if this is not the case. Setting summation_method to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: num_thresholds: (Optional) Defaults to 200. The number of thresholds to use when discretizing the roc curve. Values must be > 1. curve: (Optional) Specifies the name of the curve to be computed, 'ROC' [default] or 'PR' for the Precision-Recall-curve. summation_method: (Optional) Specifies the Riemann summation method used. 'interpolation' (default) applies mid-point summation scheme for ROC . For PR-AUC, interpolates (true/false) positives but not the ratio that is precision (see Davis & Goadrich 2006 for details); 'minoring' applies left summation for increasing intervals and right summation for decreasing intervals; 'majoring' does the opposite. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. thresholds: (Optional) A list of floating point values to use as the thresholds for discretizing the curve. If set, the num_thresholds parameter is ignored. Values should be in [0, 1]. Endpoint thresholds equal to {-epsilon, 1+epsilon} for a small positive epsilon value will be automatically included with these to correctly handle predictions equal to exactly 0 or 1. multi_label: boolean indicating whether multilabel data should be treated as such, wherein AUC is computed separately for each label and then averaged across labels, or (when False) if the data should be flattened into a single label before AUC computation. In the latter case, when multilabel data is passed to AUC, each label-prediction pair is treated as an individual data point. Should be set to False for multi-class data. label_weights: (optional) list, array, or tensor of non-negative weights used to compute AUCs for multilabel data. When multi_label is True, the weights are applied to the individual label AUCs when they are averaged to produce the multi-label AUC. When it's False, they are used to weight the individual label predictions in computing the confusion matrix on the flattened data. Note that this is unlike class_weights in that class_weights weights the example depending on the value of its label, whereas label_weights depends only on the index of that label before flattening; therefore label_weights should not be used for multi-class data. Standalone usage: m = tf.keras.metrics.AUC(num_thresholds=3) m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])","title":"AUC class"},{"location":"dlf/metrics/Keras/#threshold-values-are-0-1e-7-05-1-1e-7","text":"","title":"threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]"},{"location":"dlf/metrics/Keras/#tp-2-1-0-fp-2-0-0-fn-0-1-2-tn-0-2-2","text":"","title":"tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]"},{"location":"dlf/metrics/Keras/#recall-1-05-0-fp_rate-1-0-0","text":"","title":"recall = [1, 0.5, 0], fp_rate = [1, 0, 0]"},{"location":"dlf/metrics/Keras/#auc-10521-0-05020-0-075","text":"m.result().numpy() 0.75 m.reset_states() m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.AUC()])","title":"auc = ((((1+0.5)/2)(1-0))+ (((0.5+0)/2)(0-0))) = 0.75"},{"location":"dlf/metrics/Keras/#accuracy-class","text":"tensorflow.keras.metrics.Accuracy(name=\"accuracy\", dtype=None) Calculates how often predictions equal labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Accuracy() m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]]) m.result().numpy() 0.75 m.reset_states() m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]], ... sample_weight=[1, 1, 0, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Accuracy()])","title":"Accuracy class"},{"location":"dlf/metrics/Keras/#binaryaccuracy-class","text":"tensorflow.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5) Calculates how often predictions match binary labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. threshold: (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Standalone usage: m = tf.keras.metrics.BinaryAccuracy() m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]]) m.result().numpy() 0.75 m.reset_states() m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.BinaryAccuracy()])","title":"BinaryAccuracy class"},{"location":"dlf/metrics/Keras/#binarycrossentropy-class","text":"tensorflow.keras.metrics.BinaryCrossentropy( name=\"binary_crossentropy\", dtype=None, from_logits=False, label_smoothing=0 ) Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional )Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \". Standalone usage: m = tf.keras.metrics.BinaryCrossentropy() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 0.81492424 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 0.9162905 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.BinaryCrossentropy()])","title":"BinaryCrossentropy class"},{"location":"dlf/metrics/Keras/#categoricalaccuracy-class","text":"tensorflow.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None) Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.CategoricalAccuracy() m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], ... [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], ... [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalAccuracy()])","title":"CategoricalAccuracy class"},{"location":"dlf/metrics/Keras/#categoricalcrossentropy-class","text":"tensorflow.keras.metrics.CategoricalCrossentropy( name=\"categorical_crossentropy\", dtype=None, from_logits=False, label_smoothing=0 ) Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are multiple label classes (2 or more). Here we assume that labels are given as a one_hot representation. eg., When labels values are [2, 0, 1], y_true = [[0, 0, 1], [1, 0, 0], [0, 1, 0]]. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional) Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" Standalone usage:","title":"CategoricalCrossentropy class"},{"location":"dlf/metrics/Keras/#epsilon-1e-7-y-y_true-y-y_pred","text":"","title":"EPSILON = 1e-7, y = y_true, y` = y_pred"},{"location":"dlf/metrics/Keras/#y-clip_opsclip_by_valueoutput-epsilon-1-epsilon","text":"","title":"y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)"},{"location":"dlf/metrics/Keras/#y-005-095-epsilon-01-08-01","text":"","title":"y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]"},{"location":"dlf/metrics/Keras/#xent-sumy-logy-axis-1","text":"","title":"xent = -sum(y * log(y'), axis = -1)"},{"location":"dlf/metrics/Keras/#-log-095-log-01","text":"","title":"= -((log 0.95), (log 0.1))"},{"location":"dlf/metrics/Keras/#0051-2302","text":"","title":"= [0.051, 2.302]"},{"location":"dlf/metrics/Keras/#reduced-xent-0051-2302-2","text":"m = tf.keras.metrics.CategoricalCrossentropy() m.update_state([[0, 1, 0], [0, 0, 1]], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) m.result().numpy() 1.1769392 m.reset_states() m.update_state([[0, 1, 0], [0, 0, 1]], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], ... sample_weight=tf.constant([0.3, 0.7])) m.result().numpy() 1.6271976 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalCrossentropy()])","title":"Reduced xent = (0.051 + 2.302) / 2"},{"location":"dlf/metrics/Keras/#categoricalhinge-class","text":"tensorflow.keras.metrics.CategoricalHinge(name=\"categorical_hinge\", dtype=None) Computes the categorical hinge metric between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.CategoricalHinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.4000001 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.2 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CategoricalHinge()])","title":"CategoricalHinge class"},{"location":"dlf/metrics/Keras/#cosinesimilarity-class","text":"tensorflow.keras.metrics.CosineSimilarity(name=\"cosine_similarity\", dtype=None, axis=-1) Computes the cosine similarity between the labels and predictions. cosine similarity = (a . b) / ||a|| ||b|| See: Cosine Similarity . This metric keeps the average cosine similarity between predictions and labels over a stream of data. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. Standalone usage:","title":"CosineSimilarity class"},{"location":"dlf/metrics/Keras/#l2_normy_true-0-1-11414-11414","text":"","title":"l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]"},{"location":"dlf/metrics/Keras/#l2_normy_pred-1-0-11414-11414","text":"","title":"l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]"},{"location":"dlf/metrics/Keras/#l2_normy_true-l2_normy_pred-0-0-05-05","text":"","title":"l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]"},{"location":"dlf/metrics/Keras/#result-meansuml2_normy_true-l2_normy_pred-axis1","text":"","title":"result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))"},{"location":"dlf/metrics/Keras/#0-0-05-05-2","text":"m = tf.keras.metrics.CosineSimilarity(axis=1) m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]]) m.result().numpy() 0.49999997 m.reset_states() m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]], ... sample_weight=[0.3, 0.7]) m.result().numpy() 0.6999999 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.CosineSimilarity(axis=1)])","title":"= ((0. + 0.) +  (0.5 + 0.5)) / 2"},{"location":"dlf/metrics/Keras/#falsenegatives-class","text":"tensorflow.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None) Calculates the number of false negatives. If sample_weight is given, calculates the sum of the weights of false negatives. This metric creates one local variable, accumulator that is used to keep track of the number of false negatives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.FalseNegatives() m.update_state([0, 1, 1, 1], [0, 1, 0, 0]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.FalseNegatives()])","title":"FalseNegatives class"},{"location":"dlf/metrics/Keras/#falsepositives-class","text":"tensorflow.keras.metrics.FalsePositives(thresholds=None, name=None, dtype=None) Calculates the number of false positives. If sample_weight is given, calculates the sum of the weights of false positives. This metric creates one local variable, accumulator that is used to keep track of the number of false positives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.FalsePositives() m.update_state([0, 1, 0, 0], [0, 0, 1, 1]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.FalsePositives()])","title":"FalsePositives class"},{"location":"dlf/metrics/Keras/#hinge-class","text":"tensorflow.keras.metrics.Hinge(name=\"hinge\", dtype=None) Computes the hinge metric between y_true and y_pred . y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Hinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.3 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.1 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])","title":"Hinge class"},{"location":"dlf/metrics/Keras/#kl_divergence-function","text":"tensorflow.keras.metrics.KLD(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/metrics/Keras/#kldivergence-class","text":"tensorflow.keras.metrics.KLDivergence(name=\"kullback_leibler_divergence\", dtype=None) Computes Kullback-Leibler divergence metric between y_true and y_pred . metric = y_true * log(y_true / y_pred) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.KLDivergence() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 0.45814306 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 0.9162892 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.KLDivergence()])","title":"KLDivergence class"},{"location":"dlf/metrics/Keras/#logcosherror-class","text":"tensorflow.keras.metrics.LogCoshError(name=\"logcosh\", dtype=None) Computes the logarithm of the hyperbolic cosine of the prediction error. logcosh = log((exp(x) + exp(-x))/2) , where x is the error (y_pred - y_true) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.LogCoshError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.10844523 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.21689045 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.LogCoshError()])","title":"LogCoshError class"},{"location":"dlf/metrics/Keras/#mean_absolute_error-function","text":"tensorflow.keras.metrics.MAE(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/metrics/Keras/#mean_absolute_percentage_error-function","text":"tensorflow.keras.metrics.MAPE(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/metrics/Keras/#mean_squared_error-function","text":"tensorflow.keras.metrics.MSE(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/metrics/Keras/#mean_squared_logarithmic_error-function","text":"tensorflow.keras.metrics.MSLE(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/metrics/Keras/#mean-class","text":"tensorflow.keras.metrics.Mean(name=\"mean\", dtype=None) Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4. If the weights were specified as [1, 1, 0, 0] then the mean would be 2. This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Mean() m.update_state([1, 3, 5, 7]) m.result().numpy() 4.0 m.reset_states() m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0]) m.result().numpy() 2.0 Usage with compile() API: model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs)) model.compile(optimizer='sgd', loss='mse')","title":"Mean class"},{"location":"dlf/metrics/Keras/#meanabsoluteerror-class","text":"tensorflow.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error\", dtype=None) Computes the mean absolute error between the labels and predictions. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanAbsoluteError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.25 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()])","title":"MeanAbsoluteError class"},{"location":"dlf/metrics/Keras/#meanabsolutepercentageerror-class","text":"tensorflow.keras.metrics.MeanAbsolutePercentageError(name=\"mean_absolute_percentage_error\", dtype=None) Computes the mean absolute percentage error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanAbsolutePercentageError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 250000000.0 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 500000000.0 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])","title":"MeanAbsolutePercentageError class"},{"location":"dlf/metrics/Keras/#meaniou-class","text":"tensorflow.keras.metrics.MeanIoU(num_classes, name=None, dtype=None) Computes the mean Intersection-Over-Union metric. Mean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes. IOU is defined as follows: IOU = true_positive / (true_positive + false_positive + false_negative). The predictions are accumulated in a confusion matrix, weighted by sample_weight and the metric is then calculated from it. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: num_classes: The possible number of labels the prediction task can have. This value must be provided, since a confusion matrix of dimension = [num_classes, num_classes] will be allocated. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage:","title":"MeanIoU class"},{"location":"dlf/metrics/Keras/#cm-1-1","text":"","title":"cm = [[1, 1],"},{"location":"dlf/metrics/Keras/#1-1","text":"","title":"[1, 1]]"},{"location":"dlf/metrics/Keras/#sum_row-2-2-sum_col-2-2-true_positives-1-1","text":"","title":"sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]"},{"location":"dlf/metrics/Keras/#iou-true_positives-sum_row-sum_col-true_positives","text":"","title":"iou = true_positives / (sum_row + sum_col - true_positives))"},{"location":"dlf/metrics/Keras/#result-1-2-2-1-1-2-2-1-2-033","text":"m = tf.keras.metrics.MeanIoU(num_classes=2) m.update_state([0, 0, 1, 1], [0, 1, 0, 1]) m.result().numpy() 0.33333334 m.reset_states() m.update_state([0, 0, 1, 1], [0, 1, 0, 1], ... sample_weight=[0.3, 0.3, 0.3, 0.1]) m.result().numpy() 0.23809525 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])","title":"result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33"},{"location":"dlf/metrics/Keras/#meanrelativeerror-class","text":"tensorflow.keras.metrics.MeanRelativeError(normalizer, name=None, dtype=None) Computes the mean relative error by normalizing with the given values. This metric creates two local variables, total and count that are used to compute the mean relative error. This is weighted by sample_weight , and it is ultimately returned as mean_relative_error : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: normalizer: The normalizer values with same shape as predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3]) m.update_state([1, 3, 2, 3], [2, 4, 6, 8])","title":"MeanRelativeError class"},{"location":"dlf/metrics/Keras/#metric-meany_pred-y_true-normalizer","text":"","title":"metric = mean(|y_pred - y_true| / normalizer)"},{"location":"dlf/metrics/Keras/#mean1-1-4-5-1-3-2-3-mean1-13-2-53","text":"","title":"= mean([1, 1, 4, 5] / [1, 3, 2, 3]) = mean([1, 1/3, 2, 5/3])"},{"location":"dlf/metrics/Keras/#54-125","text":"m.result().numpy() 1.25 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])","title":"= 5/4 = 1.25"},{"location":"dlf/metrics/Keras/#meansquarederror-class","text":"tensorflow.keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None) Computes the mean squared error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanSquaredError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.25 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanSquaredError()])","title":"MeanSquaredError class"},{"location":"dlf/metrics/Keras/#meansquaredlogarithmicerror-class","text":"tensorflow.keras.metrics.MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\", dtype=None) Computes the mean squared logarithmic error between y_true and y_pred . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanSquaredLogarithmicError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.12011322 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.24022643 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])","title":"MeanSquaredLogarithmicError class"},{"location":"dlf/metrics/Keras/#meantensor-class","text":"tensorflow.keras.metrics.MeanTensor(name=\"mean_tensor\", dtype=None) Computes the element-wise (weighted) mean of the given tensors. MeanTensor returns a tensor with the same shape of the input tensors. The mean value is updated by keeping local variables total and count . The total tracks the sum of the weighted values, and count stores the sum of the weighted counts. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.MeanTensor() m.update_state([0, 1, 2, 3]) m.update_state([4, 5, 6, 7]) m.result().numpy() array([2., 3., 4., 5.], dtype=float32) m.update_state([12, 10, 8, 6], sample_weight= [0, 0.2, 0.5, 1]) m.result().numpy() array([2. , 3.6363635, 4.8 , 5.3333335], dtype=float32)","title":"MeanTensor class"},{"location":"dlf/metrics/Keras/#metric-class","text":"tensorflow.keras.metrics.Metric(name=None, dtype=None, **kwargs) Encapsulates metric logic and state. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. **kwargs: Additional layer keywords arguments. Standalone usage: m = SomeMetric(...) for input in ...: m.update_state(input) print('Final result: ', m.result().numpy()) Usage with compile() API: model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(10, activation='softmax')) model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy()]) data = np.random.random((1000, 32)) labels = np.random.random((1000, 10)) dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32) model.fit(dataset, epochs=10) To be implemented by subclasses: * __init__() : All state variables should be created in this method by calling self.add_weight() like: self.var = self.add_weight(...) * update_state() : Has all updates to the state variables like: self.var.assign_add(...). * result() : Computes and returns a value for the metric from the state variables. Example subclass implementation: class BinaryTruePositives(tf.keras.metrics.Metric): def __init__(self, name='binary_true_positives', **kwargs): super(BinaryTruePositives, self).__init__(name=name, **kwargs) self.true_positives = self.add_weight(name='tp', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.cast(y_true, tf.bool) y_pred = tf.cast(y_pred, tf.bool) values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True)) values = tf.cast(values, self.dtype) if sample_weight is not None: sample_weight = tf.cast(sample_weight, self.dtype) sample_weight = tf.broadcast_to(sample_weight, values.shape) values = tf.multiply(values, sample_weight) self.true_positives.assign_add(tf.reduce_sum(values)) def result(self): return self.true_positives","title":"Metric class"},{"location":"dlf/metrics/Keras/#poisson-class","text":"tensorflow.keras.metrics.Poisson(name=\"poisson\", dtype=None) Computes the Poisson metric between y_true and y_pred . metric = y_pred - y_true * log(y_pred) Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Poisson() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.49999997 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.99999994 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Poisson()])","title":"Poisson class"},{"location":"dlf/metrics/Keras/#precision-class","text":"tensorflow.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None) Computes the precision of the predictions with respect to the labels. The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If top_k is set, we'll calculate precision as how often on average a class among the top-k classes with the highest predicted values of a batch entry is correct and can be found in the label for that entry. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and/or in the top-k highest predictions, and computing the fraction of them for which class_id is indeed a correct label. Args: thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate precision with thresholds=0.5 . top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating precision. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Precision() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 0.6666667 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0","title":"Precision class"},{"location":"dlf/metrics/Keras/#with-top_k2-it-will-calculate-precision-over-y_true2-and-y_pred2","text":"m = tf.keras.metrics.Precision(top_k=2) m.update_state([0, 0, 1, 1], [1, 1, 1, 1]) m.result().numpy() 0.0","title":"With top_k=2, it will calculate precision over y_true[:2] and y_pred[:2]"},{"location":"dlf/metrics/Keras/#with-top_k4-it-will-calculate-precision-over-y_true4-and-y_pred4","text":"m = tf.keras.metrics.Precision(top_k=4) m.update_state([0, 0, 1, 1], [1, 1, 1, 1]) m.result().numpy() 0.5 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Precision()])","title":"With top_k=4, it will calculate precision over y_true[:4] and y_pred[:4]"},{"location":"dlf/metrics/Keras/#precisionatrecall-class","text":"tensorflow.keras.metrics.PrecisionAtRecall(recall, num_thresholds=200, name=None, dtype=None) Computes best precision where recall is >= specified value. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: recall: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given recall. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.PrecisionAtRecall(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[2, 2, 2, 1, 1]) m.result().numpy() 0.33333333 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])","title":"PrecisionAtRecall class"},{"location":"dlf/metrics/Keras/#recall-class","text":"tensorflow.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None) Computes the recall of the predictions with respect to the labels. This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If top_k is set, recall will be computed as how often on average a class among the labels of a batch entry is in the top-k predictions. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is in the label, and computing the fraction of them for which class_id is above the threshold and/or in the top-k predictions. Args: thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate recall with thresholds=0.5 . top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating recall. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Recall() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 0.6666667 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Recall()])","title":"Recall class"},{"location":"dlf/metrics/Keras/#recallatprecision-class","text":"tensorflow.keras.metrics.RecallAtPrecision(precision, num_thresholds=200, name=None, dtype=None) Computes best recall where precision is >= specified value. For a given score-label-distribution the required precision might not be achievable, in this case 0.0 is returned as recall. This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the recall at the given precision. The threshold for the given precision value is computed and used to evaluate the corresponding recall. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: precision: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given precision. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.RecallAtPrecision(0.8) m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9], ... sample_weight=[1, 0, 0, 1]) m.result().numpy() 1.0 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])","title":"RecallAtPrecision class"},{"location":"dlf/metrics/Keras/#rootmeansquarederror-class","text":"tensorflow.keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None) Computes root mean squared error metric between y_true and y_pred . Standalone usage: m = tf.keras.metrics.RootMeanSquaredError() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], ... sample_weight=[1, 0]) m.result().numpy() 0.70710677 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])","title":"RootMeanSquaredError class"},{"location":"dlf/metrics/Keras/#sensitivityatspecificity-class","text":"tensorflow.keras.metrics.SensitivityAtSpecificity(specificity, num_thresholds=200, name=None, dtype=None) Computes best sensitivity where specificity is >= specified value. the sensitivity at a given specificity. Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)). This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. For additional information about specificity and sensitivity, see the following . Args: specificity: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given specificity. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SensitivityAtSpecificity(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.5 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[1, 1, 2, 2, 1]) m.result().numpy() 0.333333 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SensitivityAtSpecificity()])","title":"SensitivityAtSpecificity class"},{"location":"dlf/metrics/Keras/#sparsecategoricalaccuracy-class","text":"tensorflow.keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\", dtype=None) Calculates how often predictions matches integer labels. acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1)) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SparseCategoricalAccuracy() m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])","title":"SparseCategoricalAccuracy class"},{"location":"dlf/metrics/Keras/#sparsecategoricalcrossentropy-class","text":"tensorflow.keras.metrics.SparseCategoricalCrossentropy( name=\"sparse_categorical_crossentropy\", dtype=None, from_logits=False, axis=-1 ) Computes the crossentropy metric between the labels and predictions. Use this crossentropy metric when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy metric. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. from_logits: (Optional) Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the metric is computed. Standalone usage:","title":"SparseCategoricalCrossentropy class"},{"location":"dlf/metrics/Keras/#y_true-one_hoty_true-0-1-0-0-0-1","text":"","title":"y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]"},{"location":"dlf/metrics/Keras/#logits-logy_pred","text":"","title":"logits = log(y_pred)"},{"location":"dlf/metrics/Keras/#softmax-explogits-sumexplogits-axis-1","text":"","title":"softmax = exp(logits) / sum(exp(logits), axis=-1)"},{"location":"dlf/metrics/Keras/#softmax-005-095-epsilon-01-08-01","text":"","title":"softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]"},{"location":"dlf/metrics/Keras/#xent-sumy-logsoftmax-1","text":"","title":"xent = -sum(y * log(softmax), 1)"},{"location":"dlf/metrics/Keras/#logsoftmax-29957-00513-161181","text":"","title":"log(softmax) = [[-2.9957, -0.0513, -16.1181],"},{"location":"dlf/metrics/Keras/#-23026-02231-23026","text":"","title":"[-2.3026, -0.2231, -2.3026]]"},{"location":"dlf/metrics/Keras/#y_true-logsoftmax-0-00513-0-0-0-23026","text":"","title":"y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]"},{"location":"dlf/metrics/Keras/#xent-00513-23026","text":"","title":"xent = [0.0513, 2.3026]"},{"location":"dlf/metrics/Keras/#reduced-xent-00513-23026-2","text":"m = tf.keras.metrics.SparseCategoricalCrossentropy() m.update_state([1, 2], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) m.result().numpy() 1.1769392 m.reset_states() m.update_state([1, 2], ... [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], ... sample_weight=tf.constant([0.3, 0.7])) m.result().numpy() 1.6271976 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])","title":"Reduced xent = (0.0513 + 2.3026) / 2"},{"location":"dlf/metrics/Keras/#sparsetopkcategoricalaccuracy-class","text":"tensorflow.keras.metrics.SparseTopKCategoricalAccuracy( k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None ) Computes how often integer targets are in the top K predictions. Args: k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1) m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])","title":"SparseTopKCategoricalAccuracy class"},{"location":"dlf/metrics/Keras/#specificityatsensitivity-class","text":"tensorflow.keras.metrics.SpecificityAtSensitivity(sensitivity, num_thresholds=200, name=None, dtype=None) Computes best specificity where sensitivity is >= specified value. Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)). This metric creates four local variables, true_positives , true_negatives , false_positives and false_negatives that are used to compute the specificity at the given sensitivity. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. For additional information about specificity and sensitivity, see the following . Args: sensitivity: A scalar value in range [0, 1] . num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given sensitivity. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SpecificityAtSensitivity(0.5) m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8]) m.result().numpy() 0.66666667 m.reset_states() m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8], ... sample_weight=[1, 1, 2, 2, 2]) m.result().numpy() 0.5 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SpecificityAtSensitivity()])","title":"SpecificityAtSensitivity class"},{"location":"dlf/metrics/Keras/#squaredhinge-class","text":"tensorflow.keras.metrics.SquaredHinge(name=\"squared_hinge\", dtype=None) Computes the squared hinge metric between y_true and y_pred . y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.SquaredHinge() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) m.result().numpy() 1.86 m.reset_states() m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ... sample_weight=[1, 0]) m.result().numpy() 1.46 Usage with compile() API: model.compile( optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.SquaredHinge()])","title":"SquaredHinge class"},{"location":"dlf/metrics/Keras/#sum-class","text":"tensorflow.keras.metrics.Sum(name=\"sum\", dtype=None) Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.Sum() m.update_state([1, 3, 5, 7]) m.result().numpy() 16.0 Usage with compile() API: model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs)) model.compile(optimizer='sgd', loss='mse')","title":"Sum class"},{"location":"dlf/metrics/Keras/#topkcategoricalaccuracy-class","text":"tensorflow.keras.metrics.TopKCategoricalAccuracy(k=5, name=\"top_k_categorical_accuracy\", dtype=None) Computes how often targets are in the top K predictions. Args: k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TopKCategoricalAccuracy(k=1) m.update_state([[0, 0, 1], [0, 1, 0]], ... [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]) m.result().numpy() 0.5 m.reset_states() m.update_state([[0, 0, 1], [0, 1, 0]], ... [[0.1, 0.9, 0.8], [0.05, 0.95, 0]], ... sample_weight=[0.7, 0.3]) m.result().numpy() 0.3 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])","title":"TopKCategoricalAccuracy class"},{"location":"dlf/metrics/Keras/#truenegatives-class","text":"tensorflow.keras.metrics.TrueNegatives(thresholds=None, name=None, dtype=None) Calculates the number of true negatives. If sample_weight is given, calculates the sum of the weights of true negatives. This metric creates one local variable, accumulator that is used to keep track of the number of true negatives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TrueNegatives() m.update_state([0, 1, 0, 0], [1, 1, 0, 0]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TrueNegatives()])","title":"TrueNegatives class"},{"location":"dlf/metrics/Keras/#truepositives-class","text":"tensorflow.keras.metrics.TruePositives(thresholds=None, name=None, dtype=None) Calculates the number of true positives. If sample_weight is given, calculates the sum of the weights of true positives. This metric creates one local variable, true_positives that is used to keep track of the number of true positives. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Args: thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true , below is false ). One metric value is generated for each threshold value. name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. Standalone usage: m = tf.keras.metrics.TruePositives() m.update_state([0, 1, 1, 1], [1, 0, 1, 1]) m.result().numpy() 2.0 m.reset_states() m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0]) m.result().numpy() 1.0 Usage with compile() API: model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.TruePositives()])","title":"TruePositives class"},{"location":"dlf/metrics/Keras/#binary_accuracy-function","text":"tensorflow.keras.metrics.binary_accuracy(y_true, y_pred, threshold=0.5) Calculates how often predictions matches binary labels. Standalone usage: y_true = [[1], [1], [0], [0]] y_pred = [[1], [1], [0], [0]] m = tf.keras.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) m.numpy() array([1., 1., 1., 1.], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . threshold: (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = [batch_size, d0, .. dN-1]","title":"binary_accuracy function"},{"location":"dlf/metrics/Keras/#binary_crossentropy-function","text":"tensorflow.keras.metrics.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the binary crossentropy loss. Standalone usage: y_true = [[0, 1], [0, 0]] y_pred = [[0.6, 0.4], [0.4, 0.6]] loss = tf.keras.losses.binary_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.916 , 0.714], dtype=float32) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1] .","title":"binary_crossentropy function"},{"location":"dlf/metrics/Keras/#categorical_accuracy-function","text":"tensorflow.keras.metrics.categorical_accuracy(y_true, y_pred) Calculates how often predictions matches one-hot labels. Standalone usage: y_true = [[0, 0, 1], [0, 1, 0]] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.categorical_accuracy(y_true, y_pred) assert m.shape == (2,) m.numpy() array([0., 1.], dtype=float32) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. Args: y_true: One-hot ground truth values. y_pred: The prediction values. Returns: Categorical accuracy values.","title":"categorical_accuracy function"},{"location":"dlf/metrics/Keras/#categorical_crossentropy-function","text":"tensorflow.keras.metrics.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0) Computes the categorical crossentropy loss. Standalone usage: y_true = [[0, 1, 0], [0, 0, 1]] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Tensor of one-hot true targets. y_pred: Tensor of predicted targets. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. label_smoothing: Float in [0, 1]. If > 0 then smooth the labels. Returns: Categorical crossentropy loss value.","title":"categorical_crossentropy function"},{"location":"dlf/metrics/Keras/#deserialize-function","text":"tensorflow.keras.metrics.deserialize(config, custom_objects=None) Deserializes a serialized metric class/function instance. Arguments: config: Metric configuration. custom_objects: Optional dictionary mapping names (strings) to custom objects (classes and functions) to be considered during deserialization. Returns: A Keras Metric instance or a metric function.","title":"deserialize function"},{"location":"dlf/metrics/Keras/#get-function","text":"tensorflow.keras.metrics.get(identifier) Retrieves a Keras metric as a function / Metric class instance. The identifier may be the string name of a metric function or class. metric = tf.keras.metrics.get(\"categorical_crossentropy\") type(metric) metric = tf.keras.metrics.get(\"CategoricalCrossentropy\") type(metric) You can also specify config of the metric to this function by passing dict containing class_name and config as an identifier. Also note that the class_name must map to a Metric class identifier = {\"class_name\": \"CategoricalCrossentropy\", ... \"config\": {\"from_logits\": True}} metric = tf.keras.metrics.get(identifier) type(metric) Arguments: identifier: A metric identifier. One of None or string name of a metric function/class or metric configuration dictionary or a metric function or a metric class instance Returns: A Keras metric as a function / Metric class instance. Raises: ValueError: If identifier cannot be interpreted.","title":"get function"},{"location":"dlf/metrics/Keras/#hinge-function","text":"tensorflow.keras.metrics.hinge(y_true, y_pred) Computes the hinge loss between y_true and y_pred . loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided they will be converted to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Hinge loss values. shape = [batch_size, d0, .. dN-1] .","title":"hinge function"},{"location":"dlf/metrics/Keras/#kl_divergence-function_1","text":"tensorflow.keras.metrics.kl_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/metrics/Keras/#kl_divergence-function_2","text":"tensorflow.keras.metrics.kld(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/metrics/Keras/#kl_divergence-function_3","text":"tensorflow.keras.metrics.kullback_leibler_divergence(y_true, y_pred) Computes Kullback-Leibler divergence loss between y_true and y_pred . loss = y_true * log(y_true / y_pred) See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred) assert loss.shape == (2,) y_true = tf.keras.backend.clip(y_true, 1e-7, 1) y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1) assert np.array_equal( ... loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1)) Args: y_true: Tensor of true targets. y_pred: Tensor of predicted targets. Returns: A Tensor with loss. Raises: TypeError: If y_true cannot be cast to the y_pred.dtype .","title":"kl_divergence function"},{"location":"dlf/metrics/Keras/#log_cosh-function","text":"tensorflow.keras.metrics.log_cosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] .","title":"log_cosh function"},{"location":"dlf/metrics/Keras/#log_cosh-function_1","text":"tensorflow.keras.metrics.logcosh(y_true, y_pred) Logarithm of the hyperbolic cosine of the prediction error. log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x . This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. Standalone usage: y_true = np.random.random(size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.logcosh(y_true, y_pred) assert loss.shape == (2,) x = y_pred - y_true assert np.allclose( ... loss.numpy(), ... np.mean(x + np.log(np.exp(-2. * x) + 1.) - math_ops.log(2.), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Logcosh error values. shape = [batch_size, d0, .. dN-1] .","title":"log_cosh function"},{"location":"dlf/metrics/Keras/#mean_absolute_error-function_1","text":"tensorflow.keras.metrics.mae(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/metrics/Keras/#mean_absolute_percentage_error-function_1","text":"tensorflow.keras.metrics.mape(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/metrics/Keras/#mean_absolute_error-function_2","text":"tensorflow.keras.metrics.mean_absolute_error(y_true, y_pred) Computes the mean absolute error between labels and predictions. loss = mean(abs(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_error function"},{"location":"dlf/metrics/Keras/#mean_absolute_percentage_error-function_2","text":"tensorflow.keras.metrics.mean_absolute_percentage_error(y_true, y_pred) Computes the mean absolute percentage error between y_true and y_pred . loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1) Standalone usage: y_true = np.random.random(size=(2, 3)) y_true = np.maximum(y_true, 1e-7) # Prevent division by zero y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... 100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_absolute_percentage_error function"},{"location":"dlf/metrics/Keras/#mean_squared_error-function_1","text":"tensorflow.keras.metrics.mean_squared_error(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/metrics/Keras/#mean_squared_logarithmic_error-function_1","text":"tensorflow.keras.metrics.mean_squared_logarithmic_error(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/metrics/Keras/#mean_squared_error-function_2","text":"tensorflow.keras.metrics.mse(y_true, y_pred) Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_error function"},{"location":"dlf/metrics/Keras/#mean_squared_logarithmic_error-function_2","text":"tensorflow.keras.metrics.msle(y_true, y_pred) Computes the mean squared logarithmic error between y_true and y_pred . loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) y_true = np.maximum(y_true, 1e-7) y_pred = np.maximum(y_pred, 1e-7) assert np.allclose( ... loss.numpy(), ... np.mean( ... np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1)) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] .","title":"mean_squared_logarithmic_error function"},{"location":"dlf/metrics/Keras/#poisson-function","text":"tensorflow.keras.metrics.poisson(y_true, y_pred) Computes the Poisson loss between y_true and y_pred. The Poisson loss is the mean of the elements of the Tensor y_pred - y_true * log(y_pred) . Standalone usage: y_true = np.random.randint(0, 2, size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.poisson(y_true, y_pred) assert loss.shape == (2,) y_pred = y_pred + 1e-7 assert np.allclose( ... loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1), ... atol=1e-5) Args: y_true: Ground truth values. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Poisson loss value. shape = [batch_size, d0, .. dN-1] . Raises: InvalidArgumentError: If y_true and y_pred have incompatible shapes.","title":"poisson function"},{"location":"dlf/metrics/Keras/#serialize-function","text":"tensorflow.keras.metrics.serialize(metric) Serializes metric function or Metric instance. Arguments: metric: A Keras Metric instance or a metric function. Returns: Metric configuration dictionary.","title":"serialize function"},{"location":"dlf/metrics/Keras/#sparse_categorical_accuracy-function","text":"tensorflow.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) Calculates how often predictions matches integer labels. Standalone usage: y_true = [2, 1] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred) assert m.shape == (2,) m.numpy() array([0., 1.], dtype=float32) You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. Args: y_true: Integer ground truth values. y_pred: The prediction values. Returns: Sparse categorical accuracy values.","title":"sparse_categorical_accuracy function"},{"location":"dlf/metrics/Keras/#sparse_categorical_crossentropy-function","text":"tensorflow.keras.metrics.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1) Computes the sparse categorical crossentropy loss. Standalone usage: y_true = [1, 2] y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) assert loss.shape == (2,) loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Ground truth values. y_pred: The predicted values. from_logits: Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. axis: (Optional) Defaults to -1. The dimension along which the entropy is computed. Returns: Sparse categorical crossentropy loss value.","title":"sparse_categorical_crossentropy function"},{"location":"dlf/metrics/Keras/#sparse_top_k_categorical_accuracy-function","text":"tensorflow.keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5) Computes how often integer targets are in the top K predictions. Standalone usage: y_true = [2, 1] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.sparse_top_k_categorical_accuracy( ... y_true, y_pred, k=3) assert m.shape == (2,) m.numpy() array([1., 1.], dtype=float32) Args: y_true: tensor of true targets. y_pred: tensor of predicted targets. k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. Returns: Sparse top K categorical accuracy value.","title":"sparse_top_k_categorical_accuracy function"},{"location":"dlf/metrics/Keras/#squared_hinge-function","text":"tensorflow.keras.metrics.squared_hinge(y_true, y_pred) Computes the squared hinge loss between y_true and y_pred . loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1) Standalone usage: y_true = np.random.choice([-1, 1], size=(2, 3)) y_pred = np.random.random(size=(2, 3)) loss = tf.keras.losses.squared_hinge(y_true, y_pred) assert loss.shape == (2,) assert np.array_equal( ... loss.numpy(), ... np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)) Args: y_true: The ground truth values. y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1. shape = [batch_size, d0, .. dN] . y_pred: The predicted values. shape = [batch_size, d0, .. dN] . Returns: Squared hinge loss values. shape = [batch_size, d0, .. dN-1] .","title":"squared_hinge function"},{"location":"dlf/metrics/Keras/#top_k_categorical_accuracy-function","text":"tensorflow.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=5) Computes how often targets are in the top K predictions. Standalone usage: y_true = [[0, 0, 1], [0, 1, 0]] y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3) assert m.shape == (2,) m.numpy() array([1., 1.], dtype=float32) Args: y_true: The ground truth values. y_pred: The prediction values. k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5. Returns: Top K categorical accuracy value.","title":"top_k_categorical_accuracy function"},{"location":"dlf/metrics/SparseCategoricalCrossentropyIgnore/","text":"SparseCategoricalCrossentropyIgnore class dlf.metrics.sparse_categorial_crossentropy_ignore.SparseCategoricalCrossentropyIgnore( num_classes, from_logits=False, name=\"sparse_categorical_crossentropy_ignore\", **kwargs ) Implementation for sparse crossentropy metric with ignore label functionality. The same implmentation like tf.keras.metrics.SparseCategoricalCrossentropy but with the addtion that this implementation ignores all label ids <= num_classes. Arguments num_classes : int. Number of classes, everthing above will be ignored name : str, optonal, Name of the loss function. Defaults to 'sparse_categorical_crossentropy_ignore'. from_logits : bool. If true, y_pred is expected to be a logits tensor. Defaults to False. Returns A tf.keras.metrics.Metric YAML Configuration metrics: SparseCategoricalCrossentropyIgnore: from_logits: False num_classes: 7","title":"SparseCategoricalCrossentropyIgnore"},{"location":"dlf/metrics/SparseCategoricalCrossentropyIgnore/#sparsecategoricalcrossentropyignore-class","text":"dlf.metrics.sparse_categorial_crossentropy_ignore.SparseCategoricalCrossentropyIgnore( num_classes, from_logits=False, name=\"sparse_categorical_crossentropy_ignore\", **kwargs ) Implementation for sparse crossentropy metric with ignore label functionality. The same implmentation like tf.keras.metrics.SparseCategoricalCrossentropy but with the addtion that this implementation ignores all label ids <= num_classes. Arguments num_classes : int. Number of classes, everthing above will be ignored name : str, optonal, Name of the loss function. Defaults to 'sparse_categorical_crossentropy_ignore'. from_logits : bool. If true, y_pred is expected to be a logits tensor. Defaults to False. Returns A tf.keras.metrics.Metric YAML Configuration metrics: SparseCategoricalCrossentropyIgnore: from_logits: False num_classes: 7","title":"SparseCategoricalCrossentropyIgnore class"},{"location":"dlf/metrics/SparseMeanIoU/","text":"SparseMeanIoU class dlf.metrics.sparse_mean_iou.SparseMeanIoU(num_classes, name=\"sparse_mean_iou\", **kwargs) Implementation of MeanIoU metric for sparse tensors This implementation allows it to use sparse tensors as y_pred for the calculation of the mIoU. In addtion, all labels <= num_classes will be ignored. Arguments num_classes : int. Number of classes, everthing above or equal will be ignored name : str. Name of the metric. Defaults to 'sparse_mean_iou'. YAML Configuration metrics: SparseMeanIoU: num_classes: 7","title":"SparseMeanIoU"},{"location":"dlf/metrics/SparseMeanIoU/#sparsemeaniou-class","text":"dlf.metrics.sparse_mean_iou.SparseMeanIoU(num_classes, name=\"sparse_mean_iou\", **kwargs) Implementation of MeanIoU metric for sparse tensors This implementation allows it to use sparse tensors as y_pred for the calculation of the mIoU. In addtion, all labels <= num_classes will be ignored. Arguments num_classes : int. Number of classes, everthing above or equal will be ignored name : str. Name of the metric. Defaults to 'sparse_mean_iou'. YAML Configuration metrics: SparseMeanIoU: num_classes: 7","title":"SparseMeanIoU class"},{"location":"dlf/models/CenterNet/","text":"CenterNet class dlf.models.centernet.CenterNet( feature_extractor, num_classes=12, score_threshold=0.1, pooling_nms=True, dropout_rate=None, max_objects=100, weight_decay=None, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of CenterNet for object detection Aliases CenterNet center_net centernet Arguments feature_extractor : dict. A feature extractor num_classes : int. Number of classes to detect score_threshold : float, optional. Minimum classification score to be a valid box pooling_nms : bool, optional. If true, max pooling is used as non maximum suppression. Defaults to False. dropout_rate : float, optional. Dropout rate if None no dropout is used. Defaults to None. max_objects : int, optional. max number of objects to detect. Defaults to 100. weight_decay : float, optional. Weight decay factor. Defaults to None. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration model: centernet: feature_extractor: model_name: ResNet50 input_shape: - &width 512 - &height 512 - 3 weights: \"imagenet\" num_classes: 6 weight_decay: 0.0005 summary: True loss: centernetloss: optimizer: - Adam: # lr for simclr unsupervised learning_rate: PolynomialDecay: initial_learning_rate: 0.001 decay_steps: 195312 end_learning_rate: 0.00001 References Objects as Points Keras Implementation","title":"CenterNet"},{"location":"dlf/models/CenterNet/#centernet-class","text":"dlf.models.centernet.CenterNet( feature_extractor, num_classes=12, score_threshold=0.1, pooling_nms=True, dropout_rate=None, max_objects=100, weight_decay=None, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of CenterNet for object detection Aliases CenterNet center_net centernet Arguments feature_extractor : dict. A feature extractor num_classes : int. Number of classes to detect score_threshold : float, optional. Minimum classification score to be a valid box pooling_nms : bool, optional. If true, max pooling is used as non maximum suppression. Defaults to False. dropout_rate : float, optional. Dropout rate if None no dropout is used. Defaults to None. max_objects : int, optional. max number of objects to detect. Defaults to 100. weight_decay : float, optional. Weight decay factor. Defaults to None. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration model: centernet: feature_extractor: model_name: ResNet50 input_shape: - &width 512 - &height 512 - 3 weights: \"imagenet\" num_classes: 6 weight_decay: 0.0005 summary: True loss: centernetloss: optimizer: - Adam: # lr for simclr unsupervised learning_rate: PolynomialDecay: initial_learning_rate: 0.001 decay_steps: 195312 end_learning_rate: 0.00001 References Objects as Points Keras Implementation","title":"CenterNet class"},{"location":"dlf/models/CycleGAN/","text":"CycleGAN class dlf.models.cycle_gan.CycleGAN( input_shape, generator=\"unet\", generator_filters=32, discriminator_filters=64, use_perceptual_loss=True, cycle_weight=10.0, identity_weight=1.0, model_weights=None, summary=False, optimizer=None, resnet_blocks=9, **kwargs ) A CycleGAN implementation This class contains a CycleGAN implemenation with an U-Net architecture as generator. Next to the default implementation of a cycle adversarial loss, mentioned in CycleGAN paper, this implementations allows it to add an optional perceptual loss function. Aliases cycle_gan CycleGAN Args input_shape : tuple(int, int , int). Input shape of this network generator : str {'unet', 'resnet'}. Defines the generator type. Defaults to unet generator_filters : int, optional. Number of CNN filters used for generator. Defaults to 32. discriminator_filters : int, optional. Number of CNN filters used for discriminator. Defaults to 64. use_perceptual_loss : bool, optional. If true, in addition to the CycleGAN implementated losses the perceptual loss is used. Defaults to True. cycle_weight : float, optional. Weight for cycle consistency loss. Defaults to 10.0. identity_weigh : float, optional. Weight for identity loss Defaults to 1.0. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. resnet_blocks : int. Only available if generator is resnet. Defaults to 9. Raises ValueError : If not exactly four optimizer are specified YAML Configuration model: cycle_gan: input_shape: - 512 - 512 - 3 generator: unet generator_filters: 32 discriminator_filters: 64 use_perceptual_loss: False summary: True optimizer: - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 References CycleGAN: https://arxiv.org/abs/1703.10593 U-Net: https://arxiv.org/abs/1505.04597 Perceptual Loss: https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf https://www.tensorflow.org/tutorials/generative/cyclegan https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py","title":"CycleGAN"},{"location":"dlf/models/CycleGAN/#cyclegan-class","text":"dlf.models.cycle_gan.CycleGAN( input_shape, generator=\"unet\", generator_filters=32, discriminator_filters=64, use_perceptual_loss=True, cycle_weight=10.0, identity_weight=1.0, model_weights=None, summary=False, optimizer=None, resnet_blocks=9, **kwargs ) A CycleGAN implementation This class contains a CycleGAN implemenation with an U-Net architecture as generator. Next to the default implementation of a cycle adversarial loss, mentioned in CycleGAN paper, this implementations allows it to add an optional perceptual loss function. Aliases cycle_gan CycleGAN Args input_shape : tuple(int, int , int). Input shape of this network generator : str {'unet', 'resnet'}. Defines the generator type. Defaults to unet generator_filters : int, optional. Number of CNN filters used for generator. Defaults to 32. discriminator_filters : int, optional. Number of CNN filters used for discriminator. Defaults to 64. use_perceptual_loss : bool, optional. If true, in addition to the CycleGAN implementated losses the perceptual loss is used. Defaults to True. cycle_weight : float, optional. Weight for cycle consistency loss. Defaults to 10.0. identity_weigh : float, optional. Weight for identity loss Defaults to 1.0. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. resnet_blocks : int. Only available if generator is resnet. Defaults to 9. Raises ValueError : If not exactly four optimizer are specified YAML Configuration model: cycle_gan: input_shape: - 512 - 512 - 3 generator: unet generator_filters: 32 discriminator_filters: 64 use_perceptual_loss: False summary: True optimizer: - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 - Adam: learning_rate: 0.0002 beta_1: 0.5 References CycleGAN: https://arxiv.org/abs/1703.10593 U-Net: https://arxiv.org/abs/1505.04597 Perceptual Loss: https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf https://www.tensorflow.org/tutorials/generative/cyclegan https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py","title":"CycleGAN class"},{"location":"dlf/models/DeeplabV3Plus/","text":"DeeplabV3Plus class dlf.models.deeplabl_v3_plus.DeeplabV3Plus( input_shape, num_classes, backbone=\"xception\", OS=16, alpha=1.0, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) Deeplabv3+ architecture Optionally loads weights pre-trained on PASCAL VOC or Cityscapes. This model is available for TensorFlow only. Aliases: deeplab_v3_plus DeeplabV3Plus Arguments input_shape : shape of input image. format HxWxC PASCAL VOC model was trained on (512,512,3) images. None is allowed as shape/width num_classes : number of desired classes. PASCAL VOC has 21 classes, Cityscapes has 19 classes. If number of classes not aligned with the weights used, last layer is initialized randomly backbone : backbone to use. one of {'xception','mobilenetv2'} OS : determines input_shape/feature_extractor_output ratio. One of {8,16}. Used only for xception backbone. alpha : controls the width of the MobileNetV2 network. This is known as the width multiplier in the MobileNetV2 paper. - If alpha < 1.0, proportionally decreases the number of filters in each layer. - If alpha > 1.0, proportionally increases the number of filters in each layer. - If alpha = 1, default number of filters from the paper are used at each layer. Used only for mobilenetv2 backbone. Pretrained is only available for alpha=1. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. Returns A Keras model instance. Raises RuntimeError : If attempting to run this model with a backend that does not support separable convolutions. ValueError : in case of invalid argument for weights or backbone YAML Configuration model: DeeplabV3Plus: input_shape: - 512 - 512 - 3 num_classes: 7 output_shape: - 512 - 512 model_weights: None backbone: xception OS: 16 alpha: 1.0 summary: True optimizer: - Adam: learning_rate: 0.0001 References keras-deeplab-v3-plus Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Xception: Deep Learning with Depthwise Separable Convolutions Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation","title":"DeeplabV3Plus"},{"location":"dlf/models/DeeplabV3Plus/#deeplabv3plus-class","text":"dlf.models.deeplabl_v3_plus.DeeplabV3Plus( input_shape, num_classes, backbone=\"xception\", OS=16, alpha=1.0, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) Deeplabv3+ architecture Optionally loads weights pre-trained on PASCAL VOC or Cityscapes. This model is available for TensorFlow only. Aliases: deeplab_v3_plus DeeplabV3Plus Arguments input_shape : shape of input image. format HxWxC PASCAL VOC model was trained on (512,512,3) images. None is allowed as shape/width num_classes : number of desired classes. PASCAL VOC has 21 classes, Cityscapes has 19 classes. If number of classes not aligned with the weights used, last layer is initialized randomly backbone : backbone to use. one of {'xception','mobilenetv2'} OS : determines input_shape/feature_extractor_output ratio. One of {8,16}. Used only for xception backbone. alpha : controls the width of the MobileNetV2 network. This is known as the width multiplier in the MobileNetV2 paper. - If alpha < 1.0, proportionally decreases the number of filters in each layer. - If alpha > 1.0, proportionally increases the number of filters in each layer. - If alpha = 1, default number of filters from the paper are used at each layer. Used only for mobilenetv2 backbone. Pretrained is only available for alpha=1. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. Returns A Keras model instance. Raises RuntimeError : If attempting to run this model with a backend that does not support separable convolutions. ValueError : in case of invalid argument for weights or backbone YAML Configuration model: DeeplabV3Plus: input_shape: - 512 - 512 - 3 num_classes: 7 output_shape: - 512 - 512 model_weights: None backbone: xception OS: 16 alpha: 1.0 summary: True optimizer: - Adam: learning_rate: 0.0001 References keras-deeplab-v3-plus Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation Xception: Deep Learning with Depthwise Separable Convolutions Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation","title":"DeeplabV3Plus class"},{"location":"dlf/models/FCN/","text":"FCN class dlf.models.fcn.FCN( feature_extractor, num_classes, output_shape, skip_layers=None, interpolation_bilinear=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of a Fully Convolutional Network Aliases FCN fcn Arguments feature_extractor : dict. A feature extractor num_classes : int. Number of classes used for segmentation output_shape : tuple or list. Output dimension (width, height) of the FCN skip_layers : list of str, optional. A list of layer which should be used for FCN skip connections. Defaults to None. interpolation_bilinear : bool, optional. If true bilinear instead of nearest is used as upsampling method. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration Example usage of a FCN model in an experiment configuration. This example initiaizes the ResNet50 from tf.keras.applications package as feature extractor and an input shape of 512x512x3. Three layer of ResNet50 where used as skip connection. model: fcn: feature_extractor: model_name: ResNet50 input_shape: - 512 - 512 - 3 output_shape: - 512 - 512 skip_layers: - conv3_block4_out - conv4_block6_out - conv5_block1_out interpolation_bilinear: True num_classes: 7 summary: True optimizer: - Adam: learning_rate: 0.0001","title":"FCN"},{"location":"dlf/models/FCN/#fcn-class","text":"dlf.models.fcn.FCN( feature_extractor, num_classes, output_shape, skip_layers=None, interpolation_bilinear=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of a Fully Convolutional Network Aliases FCN fcn Arguments feature_extractor : dict. A feature extractor num_classes : int. Number of classes used for segmentation output_shape : tuple or list. Output dimension (width, height) of the FCN skip_layers : list of str, optional. A list of layer which should be used for FCN skip connections. Defaults to None. interpolation_bilinear : bool, optional. If true bilinear instead of nearest is used as upsampling method. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration Example usage of a FCN model in an experiment configuration. This example initiaizes the ResNet50 from tf.keras.applications package as feature extractor and an input shape of 512x512x3. Three layer of ResNet50 where used as skip connection. model: fcn: feature_extractor: model_name: ResNet50 input_shape: - 512 - 512 - 3 output_shape: - 512 - 512 skip_layers: - conv3_block4_out - conv4_block6_out - conv5_block1_out interpolation_bilinear: True num_classes: 7 summary: True optimizer: - Adam: learning_rate: 0.0001","title":"FCN class"},{"location":"dlf/models/FeatureExtractor/","text":"FeatureExtractor class dlf.models.feature_extractor.FeatureExtractor( model_name, input_shape, weights=\"imagenet\", last_layer=None, freeze=False, **kwargs ) Feature Extractor This implementation allows it to initialize pre-trained Keras models for the usage as feature extractor. Aliases feature_extractor FeatureExtractor Arguments model_name : str. Name of the model e.g. ResNet50 input_shape : tuple. Tuple containing the dimension of the input weights : str. path to weights or just 'imagenet'. Defaults to 'imagenet'. freeze : bool. If false the feature extractor is not trainable. Defaults to False. last_layer : str. Name of the last layer, used as feature extractor. Defaults to None. Returns A Keras model instance.","title":"FeatureExtractor"},{"location":"dlf/models/FeatureExtractor/#featureextractor-class","text":"dlf.models.feature_extractor.FeatureExtractor( model_name, input_shape, weights=\"imagenet\", last_layer=None, freeze=False, **kwargs ) Feature Extractor This implementation allows it to initialize pre-trained Keras models for the usage as feature extractor. Aliases feature_extractor FeatureExtractor Arguments model_name : str. Name of the model e.g. ResNet50 input_shape : tuple. Tuple containing the dimension of the input weights : str. path to weights or just 'imagenet'. Defaults to 'imagenet'. freeze : bool. If false the feature extractor is not trainable. Defaults to False. last_layer : str. Name of the last layer, used as feature extractor. Defaults to None. Returns A Keras model instance.","title":"FeatureExtractor class"},{"location":"dlf/models/ResNet18/","text":"ResNet18 class dlf.models.classification.resnet.ResNet18(input_shape, num_classes=None, **kwargs) ResNet with 18 layers and v2 residual units","title":"ResNet18"},{"location":"dlf/models/ResNet18/#resnet18-class","text":"dlf.models.classification.resnet.ResNet18(input_shape, num_classes=None, **kwargs) ResNet with 18 layers and v2 residual units","title":"ResNet18 class"},{"location":"dlf/models/ResNet34/","text":"ResNet34 class dlf.models.classification.resnet.ResNet34(input_shape, num_classes=None, **kwargs) ResNet with 34 layers and v2 residual units","title":"ResNet34"},{"location":"dlf/models/ResNet34/#resnet34-class","text":"dlf.models.classification.resnet.ResNet34(input_shape, num_classes=None, **kwargs) ResNet with 34 layers and v2 residual units","title":"ResNet34 class"},{"location":"dlf/models/SSD/","text":"SSD class dlf.models.ssd.SSD( num_classes, arch=\"vgg_ssd_300\", input_shape=(300, 300, 3), weight_decay=0.0005, optimizer=None, loss=None, model_weights=None, summary=False, ratios=None, scales=None, class_score_threshold=0.6, nms_threshold=0.45, max_boxes=200, **kwargs ) A implementation for an Single Shot Detector (SSD) Aliases ssd SSD Architectures implemented vgg_ssd_300: Original Paper implmentation of SSD 300 with VGG as backend vgg_ssd_512: Original Paper implmentation of SSD 512 with VGG as backend Args num_classes : arch : str. SSD architecture. Defaults to 'vgg_ssd_300'. input_shape : tuple(int, int , int). Input shape of this network. Defaults to (300, 300, 3). weight_decay : float. Weight decay. Defaults to 5e-4. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. Not Implemented Yet! List of loss objects to build for this model. Defaults to None. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. ratios : list of list of float, optional. List of anchor box ratios for SSD. Defaults to None. scales : list of float. List of scalings for anchor boxes. Defaults to None. class_score_threshold : float, optional. Class confidence threshold . Defaults to 0.6. nms_threshold : float. Min IoU for non-maximum suppression. Defaults to 0.45. max_boxes : int, optional. Max number of Boxes. Defaults to 200. Returns A Keras model instance. YAML Configuration model: ssd: num_classes: &num_classes 7 input_shape: - 512 - 512 - 3 summary: True optimizer: - Adam: learning_rate: 0.001 class_score_threshold: 0.6 nms_threshold: 0.45 max_boxes: 200 arch: vgg_ssd_512 References Single Shot Detector https://arxiv.org/abs/1512.02325","title":"SSD"},{"location":"dlf/models/SSD/#ssd-class","text":"dlf.models.ssd.SSD( num_classes, arch=\"vgg_ssd_300\", input_shape=(300, 300, 3), weight_decay=0.0005, optimizer=None, loss=None, model_weights=None, summary=False, ratios=None, scales=None, class_score_threshold=0.6, nms_threshold=0.45, max_boxes=200, **kwargs ) A implementation for an Single Shot Detector (SSD) Aliases ssd SSD Architectures implemented vgg_ssd_300: Original Paper implmentation of SSD 300 with VGG as backend vgg_ssd_512: Original Paper implmentation of SSD 512 with VGG as backend Args num_classes : arch : str. SSD architecture. Defaults to 'vgg_ssd_300'. input_shape : tuple(int, int , int). Input shape of this network. Defaults to (300, 300, 3). weight_decay : float. Weight decay. Defaults to 5e-4. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. Not Implemented Yet! List of loss objects to build for this model. Defaults to None. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. ratios : list of list of float, optional. List of anchor box ratios for SSD. Defaults to None. scales : list of float. List of scalings for anchor boxes. Defaults to None. class_score_threshold : float, optional. Class confidence threshold . Defaults to 0.6. nms_threshold : float. Min IoU for non-maximum suppression. Defaults to 0.45. max_boxes : int, optional. Max number of Boxes. Defaults to 200. Returns A Keras model instance. YAML Configuration model: ssd: num_classes: &num_classes 7 input_shape: - 512 - 512 - 3 summary: True optimizer: - Adam: learning_rate: 0.001 class_score_threshold: 0.6 nms_threshold: 0.45 max_boxes: 200 arch: vgg_ssd_512","title":"SSD class"},{"location":"dlf/models/SSD/#references","text":"Single Shot Detector https://arxiv.org/abs/1512.02325","title":"References"},{"location":"dlf/models/SimCLR/","text":"SimCLR class dlf.models.simclr.SimCLR( feature_extractor, num_projection_dimension=128, linear_evaluation=None, weight_decay=1e-06, use_batch_norm=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of SimCLR Aliases SimCLR sim_clr simclr Arguments feature_extractor : dict. A feature extractor num_projection_dimension : int. Number of projection dimensions for unsupervised approach linear_evaluation : dict. Defines linear evaulation options, if None linear evaluation is desabled. Defaults to None. num_classes: int. Number of classes for linear evaluation linear_train_at_step: int. Number at which step the linear evaluation starts freeze_base_network: bool. If true, the feature extractor is not trainable during linear evaluation. Defaults to True. weight_decay : float. Amount of weight decay to use. Defaults to 1e-6. use_batch_norm : bool. If true, batch norm for the projection head is used. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration model: simclr: feature_extractor: model_name: ResNet50 input_shape: - &width 224 - &height 224 - 3 num_projection_dimension: 128 linear_evaluation: num_classes: 62 linear_train_at_step: 5000 freeze_base_network: False num_classes: 62 use_batch_norm: True linear_train_at_step: 5000 weight_decay: 1e-6 summary: True loss: NTXentLoss: batch_size: &batch_size 16 temperature: 0.5 use_cosine_similarity: True # categorical_labels is True in reader otherwise it should be SparseCategoricalCrossentropy CategoricalCrossentropy: from_logits: True reduction: \"none\" optimizer: - Adam: # lr for simclr unsupervised learning_rate: 0.0001 - SGD: # lr for linear evaluation learning_rate: 0.001 momentum: 0.9 References A Simple Framework for Contrastive Learning of Visual Representations","title":"SimCLR"},{"location":"dlf/models/SimCLR/#simclr-class","text":"dlf.models.simclr.SimCLR( feature_extractor, num_projection_dimension=128, linear_evaluation=None, weight_decay=1e-06, use_batch_norm=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A implementation of SimCLR Aliases SimCLR sim_clr simclr Arguments feature_extractor : dict. A feature extractor num_projection_dimension : int. Number of projection dimensions for unsupervised approach linear_evaluation : dict. Defines linear evaulation options, if None linear evaluation is desabled. Defaults to None. num_classes: int. Number of classes for linear evaluation linear_train_at_step: int. Number at which step the linear evaluation starts freeze_base_network: bool. If true, the feature extractor is not trainable during linear evaluation. Defaults to True. weight_decay : float. Amount of weight decay to use. Defaults to 1e-6. use_batch_norm : bool. If true, batch norm for the projection head is used. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. YAML Configuration model: simclr: feature_extractor: model_name: ResNet50 input_shape: - &width 224 - &height 224 - 3 num_projection_dimension: 128 linear_evaluation: num_classes: 62 linear_train_at_step: 5000 freeze_base_network: False num_classes: 62 use_batch_norm: True linear_train_at_step: 5000 weight_decay: 1e-6 summary: True loss: NTXentLoss: batch_size: &batch_size 16 temperature: 0.5 use_cosine_similarity: True # categorical_labels is True in reader otherwise it should be SparseCategoricalCrossentropy CategoricalCrossentropy: from_logits: True reduction: \"none\" optimizer: - Adam: # lr for simclr unsupervised learning_rate: 0.0001 - SGD: # lr for linear evaluation learning_rate: 0.001 momentum: 0.9 References A Simple Framework for Contrastive Learning of Visual Representations","title":"SimCLR class"},{"location":"dlf/models/VggEncoderDecoder/","text":"VggEncoderDecoder class dlf.models.vgg_encoder_decoder.VggEncoderDecoder( input_shape, num_classes, use_skip_layers=True, use_transposed_conv=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A SegNet like implementation of an Encoder-Decoder using VGG16. Compared to SegNet this implementation differs by using a 2D UpSampling layer (bilinear or nearest). Optional you can use the skip connections SegNet: https://arxiv.org/abs/1511.00561 Aliases: VggEncoderDecoder vgg_encoder_decoder Args: input_shape : tuple(int, int , int). Input shape of this network num_classes : int. Number of classes use_skip_layers : bool, optional. If true the network uses skip layers. Defaults to True. use_transposed_conv : bool, optional. If true a transposed convolution instead of UpSample2D is used. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. Returns A Keras model instance. YAML Configuration model: vgg_encoder_decoder: input_shape: - 512 - 512 - 3 num_classes: &num_classes 7 use_skip_layers: True use_transposed_conv: True model_weights: None summary: True optimizer: - Adam: learning_rate: 0.00001 loss: - SparseCategoricalCrossentropyIgnore: num_classes: *num_classes from_logits: False","title":"VggEncoderDecoder"},{"location":"dlf/models/VggEncoderDecoder/#vggencoderdecoder-class","text":"dlf.models.vgg_encoder_decoder.VggEncoderDecoder( input_shape, num_classes, use_skip_layers=True, use_transposed_conv=True, model_weights=None, summary=False, optimizer=None, loss=None, **kwargs ) A SegNet like implementation of an Encoder-Decoder using VGG16. Compared to SegNet this implementation differs by using a 2D UpSampling layer (bilinear or nearest). Optional you can use the skip connections SegNet: https://arxiv.org/abs/1511.00561 Aliases: VggEncoderDecoder vgg_encoder_decoder Args: input_shape : tuple(int, int , int). Input shape of this network num_classes : int. Number of classes use_skip_layers : bool, optional. If true the network uses skip layers. Defaults to True. use_transposed_conv : bool, optional. If true a transposed convolution instead of UpSample2D is used. Defaults to True. model_weights : str, optional. Path to the pretrained model weights. Defaults to None. summary : bool, optional. If true a summary of the model will be printed to stdout. Defaults to False. optimizer : list of dict, optional. Name of optimizer used for training. loss : list of dict, optional. List of loss objects to build for this model. Defaults to None. Returns A Keras model instance. YAML Configuration model: vgg_encoder_decoder: input_shape: - 512 - 512 - 3 num_classes: &num_classes 7 use_skip_layers: True use_transposed_conv: True model_weights: None summary: True optimizer: - Adam: learning_rate: 0.00001 loss: - SparseCategoricalCrossentropyIgnore: num_classes: *num_classes from_logits: False","title":"VggEncoderDecoder class"},{"location":"dlf/preprocessing_methods/RandomBlur/","text":"RandomBlur class dlf.preprocessing.blur.RandomBlur(size=3, mean=0, std=5) Data augmentation method that applies randomly a Gaussian blur to an image Aliases blur random_blur Blur RandomBlur Arguments size : float. Size of the convolutional kernel. Defaults to 3. mean : float. Mean value, centre of the distribution. Defaults to 0. std : float. Standard deviation or width of the distribution. Defaults to 5. YAML Configuration preprocess_list: blur: size: 5 mean: 0.0 std: 6","title":"RandomBlur"},{"location":"dlf/preprocessing_methods/RandomBlur/#randomblur-class","text":"dlf.preprocessing.blur.RandomBlur(size=3, mean=0, std=5) Data augmentation method that applies randomly a Gaussian blur to an image Aliases blur random_blur Blur RandomBlur Arguments size : float. Size of the convolutional kernel. Defaults to 3. mean : float. Mean value, centre of the distribution. Defaults to 0. std : float. Standard deviation or width of the distribution. Defaults to 5. YAML Configuration preprocess_list: blur: size: 5 mean: 0.0 std: 6","title":"RandomBlur class"},{"location":"dlf/preprocessing_methods/RandomBrightness/","text":"RandomBrightness class dlf.preprocessing.brightness.RandomBrightness(max_delta=0.2) Data augmentation method that randomly adjust the brightness of an RGB image Aliases brightness Brightness random_brightness RandomBrightness Arguments max_delta : float [0,1). Scalar that is added to the pixel. Defaults to 0.2. YAML Configuration preprocess_list: brigthness: max_delta: 0.2","title":"RandomBrightness"},{"location":"dlf/preprocessing_methods/RandomBrightness/#randombrightness-class","text":"dlf.preprocessing.brightness.RandomBrightness(max_delta=0.2) Data augmentation method that randomly adjust the brightness of an RGB image Aliases brightness Brightness random_brightness RandomBrightness Arguments max_delta : float [0,1). Scalar that is added to the pixel. Defaults to 0.2. YAML Configuration preprocess_list: brigthness: max_delta: 0.2","title":"RandomBrightness class"},{"location":"dlf/preprocessing_methods/RandomColorDistortion/","text":"RandomColorDistortion class dlf.preprocessing.color_distortion.RandomColorDistortion( s=1.0, color_jitter_probability=0.8, color_drop_probability=0.2 ) Data augmentation method that randomly distorts colors of an RGB image Aliases color_distortion ColorDistortion random_color_distortion RandomColorDistortion Arguments s : float. Scalar that scales the distortion effect. Defaults to 1.0. color_jitter_probability : float [0,1]. Probability that color jittering is applied. Defaults to 0.8 color_drop_probability : float [0,1]. Probability that color drop (RGB to Gray) is applied. Defaults to 0.8 YAML Configuration preprocess_list: color_distortion: s: 0.7 color_jitter_probability: 0.5 color_drop_probability: 0.3 References SimCLR: https://arxiv.org/abs/2002.05709","title":"RandomColorDistortion"},{"location":"dlf/preprocessing_methods/RandomColorDistortion/#randomcolordistortion-class","text":"dlf.preprocessing.color_distortion.RandomColorDistortion( s=1.0, color_jitter_probability=0.8, color_drop_probability=0.2 ) Data augmentation method that randomly distorts colors of an RGB image Aliases color_distortion ColorDistortion random_color_distortion RandomColorDistortion Arguments s : float. Scalar that scales the distortion effect. Defaults to 1.0. color_jitter_probability : float [0,1]. Probability that color jittering is applied. Defaults to 0.8 color_drop_probability : float [0,1]. Probability that color drop (RGB to Gray) is applied. Defaults to 0.8 YAML Configuration preprocess_list: color_distortion: s: 0.7 color_jitter_probability: 0.5 color_drop_probability: 0.3 References SimCLR: https://arxiv.org/abs/2002.05709","title":"RandomColorDistortion class"},{"location":"dlf/preprocessing_methods/RandomCrop/","text":"RandomCrop class dlf.preprocessing.crop.RandomCrop(width, height, scale=1.5) Data augmentation method that randomly crops an RGB image Aliases crop Crop RandomCrop random_crop Arguments width : int. Width of the cropped image height : int. Height of the cropped image YAML Configuration preprocess_list: crop: width: 512 height: 512","title":"RandomCrop"},{"location":"dlf/preprocessing_methods/RandomCrop/#randomcrop-class","text":"dlf.preprocessing.crop.RandomCrop(width, height, scale=1.5) Data augmentation method that randomly crops an RGB image Aliases crop Crop RandomCrop random_crop Arguments width : int. Width of the cropped image height : int. Height of the cropped image YAML Configuration preprocess_list: crop: width: 512 height: 512","title":"RandomCrop class"},{"location":"dlf/preprocessing_methods/RandomFisheye/","text":"RandomFisheye class dlf.preprocessing.fisheye.RandomFisheye(fov=179.9) Data augmentation method to create an interpolated fisheye distorted image of a perspective image Aliases fisheye fisheye_distortion random_fisheye radnom_fisheye_distortion Arguments fov : float, optional. Field of view for distortion. Defaults to 179.0. Example YAML Configuration preprocess_list: fisheye: Warning This module is very experimental and slow. You should install numba ( pip install numba ) otherwise it's realy unusable. Note The bottleneck of this function is remap = self.fisheye_transform(*image.shape[:-1]) . Maybe we should write it in cython or the are further optimizations available.","title":"RandomFisheye"},{"location":"dlf/preprocessing_methods/RandomFisheye/#randomfisheye-class","text":"dlf.preprocessing.fisheye.RandomFisheye(fov=179.9) Data augmentation method to create an interpolated fisheye distorted image of a perspective image Aliases fisheye fisheye_distortion random_fisheye radnom_fisheye_distortion Arguments fov : float, optional. Field of view for distortion. Defaults to 179.0. Example YAML Configuration preprocess_list: fisheye: Warning This module is very experimental and slow. You should install numba ( pip install numba ) otherwise it's realy unusable. Note The bottleneck of this function is remap = self.fisheye_transform(*image.shape[:-1]) . Maybe we should write it in cython or the are further optimizations available.","title":"RandomFisheye class"},{"location":"dlf/preprocessing_methods/RandomHorizontalFlip/","text":"RandomHorizontalFlip class dlf.preprocessing.flip.RandomHorizontalFlip() Data augmentation method that randomly horizontal flips a RGB image Aliases h_flip horizontal_flip HFlip HorizontalFlip YAML Configuration preprocess_list: h_flip:","title":"RandomHorizontalFlip"},{"location":"dlf/preprocessing_methods/RandomHorizontalFlip/#randomhorizontalflip-class","text":"dlf.preprocessing.flip.RandomHorizontalFlip() Data augmentation method that randomly horizontal flips a RGB image Aliases h_flip horizontal_flip HFlip HorizontalFlip YAML Configuration preprocess_list: h_flip:","title":"RandomHorizontalFlip class"},{"location":"dlf/preprocessing_methods/RandomHue/","text":"RandomHue class dlf.preprocessing.hue.RandomHue(max_delta=0.2) Data augmentation method that randomly adjust the hue of a RGB image Aliases hue Hue random_hue RandomHue Arguments max_delta : float [-1,1]. Scalar that is added to the hue. Defaults to 0.2. YAML Configuration preprocess_list: hue: max_delta: 0.2","title":"RandomHue"},{"location":"dlf/preprocessing_methods/RandomHue/#randomhue-class","text":"dlf.preprocessing.hue.RandomHue(max_delta=0.2) Data augmentation method that randomly adjust the hue of a RGB image Aliases hue Hue random_hue RandomHue Arguments max_delta : float [-1,1]. Scalar that is added to the hue. Defaults to 0.2. YAML Configuration preprocess_list: hue: max_delta: 0.2","title":"RandomHue class"},{"location":"dlf/preprocessing_methods/RandomNoise/","text":"RandomNoise class dlf.preprocessing.noise.RandomNoise(mean=0.0, std=5.0, color=True) Data augmentation method that randomly adds Gaussian noise to a RGB image Aliases noise random_noise Noise RandomNoise Arguments mean : float. Mean value, centre of the distribution. Defaults to 0. std : float. Standard deviation or width of the distribution. Defaults to 5. color : bool. If true, noise is added to color channels separately. Defaults to True. YAML Configuration preprocess_list: noise: mean: 0.0 std: 0.8 color: False","title":"RandomNoise"},{"location":"dlf/preprocessing_methods/RandomNoise/#randomnoise-class","text":"dlf.preprocessing.noise.RandomNoise(mean=0.0, std=5.0, color=True) Data augmentation method that randomly adds Gaussian noise to a RGB image Aliases noise random_noise Noise RandomNoise Arguments mean : float. Mean value, centre of the distribution. Defaults to 0. std : float. Standard deviation or width of the distribution. Defaults to 5. color : bool. If true, noise is added to color channels separately. Defaults to True. YAML Configuration preprocess_list: noise: mean: 0.0 std: 0.8 color: False","title":"RandomNoise class"},{"location":"dlf/preprocessing_methods/RandomSaturation/","text":"RandomSaturation class dlf.preprocessing.saturation.RandomSaturation(lower=0.8, upper=1.25) Data augmentation method that randomly adjust the saturation of a RGB image Aliases saturation Saturation RandomStaturation random_staturation Arguments lower : float. Lowest value that is added to the saturation channel of a HSV converted image upper : float. Highest value that is added to the saturation channel of a HSV converted image YAML Configuration preprocess_list: saturation: lower: 0.3 upper: 1.5","title":"RandomSaturation"},{"location":"dlf/preprocessing_methods/RandomSaturation/#randomsaturation-class","text":"dlf.preprocessing.saturation.RandomSaturation(lower=0.8, upper=1.25) Data augmentation method that randomly adjust the saturation of a RGB image Aliases saturation Saturation RandomStaturation random_staturation Arguments lower : float. Lowest value that is added to the saturation channel of a HSV converted image upper : float. Highest value that is added to the saturation channel of a HSV converted image YAML Configuration preprocess_list: saturation: lower: 0.3 upper: 1.5","title":"RandomSaturation class"},{"location":"dlf/preprocessing_methods/RandomTransform/","text":"RandomTransform class dlf.preprocessing.transform.RandomTransform( min_pitch=-60.0, max_pitch=60.0, min_roll=-60.0, max_roll=60.0, min_yaw=-60.0, max_yaw=60.0, min_translate_x=-1.0, max_translate_x=1.0, min_translate_y=-1.0, max_translate_y=1.0, min_translate_z=-1.0, max_translate_z=1.0, min_scale_x=1.0, max_scale_x=1.0, min_scale_y=1.0, max_scale_y=1.0, min_shear_x=-5.0, max_shear_x=5.0, min_shear_y=-5.0, max_shear_y=5.0, min_shear_z=-5.0, max_shear_z=5.0, ) Data augmentation method that randomly transforms an image with 6 DoF Aliases transform 6dof Transform random_transform RandomTransform Arguments min_pitch : float, optional. Minimum ptich angle. Defaults to -60.0. max_pitch : float, optional. Maximum ptich angle. Defaults to 60.0. min_roll : float, optional. Minimum roll angle. Defaults to -60.0. max_roll : float, optional. Maximum roll angle. Defaults to 60.0. min_yaw : float, optional. Minimum yaw angle. Defaults to -60.0. max_yaw : float, optional. Maximum yaw angle. Defaults to 60.0. min_translate_x : float, optional. Minimum x translation postion (1.0 means 100% of width). Defaults to -1.0. max_translate_x : float, optional. Maximum x translation postion (1.0 means 100% of width). Defaults to 1.0. min_translate_y : float, optional. Minimum y translation postion (1.0 means 100% of height). Defaults to -1.0. max_translate_y : float, optional. Maximum y translation postion (1.0 means 100% of height). Defaults to 1.0. min_translate_z : float, optional. Minimum z translation postion (1.0 means 100% of height). Defaults to -1.0. max_translate_z : float, optional. Maximum z translation postion (1.0 means 100% of height). Defaults to 1.0. min_scale_x : float, optional. Minimum x scale. Defaults to 1.0. max_scale_x : float, optional. Maximum x scale. Defaults to 1.0. min_scale_y : float, optional. Minimum y scale. Defaults to 1.0. max_scale_y : float, optional. Maximum y scale. Defaults to 1.0. min_shear_x : float, optional. Minimum x shear. Defaults to -5.0. max_shear_x : float, optional. Maximum x shear. Defaults to 5.0. min_shear_y : float, optional. Minimum y shear. Defaults to -5.0. max_shear_y : float, optional. Maximum y shear. Defaults to 5.0. min_shear_z : float, optional. Minimum z shear. Defaults to -5.0. max_shear_z : float, optional. Maximum z shear. Defaults to 5.0. Example YAML Configuration preprocess_list: transform: References: https://towardsdatascience.com/how-to-transform-a-2d-image-into-a-3d-space-5fc2306e3d36","title":"RandomTransform"},{"location":"dlf/preprocessing_methods/RandomTransform/#randomtransform-class","text":"dlf.preprocessing.transform.RandomTransform( min_pitch=-60.0, max_pitch=60.0, min_roll=-60.0, max_roll=60.0, min_yaw=-60.0, max_yaw=60.0, min_translate_x=-1.0, max_translate_x=1.0, min_translate_y=-1.0, max_translate_y=1.0, min_translate_z=-1.0, max_translate_z=1.0, min_scale_x=1.0, max_scale_x=1.0, min_scale_y=1.0, max_scale_y=1.0, min_shear_x=-5.0, max_shear_x=5.0, min_shear_y=-5.0, max_shear_y=5.0, min_shear_z=-5.0, max_shear_z=5.0, ) Data augmentation method that randomly transforms an image with 6 DoF Aliases transform 6dof Transform random_transform RandomTransform Arguments min_pitch : float, optional. Minimum ptich angle. Defaults to -60.0. max_pitch : float, optional. Maximum ptich angle. Defaults to 60.0. min_roll : float, optional. Minimum roll angle. Defaults to -60.0. max_roll : float, optional. Maximum roll angle. Defaults to 60.0. min_yaw : float, optional. Minimum yaw angle. Defaults to -60.0. max_yaw : float, optional. Maximum yaw angle. Defaults to 60.0. min_translate_x : float, optional. Minimum x translation postion (1.0 means 100% of width). Defaults to -1.0. max_translate_x : float, optional. Maximum x translation postion (1.0 means 100% of width). Defaults to 1.0. min_translate_y : float, optional. Minimum y translation postion (1.0 means 100% of height). Defaults to -1.0. max_translate_y : float, optional. Maximum y translation postion (1.0 means 100% of height). Defaults to 1.0. min_translate_z : float, optional. Minimum z translation postion (1.0 means 100% of height). Defaults to -1.0. max_translate_z : float, optional. Maximum z translation postion (1.0 means 100% of height). Defaults to 1.0. min_scale_x : float, optional. Minimum x scale. Defaults to 1.0. max_scale_x : float, optional. Maximum x scale. Defaults to 1.0. min_scale_y : float, optional. Minimum y scale. Defaults to 1.0. max_scale_y : float, optional. Maximum y scale. Defaults to 1.0. min_shear_x : float, optional. Minimum x shear. Defaults to -5.0. max_shear_x : float, optional. Maximum x shear. Defaults to 5.0. min_shear_y : float, optional. Minimum y shear. Defaults to -5.0. max_shear_y : float, optional. Maximum y shear. Defaults to 5.0. min_shear_z : float, optional. Minimum z shear. Defaults to -5.0. max_shear_z : float, optional. Maximum z shear. Defaults to 5.0. Example YAML Configuration preprocess_list: transform: References: https://towardsdatascience.com/how-to-transform-a-2d-image-into-a-3d-space-5fc2306e3d36","title":"RandomTransform class"},{"location":"dlf/preprocessing_methods/RandomVerticalFlip/","text":"RandomVerticalFlip class dlf.preprocessing.flip.RandomVerticalFlip() Data augmentation method that randomly vertical flips a RGB image Aliases v_flip vertical_flip VFlip VerticalFlip YAML Configuration preprocess_list: v_flip:","title":"RandomVerticalFlip"},{"location":"dlf/preprocessing_methods/RandomVerticalFlip/#randomverticalflip-class","text":"dlf.preprocessing.flip.RandomVerticalFlip() Data augmentation method that randomly vertical flips a RGB image Aliases v_flip vertical_flip VFlip VerticalFlip YAML Configuration preprocess_list: v_flip:","title":"RandomVerticalFlip class"},{"location":"dlf/preprocessing_methods/Resize/","text":"Resize class dlf.preprocessing.resize.Resize( width, height, method=\"bilinear\", preserve_aspect_ratio=False, antialias=False ) Data augmentation method that resizes the dimension of a RGB image For details tf.image.resize Aliases resize Resize Arguments width : int. Width of the resized image height : int. Height of the resized image method : ResizeMethod. Method used for resizing. Defaults to tf.image.ResizeMethod.BILINEAR. preserve_aspect_ratio : bool. If true, aspect ratio is preseverd. Defaults to False. antialias : bool. If true, an anti-aliasing filter is used for downsampling. Defaults to False. YAML Configuration preprocess_list: resize: width: 512 height: 512","title":"Resize"},{"location":"dlf/preprocessing_methods/Resize/#resize-class","text":"dlf.preprocessing.resize.Resize( width, height, method=\"bilinear\", preserve_aspect_ratio=False, antialias=False ) Data augmentation method that resizes the dimension of a RGB image For details tf.image.resize Aliases resize Resize Arguments width : int. Width of the resized image height : int. Height of the resized image method : ResizeMethod. Method used for resizing. Defaults to tf.image.ResizeMethod.BILINEAR. preserve_aspect_ratio : bool. If true, aspect ratio is preseverd. Defaults to False. antialias : bool. If true, an anti-aliasing filter is used for downsampling. Defaults to False. YAML Configuration preprocess_list: resize: width: 512 height: 512","title":"Resize class"}]}